{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f043e5",
   "metadata": {
    "id": "a3f043e5"
   },
   "source": [
    "# CNV MMRF CoMMpass — Pipeline (V7, reproducible)\n",
    "\n",
    "This notebook is a **scientifically rigorous and reproducible** CNV pipeline for the **MMRF CoMMpass** project (via GDC).\n",
    "\n",
    "## What V7 improves (key points)\n",
    "- **CNV ↔ Patient join**: builds `file_metadata.tsv` with `cases.submitter_id` (Participant_ID) directly from the GDC API (no UUID heuristics).\n",
    "- **Single source of truth**: produces a canonical `df_cnvs_final` with schema/type checks and chromosome normalization.\n",
    "- **No synthetic fallbacks**: no example data is injected.\n",
    "- **Biologically meaningful recurrence**: recurrence by **cytoband overlap** and by **fixed genomic bins** (instead of exact `chr:start-end`).\n",
    "- **Survival robustness**: avoids converting missingness (NA) into 0; includes FDR control and PH checks (when applicable).\n",
    "- **Predictive models (optional)**: anti-leakage survival risk prediction (penalized Cox) and ISS stage classification (macro-F1/accuracy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae9444",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cae9444",
    "outputId": "2c645aff-260a-4914-9cbf-f25e0a26868b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 0) SETUP (Colab-friendly)\n",
    "# =========================\n",
    "import sys, subprocess, importlib.util, os, json, hashlib\n",
    "\n",
    "def _ensure(pkg: str, pip_name: str | None = None):\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", (pip_name or pkg)])\n",
    "\n",
    "for pkg, pipn in [\n",
    "    (\"requests\", None),\n",
    "    (\"tqdm\", None),\n",
    "    (\"pandas\", None),\n",
    "    (\"numpy\", None),\n",
    "    (\"matplotlib\", None),\n",
    "    (\"sklearn\", \"scikit-learn\"),\n",
    "    (\"lifelines\", None),\n",
    "    (\"pyarrow\", None),\n",
    "    (\"pyranges\", None),\n",
    "]:\n",
    "    _ensure(pkg, pipn)\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pyranges as pr\n",
    "\n",
    "print(\"[OK] Dependências carregadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe9d5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bbe9d5f",
    "outputId": "be32c96d-458b-41ed-da3a-cd8214c9f34e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1) CONFIG + PROVENANCE\n",
    "# =========================\n",
    "from datetime import datetime\n",
    "\n",
    "RUN_ID = \"20260123_154712\"\n",
    "BASE_OUT_DIR = os.path.join(\"outputs\", f\"run_{RUN_ID}\")\n",
    "RAW_DIR  = os.path.join(BASE_OUT_DIR, \"raw\")\n",
    "PROC_DIR = os.path.join(BASE_OUT_DIR, \"processed\")\n",
    "RES_DIR  = os.path.join(BASE_OUT_DIR, \"results\")\n",
    "LOG_DIR  = os.path.join(BASE_OUT_DIR, \"logs\")\n",
    "\n",
    "# aliases (compatibilidade com versões antigas)\n",
    "OUT_DIR = BASE_OUT_DIR\n",
    "RESULTS_DIR = RES_DIR\n",
    "PROCESSED_DIR = PROC_DIR\n",
    "\n",
    "for d in [BASE_OUT_DIR, RAW_DIR, PROC_DIR, RES_DIR, LOG_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "GDC_FILES_URL = \"https://api.gdc.cancer.gov/files\"\n",
    "GDC_DATA_URL  = \"https://api.gdc.cancer.gov/data\"\n",
    "\n",
    "PROJECT_ID = \"MMRF-COMMPASS\"\n",
    "DATA_CATEGORY = \"Copy Number Variation\"\n",
    "\n",
    "DOWNLOAD_DIR = os.path.join(RAW_DIR, \"mmrf_cnv_data\")\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "THRESHOLDS = {\n",
    "    \"LOSS_HOMDEL\": -1.5,\n",
    "    \"LOSS_1COPY_MAX\": -0.5,\n",
    "    \"NEUTRAL_MIN\": -0.2,\n",
    "    \"NEUTRAL_MAX\":  0.2,\n",
    "    \"GAIN_1COPY_MIN\": 0.5,\n",
    "    \"GAIN_AMP\": 1.0\n",
    "}\n",
    "\n",
    "BIN_SIZE = 1_000_000\n",
    "TOP_N_REGIONS = 50\n",
    "\n",
    "# parâmetros do módulo data-driven (recurrent CNV)\n",
    "TOP_K_TOPCNV = 10       # quantas regiões recorrentes testar (bins)\n",
    "TOP_KM_PLOTS  = 10      # quantos KM plots salvar (seleção por p/frequência)\n",
    "MIN_OVERLAP_BP = 1      # presença em bin: overlap mínimo em bp\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "params = {\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"PROJECT_ID\": PROJECT_ID,\n",
    "    \"DATA_CATEGORY\": DATA_CATEGORY,\n",
    "    \"THRESHOLDS\": THRESHOLDS,\n",
    "    \"BIN_SIZE\": BIN_SIZE,\n",
    "    \"SEED\": SEED\n",
    "}\n",
    "with open(os.path.join(LOG_DIR, \"run_params.json\"), \"w\") as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "\n",
    "print(f\"[OK] RUN_ID={RUN_ID}\")\n",
    "print(f\"[OK] Saídas em: {BASE_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314559c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "314559c1",
    "outputId": "436caf6b-617e-4fe7-a8e9-fc13bbf65d6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 2) UTILITIES (auditáveis)\n",
    "# =========================\n",
    "def export_df_stats(df: pd.DataFrame, label: str, out_dir: str = None, n_head: int = 5):\n",
    "    # Pequeno relatório (shape, dtypes, NA, head) para auditoria\n",
    "    out_dir = out_dir or LOG_DIR\n",
    "    out = []\n",
    "    out.append(f\"label: {label}\")\n",
    "    out.append(f\"shape: {df.shape}\")\n",
    "    out.append(\"columns:\")\n",
    "    out.append(\", \".join(map(str, df.columns.tolist())))\n",
    "    out.append(\"\\nmissingness (top 20):\")\n",
    "    miss = df.isna().mean().sort_values(ascending=False).head(20)\n",
    "    out.append(miss.to_string())\n",
    "    out.append(\"\\ndtypes:\")\n",
    "    out.append(df.dtypes.astype(str).to_string())\n",
    "    out.append(\"\\nhead:\")\n",
    "    out.append(df.head(n_head).to_string(index=False))\n",
    "\n",
    "    path = os.path.join(out_dir, f\"{label}__stats.txt\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(out))\n",
    "    print(f\"[OK] stats → {path}\")\n",
    "\n",
    "def md5_file(path: str, chunk_size: int = 1<<20) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def safe_col(df: pd.DataFrame, candidates: list[str]):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "print(\"[OK] Utilitários prontos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6639e06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "b6639e06",
    "outputId": "c835e6ce-7efe-4496-d380-1f35df19dd64"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3) GDC: LIST FILES + METADATA (JOIN)\n",
    "# ==========================================\n",
    "filters = {\n",
    "    \"op\": \"and\",\n",
    "    \"content\": [\n",
    "        {\"op\": \"=\",  \"content\": {\"field\": \"cases.project.project_id\", \"value\": [PROJECT_ID]}},\n",
    "        {\"op\": \"=\",  \"content\": {\"field\": \"data_category\",            \"value\": [DATA_CATEGORY]}},\n",
    "        {\"op\": \"in\", \"content\": {\"field\": \"access\",                  \"value\": [\"open\"]}},\n",
    "    ]\n",
    "}\n",
    "\n",
    "fields = [\n",
    "    \"file_id\",\"file_name\",\"data_category\",\"data_type\",\"data_format\",\"file_size\",\"md5sum\",\n",
    "    \"cases.submitter_id\",\"cases.case_id\"\n",
    "]\n",
    "fields_str = \",\".join(fields)\n",
    "\n",
    "def gdc_list_files(filters: dict, fields: str, page_size: int = 2000) -> pd.DataFrame:\n",
    "    payload = {\n",
    "        \"filters\": filters,\n",
    "        \"fields\": fields,\n",
    "        \"format\": \"JSON\",\n",
    "        \"size\": page_size\n",
    "    }\n",
    "    r = requests.post(GDC_FILES_URL, json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    hits = j[\"data\"][\"hits\"]\n",
    "    rows = []\n",
    "    for h in hits:\n",
    "        case_submitter = None\n",
    "        case_id = None\n",
    "        if isinstance(h.get(\"cases\"), list) and len(h[\"cases\"]) > 0:\n",
    "            case_submitter = h[\"cases\"][0].get(\"submitter_id\")\n",
    "            case_id = h[\"cases\"][0].get(\"case_id\")\n",
    "        rows.append({\n",
    "            \"file_id\": h.get(\"file_id\"),\n",
    "            \"file_name\": h.get(\"file_name\"),\n",
    "            \"data_category\": h.get(\"data_category\"),\n",
    "            \"data_type\": h.get(\"data_type\"),\n",
    "            \"data_format\": h.get(\"data_format\"),\n",
    "            \"file_size\": h.get(\"file_size\"),\n",
    "            \"md5sum\": h.get(\"md5sum\"),\n",
    "            \"cases.submitter_id\": case_submitter,\n",
    "            \"cases.case_id\": case_id\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "files_df = gdc_list_files(filters, fields_str)\n",
    "if files_df.empty:\n",
    "    raise RuntimeError(\"Nenhum arquivo CNV open-access encontrado. Verifique PROJECT_ID/DATA_CATEGORY.\")\n",
    "\n",
    "print(f\"[OK] arquivos CNV encontrados: {len(files_df)}\")\n",
    "print(f\"[INFO] % sem cases.submitter_id: {files_df['cases.submitter_id'].isna().mean():.3f}\")\n",
    "\n",
    "files_df_path = os.path.join(RAW_DIR, \"gdc_cnv_files.tsv\")\n",
    "files_df.to_csv(files_df_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(files_df, \"gdc_cnv_files\")\n",
    "\n",
    "files_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48394c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "e36fd5ec0a604047aadaa4873bb486f8",
      "4fae1f46dde540cbafccd5589fbc2f89",
      "a67afb3caa0341a6a31728dac1cc5f48",
      "8ec11ae9e49d43049d2831d2031bcbcf",
      "548e9bb161c54ad69438a3514ea5a5cc",
      "6d81328f457846ddbe91de0f297ea45b",
      "2f4520e102b64946ab8bd48a29988a3d",
      "e86b1d90b727417a89fc31b183d2aee2",
      "5232cf4be73542bda6cea7e0aad12605",
      "ccf10e19a7b845a5953850853ceccdf6",
      "d2705b24196b4340a664a007884460f8"
     ]
    },
    "id": "fd48394c",
    "outputId": "343b2f18-b675-4cda-bc43-37a8837302cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 4) DOWNLOAD (with MD5 verification)\n",
    "# ================================\n",
    "def download_gdc_file(file_id: str, file_name: str, md5_expected: str | None, out_dir: str = DOWNLOAD_DIR) -> str:\n",
    "    out_path = os.path.join(out_dir, file_name)\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
    "        if md5_expected:\n",
    "            md5_now = md5_file(out_path)\n",
    "            if md5_now.lower() == str(md5_expected).lower():\n",
    "                return out_path\n",
    "            else:\n",
    "                print(f\"[WARN] md5 divergente para {file_name}. Rebaixando…\")\n",
    "                os.remove(out_path)\n",
    "\n",
    "    url = f\"{GDC_DATA_URL}/{file_id}\"\n",
    "    with requests.get(url, stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1<<20):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    if md5_expected:\n",
    "        md5_now = md5_file(out_path)\n",
    "        if md5_now.lower() != str(md5_expected).lower():\n",
    "            raise RuntimeError(f\"md5 inválido: {file_name} (expected={md5_expected}, got={md5_now})\")\n",
    "    return out_path\n",
    "\n",
    "paths = []\n",
    "for _, row in tqdm(files_df.iterrows(), total=len(files_df), desc=\"Downloading\"):\n",
    "    paths.append(download_gdc_file(row[\"file_id\"], row[\"file_name\"], row.get(\"md5sum\")))\n",
    "\n",
    "print(f\"[OK] Downloads em: {DOWNLOAD_DIR}\")\n",
    "print(f\"[OK] Arquivos baixados: {len(paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821b672",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "c821b672",
    "outputId": "90c24820-670e-4045-d941-bd6c38d77da8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5) FILE METADATA (JOIN TABLE) — PUBLICÁVEL\n",
    "# ==========================================\n",
    "file_metadata = files_df[[\"file_id\",\"file_name\",\"md5sum\",\"file_size\",\"data_type\",\"data_format\",\"cases.submitter_id\"]].copy()\n",
    "file_metadata = file_metadata.rename(columns={\"cases.submitter_id\":\"Participant_ID\"})\n",
    "file_metadata[\"Participant_ID\"] = file_metadata[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "meta_path = os.path.join(PROC_DIR, \"file_metadata.tsv\")\n",
    "file_metadata.to_csv(meta_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(file_metadata, \"file_metadata\")\n",
    "\n",
    "file_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d2ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360,
     "referenced_widgets": [
      "af79ddf412f44f6999deae91cf97f785",
      "02f4a119938a4d72b68736cc06a592c9",
      "63933634114146ccb47686661022da35",
      "19a0ea423bbd4f608164e21d19e24994",
      "e5b94c94fe24404980a845c00514a93d",
      "e731de3b1a6c434c891adcaca6c4b487",
      "92dc4a519ef342b6a471410a4943abbf",
      "5daaed9883924339b1a1caf411337e00",
      "3dcb3bc452904ac58716792953830c40",
      "21f09a58f4d040b2baf6037a87d67fa0",
      "b9d0f0712e624a9e87ea78245e91452c"
     ]
    },
    "id": "072d2ce3",
    "outputId": "fc335b76-7f19-474b-c5dd-2fa91267317a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 6) LOAD + NORMALIZATION + QC (CANÔNICO)\n",
    "# =====================================\n",
    "COLMAP_CANDIDATES = {\n",
    "    \"Chromosome\": [\"Chromosome\",\"chromosome\",\"CHR\",\"chr\",\"Chrom\",\"chrom\"],\n",
    "    \"Start\": [\"Start\",\"start\",\"loc.start\",\"start_pos\",\"POS_START\",\"Begin\",\"start_position\"],\n",
    "    \"End\": [\"End\",\"end\",\"loc.end\",\"end_pos\",\"POS_END\",\"End_Position\",\"end_position\"],\n",
    "    \"Segment_Mean\": [\"Segment_Mean\",\"segment_mean\",\"SegmentMean\",\"seg.mean\",\"mean\",\"value\"]\n",
    "}\n",
    "\n",
    "def normalize_chr(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    s = str(x).strip()\n",
    "    s = s.replace(\"chr\",\"\").replace(\"CHR\",\"\").replace(\"Chr\",\"\")\n",
    "    s = s.strip()\n",
    "    if s in [\"M\",\"MT\",\"m\",\"mt\"]:\n",
    "        return pd.NA\n",
    "    return s\n",
    "\n",
    "VALID_CHR = set([str(i) for i in range(1,23)] + [\"X\",\"Y\"])\n",
    "\n",
    "def read_cnv_file(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", low_memory=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", low_memory=False)\n",
    "\n",
    "    out = {}\n",
    "    for std, cands in COLMAP_CANDIDATES.items():\n",
    "        c = safe_col(df, cands)\n",
    "        out[std] = df[c] if c is not None else pd.Series([pd.NA]*len(df))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "meta_index = file_metadata.set_index(\"file_name\")[[\"file_id\",\"Participant_ID\"]].to_dict(orient=\"index\")\n",
    "\n",
    "dfs = []\n",
    "skipped = 0\n",
    "for fname in tqdm(sorted(os.listdir(DOWNLOAD_DIR)), desc=\"Parsing CNV files\"):\n",
    "    fpath = os.path.join(DOWNLOAD_DIR, fname)\n",
    "    if not os.path.isfile(fpath):\n",
    "        continue\n",
    "    if fname.endswith(\".gz\"):\n",
    "        print(f\"[WARN] Arquivo .gz não processado automaticamente: {fname}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    df0 = read_cnv_file(fpath)\n",
    "    m = meta_index.get(fname, None)\n",
    "    df0[\"source_file\"] = fname\n",
    "    df0[\"file_id\"] = m[\"file_id\"] if m else pd.NA\n",
    "    df0[\"Participant_ID\"] = m[\"Participant_ID\"] if m else pd.NA\n",
    "    dfs.append(df0)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"[OK] Segmentos carregados: {len(df_all):,} | arquivos ignorados: {skipped}\")\n",
    "\n",
    "df_all[\"Chromosome\"] = df_all[\"Chromosome\"].map(normalize_chr).astype(\"string\")\n",
    "df_all[\"Start\"] = pd.to_numeric(df_all[\"Start\"], errors=\"coerce\")\n",
    "df_all[\"End\"]   = pd.to_numeric(df_all[\"End\"], errors=\"coerce\")\n",
    "df_all[\"Segment_Mean\"] = pd.to_numeric(df_all[\"Segment_Mean\"], errors=\"coerce\")\n",
    "\n",
    "df_all = df_all[df_all[\"Chromosome\"].isin(VALID_CHR)]\n",
    "df_all = df_all[df_all[\"Start\"].notna() & df_all[\"End\"].notna()]\n",
    "df_all = df_all[df_all[\"End\"] > df_all[\"Start\"]]\n",
    "df_all = df_all[df_all[\"Participant_ID\"].notna()]\n",
    "\n",
    "df_cnvs_final = df_all.copy()\n",
    "df_cnvs_final[\"__JOIN_ID__\"] = df_cnvs_final[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "parq_path = os.path.join(PROC_DIR, \"cnv_segments.parquet\")\n",
    "tsv_path  = os.path.join(PROC_DIR, \"combined_cnvs.txt\")\n",
    "df_cnvs_final.to_parquet(parq_path, index=False)\n",
    "df_cnvs_final.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
    "\n",
    "export_df_stats(df_cnvs_final, \"df_cnvs_final\")\n",
    "\n",
    "df_cnvs_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9080",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "43cc9080",
    "outputId": "e6ec1610-cfd9-4c29-d0cb-4ad14296b931"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) CNA CLASSIFICATION (Segment_Mean) — SAFE\n",
    "# ==========================================\n",
    "def classify_segment_mean(x: float) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"Uncertain / margin\"\n",
    "    if x <= THRESHOLDS[\"LOSS_HOMDEL\"]:\n",
    "        return \"Homozygous deletion (0 copies)\"\n",
    "    if x < THRESHOLDS[\"LOSS_1COPY_MAX\"]:\n",
    "        return \"Loss (1 copy)\"\n",
    "    if THRESHOLDS[\"NEUTRAL_MIN\"] <= x <= THRESHOLDS[\"NEUTRAL_MAX\"]:\n",
    "        return \"Neutral (2 copies)\"\n",
    "    if x >= THRESHOLDS[\"GAIN_AMP\"]:\n",
    "        return \"High-level amplification (≥4 copies)\"\n",
    "    if x > THRESHOLDS[\"GAIN_1COPY_MIN\"]:\n",
    "        return \"Gain (3 copies)\"\n",
    "    return \"Uncertain / margin\"\n",
    "\n",
    "df_cnvs_final[\"CNV_Type_Ajustado\"] = df_cnvs_final[\"Segment_Mean\"].map(classify_segment_mean).astype(\"string\")\n",
    "\n",
    "classified_path = os.path.join(PROC_DIR, \"combined_cnvs_classified_adjusted.tsv\")\n",
    "df_cnvs_final.to_csv(classified_path, sep=\"\\t\", index=False)\n",
    "\n",
    "export_df_stats(df_cnvs_final, \"df_cnvs_final__classified\")\n",
    "df_cnvs_final[\"CNV_Type_Ajustado\"].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320f4ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "8320f4ce",
    "outputId": "2c58767f-c455-4cf6-8720-81852ad35723"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# 8) FILTER “HIGH-CONFIDENCE CNAs” (semântica correta)\n",
    "# ====================================================\n",
    "REMOVE_CLASSES = {\"Cópia Neutra (2 Cópias)\", \"Indeterminado/Margem de Incerteza\"}\n",
    "df_cnvs_hc = df_cnvs_final[~df_cnvs_final[\"CNV_Type_Ajustado\"].isin(REMOVE_CLASSES)].copy()\n",
    "\n",
    "hc_path = os.path.join(PROC_DIR, \"combined_cnvs_filtrado_somatica.txt\")\n",
    "df_cnvs_hc.to_csv(hc_path, sep=\"\\t\", index=False)\n",
    "\n",
    "export_df_stats(df_cnvs_hc, \"df_cnvs_high_confidence\")\n",
    "df_cnvs_hc.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e63f69",
   "metadata": {
    "id": "d1e63f69"
   },
   "source": [
    "# 8.5) RECURRENCE: exact SegmentID (chr:start-end) — “by patient” vs “by occurrence”\n",
    "\n",
    "> **Why this step exists:** exact `SegmentID` is *fragile* (breakpoints vary across patients), so it typically underestimates true biological recurrence.\n",
    "> Still, it can be useful for comparisons with “baseline” analyses and for discussion in the manuscript.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be41727",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "4be41727",
    "outputId": "9d1deb09-8400-4ec7-9f6a-4640fc1732fa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 8.5) RECURRÊNCIA: SegmentID exato (chr:start-end)\n",
    "#   - Por PACIENTE: % de pacientes com a CNV exata\n",
    "#   - Por OCORRÊNCIA: % das linhas/segmentos (equivalente ao método antigo)\n",
    "# ==========================================\n",
    "\n",
    "if \"df_cnvs_hc\" not in globals() or not isinstance(df_cnvs_hc, pd.DataFrame) or len(df_cnvs_hc) == 0:\n",
    "    raise RuntimeError(\"df_cnvs_hc não encontrado. Rode a etapa 8) FILTRO 'HIGH-CONFIDENCE CNAs' antes.\")\n",
    "\n",
    "df_seg = df_cnvs_hc.copy()\n",
    "\n",
    "# Colunas canônicas esperadas no V4\n",
    "REQ = [\"Participant_ID\", \"Chromosome\", \"Start\", \"End\", \"CNV_Type_Ajustado\"]\n",
    "missing = [c for c in REQ if c not in df_seg.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltam colunas para SegmentID exato: {missing}. Disponíveis: {list(df_seg.columns)}\")\n",
    "\n",
    "# Tipos / limpeza mínima\n",
    "df_seg[\"Chromosome\"] = df_seg[\"Chromosome\"].astype(str)\n",
    "df_seg[\"Start\"] = pd.to_numeric(df_seg[\"Start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_seg[\"End\"]   = pd.to_numeric(df_seg[\"End\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "df_seg = df_seg.dropna(subset=[\"Participant_ID\", \"Chromosome\", \"Start\", \"End\"])\n",
    "\n",
    "# SegmentID exato\n",
    "df_seg[\"SegmentID_exact\"] = (\n",
    "    df_seg[\"Chromosome\"] + \":\" +\n",
    "    df_seg[\"Start\"].astype(str) + \"-\" +\n",
    "    df_seg[\"End\"].astype(str)\n",
    ")\n",
    "\n",
    "N_patients = df_seg[\"Participant_ID\"].nunique()\n",
    "if N_patients == 0:\n",
    "    raise RuntimeError(\"N_patients=0 após limpeza. Verifique Participant_ID / filtros anteriores.\")\n",
    "\n",
    "print(f\"[INFO] SegmentID exact — N pacientes distintos: {N_patients:,}\")\n",
    "print(f\"[INFO] SegmentID exact — N linhas (HC): {len(df_seg):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# A) Por PACIENTE (coorte)\n",
    "# -------------------------\n",
    "df_uniq = df_seg[[\"Participant_ID\", \"SegmentID_exact\", \"CNV_Type_Ajustado\"]].drop_duplicates()\n",
    "\n",
    "seg_by_patient = (\n",
    "    df_uniq\n",
    "    .groupby([\"SegmentID_exact\", \"CNV_Type_Ajustado\"])[\"Participant_ID\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_patients\")\n",
    ")\n",
    "seg_by_patient[\"pct_patients\"] = (seg_by_patient[\"n_patients\"] / N_patients) * 100\n",
    "seg_by_patient = seg_by_patient.sort_values([\"n_patients\", \"pct_patients\"], ascending=False)\n",
    "\n",
    "# -------------------------\n",
    "# B) Por OCORRÊNCIA (linhas)\n",
    "# -------------------------\n",
    "seg_by_rows = (\n",
    "    df_seg\n",
    "    .groupby([\"SegmentID_exact\", \"CNV_Type_Ajustado\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count_rows\")\n",
    ")\n",
    "total_rows = int(seg_by_rows[\"count_rows\"].sum())\n",
    "seg_by_rows[\"pct_rows\"] = (seg_by_rows[\"count_rows\"] / total_rows) * 100\n",
    "seg_by_rows = seg_by_rows.sort_values(\"count_rows\", ascending=False)\n",
    "\n",
    "# -------------------------\n",
    "# C) Comparativo lado-a-lado\n",
    "# -------------------------\n",
    "seg_compare = (\n",
    "    seg_by_patient\n",
    "    .merge(seg_by_rows, on=[\"SegmentID_exact\", \"CNV_Type_Ajustado\"], how=\"left\")\n",
    "    .sort_values([\"n_patients\", \"count_rows\"], ascending=False)\n",
    ")\n",
    "\n",
    "display(seg_compare.head(15))\n",
    "\n",
    "# -------------------------\n",
    "# Export (TSV)\n",
    "# -------------------------\n",
    "RES_DIR = globals().get(\"RES_DIR\", globals().get(\"RESULTS_DIR\", \"outputs/segmentid_exact/results\"))\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "p_patient = os.path.join(RES_DIR, \"cnv_recurrence_by_segmentid_exact__by_patient.tsv\")\n",
    "p_rows    = os.path.join(RES_DIR, \"cnv_recurrence_by_segmentid_exact__by_rows.tsv\")\n",
    "p_cmp     = os.path.join(RES_DIR, \"cnv_recurrence_by_segmentid_exact__compare.tsv\")\n",
    "\n",
    "seg_by_patient.to_csv(p_patient, sep=\"\\t\", index=False)\n",
    "seg_by_rows.to_csv(p_rows, sep=\"\\t\", index=False)\n",
    "seg_compare.to_csv(p_cmp, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"[OK] Exportados:\")\n",
    "print(\" -\", p_patient)\n",
    "print(\" -\", p_rows)\n",
    "print(\" -\", p_cmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322663d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "0322663d",
    "outputId": "e6cdcd21-7fdb-4e77-e250-ce3e17620063"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9) RECURRÊNCIA: CITOBANDA (hg38, overlap) — MEMORY SAFE + PUBLICÁVEL\n",
    "# ==========================================\n",
    "import os, gzip, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pyranges\n",
    "try:\n",
    "    import pyranges as pr\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"pyranges não está disponível. Instale: !pip -q install pyranges\") from e\n",
    "\n",
    "# Diretórios (padrão do notebook)\n",
    "RAW_DIR  = globals().get(\"RAW_DIR\",  os.path.join(BASE_OUT_DIR, \"raw\"))\n",
    "PROC_DIR = globals().get(\"PROC_DIR\", os.path.join(BASE_OUT_DIR, \"processed\"))\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "CYTO_URL = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/cytoBand.txt.gz\"\n",
    "cyto_gz = os.path.join(RAW_DIR, \"cytoBand_hg38.txt.gz\")\n",
    "cyto_tsv = os.path.join(PROC_DIR, \"cytoBand_hg38.tsv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Baixar + parse (cacheado)\n",
    "# ----------------------------\n",
    "if not os.path.exists(cyto_tsv):\n",
    "    if not os.path.exists(cyto_gz):\n",
    "        r = requests.get(CYTO_URL, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        with open(cyto_gz, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"[OK] baixado: {cyto_gz}\")\n",
    "\n",
    "    rows = []\n",
    "    with gzip.open(cyto_gz, \"rt\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 5:\n",
    "                chrom, start, end, name, stain = parts[:5]\n",
    "                chrom = chrom.replace(\"chr\", \"\")\n",
    "                if chrom in VALID_CHR:\n",
    "                    rows.append([chrom, int(start), int(end), name, stain])\n",
    "\n",
    "    cyto_df = pd.DataFrame(rows, columns=[\"Chromosome\", \"Start\", \"End\", \"Cytoband\", \"Stain\"])\n",
    "    cyto_df.to_csv(cyto_tsv, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    cyto_df = pd.read_csv(cyto_tsv, sep=\"\\t\")\n",
    "\n",
    "export_df_stats(cyto_df, \"cytoBand_hg38\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Validar df_cnvs_hc e padronizar\n",
    "# -----------------------------------\n",
    "cnv_cols = [\"Chromosome\", \"Start\", \"End\", \"Participant_ID\", \"CNV_Type_Ajustado\"]\n",
    "missing = [c for c in cnv_cols if c not in df_cnvs_hc.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"df_cnvs_hc está sem colunas obrigatórias para citobanda: {missing}\")\n",
    "\n",
    "df_cnvs_hc = df_cnvs_hc.copy()\n",
    "df_cnvs_hc[\"Chromosome\"] = df_cnvs_hc[\"Chromosome\"].astype(str).str.replace(\"chr\", \"\", regex=False)\n",
    "df_cnvs_hc[\"Participant_ID\"] = df_cnvs_hc[\"Participant_ID\"].astype(str)\n",
    "df_cnvs_hc[\"CNV_Type_Ajustado\"] = df_cnvs_hc[\"CNV_Type_Ajustado\"].astype(str)\n",
    "df_cnvs_hc[\"Start\"] = pd.to_numeric(df_cnvs_hc[\"Start\"], errors=\"coerce\")\n",
    "df_cnvs_hc[\"End\"]   = pd.to_numeric(df_cnvs_hc[\"End\"], errors=\"coerce\")\n",
    "df_cnvs_hc = df_cnvs_hc.dropna(subset=[\"Start\",\"End\"])\n",
    "df_cnvs_hc[\"Start\"] = df_cnvs_hc[\"Start\"].astype(int)\n",
    "df_cnvs_hc[\"End\"]   = df_cnvs_hc[\"End\"].astype(int)\n",
    "df_cnvs_hc = df_cnvs_hc[df_cnvs_hc[\"Chromosome\"].isin(VALID_CHR)].copy()\n",
    "\n",
    "seg_key = [\"Participant_ID\", \"Chromosome\", \"Start\", \"End\", \"CNV_Type_Ajustado\"]\n",
    "\n",
    "def chr_sort_key(x: str):\n",
    "    if x.isdigit():\n",
    "        return (0, int(x))\n",
    "    if x == \"X\":\n",
    "        return (1, 23)\n",
    "    if x == \"Y\":\n",
    "        return (1, 24)\n",
    "    return (2, 999)\n",
    "\n",
    "chroms_to_process = sorted(df_cnvs_hc[\"Chromosome\"].unique().tolist(), key=chr_sort_key)\n",
    "best_chunks = []\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Join CNV × cytoband por cromossomo (memory safe)\n",
    "# ----------------------------\n",
    "for chrom in chroms_to_process:\n",
    "    df_chr = df_cnvs_hc[df_cnvs_hc[\"Chromosome\"] == chrom][cnv_cols].copy()\n",
    "    if df_chr.empty:\n",
    "        continue\n",
    "\n",
    "    cyto_chr = cyto_df[cyto_df[\"Chromosome\"] == chrom][[\"Chromosome\", \"Start\", \"End\", \"Cytoband\"]].copy()\n",
    "    if cyto_chr.empty:\n",
    "        tmp = df_chr.copy()\n",
    "        tmp[\"Cytoband\"] = np.nan\n",
    "        tmp[\"overlap_bp\"] = 0\n",
    "        best_chunks.append(tmp[seg_key + [\"Cytoband\", \"overlap_bp\"]])\n",
    "        continue\n",
    "\n",
    "    cnv_pr = pr.PyRanges(df_chr)\n",
    "    cyto_pr = pr.PyRanges(cyto_chr)\n",
    "\n",
    "    ov = cnv_pr.join(cyto_pr, suffix=\"_cyto\")\n",
    "    ov_df = ov.df\n",
    "    if ov_df.empty:\n",
    "        tmp = df_chr.copy()\n",
    "        tmp[\"Cytoband\"] = np.nan\n",
    "        tmp[\"overlap_bp\"] = 0\n",
    "        best_chunks.append(tmp[seg_key + [\"Cytoband\", \"overlap_bp\"]])\n",
    "        continue\n",
    "\n",
    "    cyto_col = \"Cytoband_cyto\" if \"Cytoband_cyto\" in ov_df.columns else \"Cytoband\"\n",
    "    ov_df = ov_df[seg_key + [\"Start_cyto\", \"End_cyto\", cyto_col]].copy()\n",
    "\n",
    "    ov_df[\"overlap_bp\"] = (\n",
    "        np.minimum(ov_df[\"End\"], ov_df[\"End_cyto\"]) - np.maximum(ov_df[\"Start\"], ov_df[\"Start_cyto\"])\n",
    "    ).clip(lower=0).astype(\"int32\")\n",
    "\n",
    "    idx = ov_df.groupby(seg_key, observed=True)[\"overlap_bp\"].idxmax()\n",
    "    best_chr = ov_df.loc[idx, seg_key + [cyto_col, \"overlap_bp\"]].copy()\n",
    "    best_chr = best_chr.rename(columns={cyto_col: \"Cytoband\"})\n",
    "    best_chunks.append(best_chr)\n",
    "\n",
    "best = pd.concat(best_chunks, ignore_index=True) if best_chunks else pd.DataFrame(columns=seg_key + [\"Cytoband\", \"overlap_bp\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Merge, export e recorrência publicável\n",
    "# ----------------------------\n",
    "df_cnvs_hc_annot = df_cnvs_hc.merge(best, on=seg_key, how=\"left\")\n",
    "\n",
    "annot_path = os.path.join(PROC_DIR, \"cnv_segments_high_confidence_cytoband.tsv\")\n",
    "df_cnvs_hc_annot.to_csv(annot_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(df_cnvs_hc_annot, \"df_cnvs_hc__cytoband\")\n",
    "\n",
    "# edges paciente↔citobanda (presença)\n",
    "edges_cyto = df_cnvs_hc_annot[[\"Participant_ID\",\"Chromosome\",\"Cytoband\",\"CNV_Type_Ajustado\",\"overlap_bp\"]].copy()\n",
    "edges_cyto[\"Participant_ID\"] = edges_cyto[\"Participant_ID\"].astype(str)\n",
    "edges_cyto[\"Chromosome\"] = edges_cyto[\"Chromosome\"].astype(str)\n",
    "edges_cyto[\"CNV_Type_Ajustado\"] = edges_cyto[\"CNV_Type_Ajustado\"].astype(str)\n",
    "edges_cyto = edges_cyto.dropna(subset=[\"Cytoband\"])\n",
    "edges_cyto = edges_cyto.drop_duplicates([\"Participant_ID\",\"Chromosome\",\"Cytoband\",\"CNV_Type_Ajustado\"])\n",
    "\n",
    "edges_path = os.path.join(PROC_DIR, \"cnv_patient_cytoband_overlaps.tsv\")\n",
    "edges_cyto.to_csv(edges_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# recorrência por citobanda (pacientes únicos)\n",
    "rec_cyto = (edges_cyto\n",
    "    .groupby([\"Chromosome\",\"Cytoband\",\"CNV_Type_Ajustado\"], observed=True)[\"Participant_ID\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_patients\")\n",
    "    .sort_values([\"Chromosome\",\"n_patients\"], ascending=[True, False])\n",
    ")\n",
    "out_path = os.path.join(RES_DIR, \"cnv_recurrence_by_cytoband.tsv\")\n",
    "rec_cyto.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(rec_cyto, \"cnv_recurrence_by_cytoband\")\n",
    "\n",
    "print(f\"[OK] anotado: {annot_path}\")\n",
    "print(f\"[OK] edges paciente↔citobanda: {edges_path}\")\n",
    "print(f\"[OK] recorrência citobanda: {out_path}\")\n",
    "\n",
    "rec_cyto.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f1e2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "684f1e2a",
    "outputId": "e67d0e34-9879-418b-f1d6-9a9923d5daf5"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) RECURRÊNCIA: BINS FIXOS (1Mb, hg38, overlap) — PUBLICÁVEL + COMPATÍVEL COM PLOTS\n",
    "# ==========================================\n",
    "import os, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import pyranges as pr\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"pyranges não está disponível. Instale: !pip -q install pyranges\") from e\n",
    "\n",
    "# Diretórios (padrão do notebook)\n",
    "PROC_DIR = globals().get(\"PROC_DIR\", os.path.join(BASE_OUT_DIR, \"processed\"))\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "LOG_DIR  = globals().get(\"LOG_DIR\",  os.path.join(BASE_OUT_DIR, \"logs\"))\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "CHROMSIZES_URL = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes\"\n",
    "chromsizes_path = os.path.join(PROC_DIR, \"hg38.chrom.sizes\")\n",
    "\n",
    "BIN_SIZE = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "MIN_OVERLAP_BP = int(globals().get(\"MIN_OVERLAP_BP\", 1))  # para presença em bin\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Baixar chrom.sizes (cacheado)\n",
    "# ----------------------------\n",
    "if not os.path.exists(chromsizes_path):\n",
    "    r = requests.get(CHROMSIZES_URL, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with open(chromsizes_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"[OK] chrom.sizes → {chromsizes_path}\")\n",
    "else:\n",
    "    print(f\"[OK] chrom.sizes (cache) → {chromsizes_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Ler tamanhos e filtrar cromossomos\n",
    "# ----------------------------\n",
    "sizes = []\n",
    "with open(chromsizes_path, \"rt\") as f:\n",
    "    for line in f:\n",
    "        chrom, size = line.strip().split(\"\\t\")[:2]\n",
    "        chrom = chrom.replace(\"chr\", \"\")\n",
    "        if chrom in VALID_CHR:\n",
    "            sizes.append((chrom, int(size)))\n",
    "\n",
    "sizes_df = pd.DataFrame(sizes, columns=[\"Chromosome\", \"ChromSize\"])\n",
    "export_df_stats(sizes_df, \"hg38_chrom_sizes_filtered\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Gerar bins 1Mb (BinStart/BinEnd) + BinID\n",
    "# ----------------------------\n",
    "bin_rows = []\n",
    "for chrom, chrom_size in sizes:\n",
    "    for bs in range(0, chrom_size, BIN_SIZE):\n",
    "        be = min(bs + BIN_SIZE, chrom_size)\n",
    "        bin_rows.append((chrom, bs, be))\n",
    "\n",
    "bins_df = pd.DataFrame(bin_rows, columns=[\"Chromosome\", \"BinStart\", \"BinEnd\"])\n",
    "bins_df[\"BinID\"] = (\n",
    "    bins_df[\"Chromosome\"].astype(str)\n",
    "    + \":\"\n",
    "    + bins_df[\"BinStart\"].astype(str)\n",
    "    + \"-\"\n",
    "    + bins_df[\"BinEnd\"].astype(str)\n",
    ")\n",
    "\n",
    "# colunas Start/End para o PyRanges (mantendo BinStart/BinEnd para export/plots)\n",
    "bins_pr_df = bins_df.rename(columns={\"BinStart\": \"Start\", \"BinEnd\": \"End\"})[[\"Chromosome\",\"Start\",\"End\",\"BinID\"]].copy()\n",
    "bins_pr_df[\"Start\"] = bins_pr_df[\"Start\"].astype(int)\n",
    "bins_pr_df[\"End\"]   = bins_pr_df[\"End\"].astype(int)\n",
    "\n",
    "export_df_stats(bins_df, f\"hg38_bins_{BIN_SIZE}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Criar PyRanges a partir de DataFrame\n",
    "# ----------------------------\n",
    "bins_pr = pr.PyRanges(bins_pr_df)\n",
    "\n",
    "# CNVs high-confidence\n",
    "cnv_cols = [\"Chromosome\", \"Start\", \"End\", \"Participant_ID\", \"CNV_Type_Ajustado\"]\n",
    "missing = [c for c in cnv_cols if c not in df_cnvs_hc.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"df_cnvs_hc está sem colunas obrigatórias para bins: {missing}\")\n",
    "\n",
    "df_cnv_bins = df_cnvs_hc[cnv_cols].copy()\n",
    "df_cnv_bins[\"Chromosome\"] = df_cnv_bins[\"Chromosome\"].astype(str).str.replace(\"chr\",\"\", regex=False)\n",
    "df_cnv_bins[\"Participant_ID\"] = df_cnv_bins[\"Participant_ID\"].astype(str)\n",
    "df_cnv_bins[\"CNV_Type_Ajustado\"] = df_cnv_bins[\"CNV_Type_Ajustado\"].astype(str)\n",
    "\n",
    "df_cnv_bins[\"Start\"] = pd.to_numeric(df_cnv_bins[\"Start\"], errors=\"coerce\")\n",
    "df_cnv_bins[\"End\"]   = pd.to_numeric(df_cnv_bins[\"End\"], errors=\"coerce\")\n",
    "df_cnv_bins = df_cnv_bins.dropna(subset=[\"Start\",\"End\"])\n",
    "df_cnv_bins[\"Start\"] = df_cnv_bins[\"Start\"].astype(int)\n",
    "df_cnv_bins[\"End\"]   = df_cnv_bins[\"End\"].astype(int)\n",
    "\n",
    "df_cnv_bins = df_cnv_bins[df_cnv_bins[\"Chromosome\"].isin(VALID_CHR)].copy()\n",
    "\n",
    "cnv_pr = pr.PyRanges(df_cnv_bins)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Overlap CNV × bins (edges paciente↔bin) + recorrência\n",
    "# ----------------------------\n",
    "ov = cnv_pr.join(bins_pr, suffix=\"_bin\")\n",
    "ov_df = ov.df.copy()\n",
    "\n",
    "if ov_df.empty:\n",
    "    raise RuntimeError(\"Overlap CNV×bins retornou vazio. Verifique cromossomos/Start/End.\")\n",
    "\n",
    "# detectar nomes com suffix\n",
    "binid_col = \"BinID_bin\" if \"BinID_bin\" in ov_df.columns else \"BinID\"\n",
    "start_bin_col = \"Start_bin\" if \"Start_bin\" in ov_df.columns else (\"Start_bin\" if \"Start_bin\" in ov_df.columns else None)\n",
    "end_bin_col   = \"End_bin\"   if \"End_bin\" in ov_df.columns else (\"End_bin\"   if \"End_bin\"   in ov_df.columns else None)\n",
    "\n",
    "# Em algumas versões, as colunas podem vir como Start_bin / End_bin mesmo (normal)\n",
    "if start_bin_col is None or end_bin_col is None:\n",
    "    # fallback: tenta 'Start_bin'/'End_bin' padrões do pyranges\n",
    "    cand = [c for c in ov_df.columns if c.lower() in (\"start_bin\",\"start_b\",\"start_right\")]\n",
    "    cand2 = [c for c in ov_df.columns if c.lower() in (\"end_bin\",\"end_b\",\"end_right\")]\n",
    "    if cand and cand2:\n",
    "        start_bin_col, end_bin_col = cand[0], cand2[0]\n",
    "    else:\n",
    "        raise RuntimeError(f\"Não encontrei colunas de bin Start/End no join. Colunas: {list(ov_df.columns)}\")\n",
    "\n",
    "# overlap_bp para filtro\n",
    "ov_df[\"overlap_bp\"] = (\n",
    "    np.minimum(ov_df[\"End\"], ov_df[end_bin_col]) - np.maximum(ov_df[\"Start\"], ov_df[start_bin_col])\n",
    ").clip(lower=0).astype(\"int32\")\n",
    "\n",
    "ov_df = ov_df[ov_df[\"overlap_bp\"] >= MIN_OVERLAP_BP].copy()\n",
    "\n",
    "edges = ov_df[[\"Participant_ID\",\"Chromosome\", start_bin_col, end_bin_col, binid_col, \"CNV_Type_Ajustado\", \"overlap_bp\"]].copy()\n",
    "edges = edges.rename(columns={\n",
    "    start_bin_col: \"BinStart\",\n",
    "    end_bin_col: \"BinEnd\",\n",
    "    binid_col: \"BinID\"\n",
    "})\n",
    "edges[\"Participant_ID\"] = edges[\"Participant_ID\"].astype(str)\n",
    "edges[\"Chromosome\"] = edges[\"Chromosome\"].astype(str).str.replace(\"chr\",\"\", regex=False)\n",
    "edges[\"CNV_Type_Ajustado\"] = edges[\"CNV_Type_Ajustado\"].astype(str)\n",
    "edges[\"BinStart\"] = edges[\"BinStart\"].astype(int)\n",
    "edges[\"BinEnd\"]   = edges[\"BinEnd\"].astype(int)\n",
    "edges[\"BinID\"]    = edges[\"BinID\"].astype(str)\n",
    "\n",
    "# dedup: presença por paciente/bin/tipo\n",
    "edges_uniq = edges.drop_duplicates([\"Participant_ID\",\"BinID\",\"CNV_Type_Ajustado\"]).copy()\n",
    "\n",
    "edges_path = os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "edges_uniq.to_csv(edges_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(edges_uniq, f\"cnv_patient_bin_overlaps_{BIN_SIZE}\")\n",
    "\n",
    "# recorrência: pacientes únicos por bin/tipo\n",
    "rec_bins = (edges_uniq\n",
    "    .groupby([\"Chromosome\",\"BinStart\",\"BinEnd\",\"BinID\",\"CNV_Type_Ajustado\"], observed=True)[\"Participant_ID\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_patients\")\n",
    "    .sort_values([\"Chromosome\",\"n_patients\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "out_path = os.path.join(RES_DIR, f\"cnv_recurrence_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "rec_bins.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "export_df_stats(rec_bins, f\"cnv_recurrence_by_bins_{BIN_SIZE}\")\n",
    "\n",
    "print(f\"[OK] edges paciente↔bin: {edges_path}\")\n",
    "print(f\"[OK] recorrência por bins salva: {out_path}\")\n",
    "rec_bins.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacba6d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "eacba6d4",
    "outputId": "ea416970-1a30-45cd-86b2-4e6030667412"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 11) BREAKPOINT FEATURES (por paciente)\n",
    "# ==========================================\n",
    "df = df_cnvs_hc.sort_values([\"Participant_ID\",\"Chromosome\",\"Start\",\"End\"]).copy()\n",
    "\n",
    "bp = pd.concat([\n",
    "    df[[\"Participant_ID\",\"Chromosome\",\"Start\"]].rename(columns={\"Start\":\"pos\"}),\n",
    "    df[[\"Participant_ID\",\"Chromosome\",\"End\"]].rename(columns={\"End\":\"pos\"})\n",
    "], ignore_index=True)\n",
    "\n",
    "bp[\"pos\"] = pd.to_numeric(bp[\"pos\"], errors=\"coerce\")\n",
    "bp = bp.dropna(subset=[\"pos\"])\n",
    "\n",
    "bp_total = bp.groupby(\"Participant_ID\").size().rename(\"BP_total\").reset_index()\n",
    "bp_unique = bp.groupby(\"Participant_ID\")[\"pos\"].nunique().rename(\"BP_unique\").reset_index()\n",
    "\n",
    "chr_counts = bp.groupby([\"Participant_ID\",\"Chromosome\"]).size().reset_index(name=\"n_bp\")\n",
    "\n",
    "def shannon_entropy_norm(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[x>0]\n",
    "    if len(x) <= 1:\n",
    "        return 0.0\n",
    "    p = x / x.sum()\n",
    "    H = -(p * np.log(p)).sum()\n",
    "    Hmax = np.log(len(p))\n",
    "    return float(H / Hmax) if Hmax>0 else 0.0\n",
    "\n",
    "def gini(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[x>=0]\n",
    "    if len(x)==0:\n",
    "        return np.nan\n",
    "    if np.all(x==0):\n",
    "        return 0.0\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    cumx = np.cumsum(x)\n",
    "    return float((n + 1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
    "\n",
    "ent = chr_counts.groupby(\"Participant_ID\")[\"n_bp\"].apply(shannon_entropy_norm).rename(\"BP_entropy_chr\").reset_index()\n",
    "gin = chr_counts.groupby(\"Participant_ID\")[\"n_bp\"].apply(gini).rename(\"BP_gini_chr\").reset_index()\n",
    "\n",
    "bp_metrics_df = bp_total.merge(bp_unique, on=\"Participant_ID\").merge(ent, on=\"Participant_ID\").merge(gin, on=\"Participant_ID\")\n",
    "bp_metrics_df[\"__JOIN_ID__\"] = bp_metrics_df[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "export_df_stats(bp_metrics_df, \"bp_metrics_df\")\n",
    "bp_metrics_path = os.path.join(RES_DIR, \"breakpoint_metrics_per_patient.tsv\")\n",
    "bp_metrics_df.to_csv(bp_metrics_path, sep=\"\\t\", index=False)\n",
    "\n",
    "bp_metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749dafa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "2749dafa",
    "outputId": "3a1841cd-81c7-4565-c9b2-e2d0c91619da"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================================\n",
    "# 12) HOTSPOT FEATURES (por paciente, janelas 1Mb)\n",
    "# ==========================================================\n",
    "WINDOW_SIZE = BIN_SIZE\n",
    "TOPK = 50\n",
    "\n",
    "bp2 = bp.copy()\n",
    "bp2[\"BinStart\"] = (bp2[\"pos\"] // WINDOW_SIZE) * WINDOW_SIZE\n",
    "win_counts = bp2.groupby([\"Participant_ID\",\"Chromosome\",\"BinStart\"]).size().reset_index(name=\"bp_count\")\n",
    "\n",
    "p95 = np.percentile(win_counts[\"bp_count\"], 95) if len(win_counts) else np.nan\n",
    "\n",
    "hot_p95 = (\n",
    "    win_counts.assign(is_hot = win_counts[\"bp_count\"] >= p95)\n",
    "    .groupby(\"Participant_ID\")[\"is_hot\"].sum()\n",
    "    .rename(\"HotspotCount_P95\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "def topk_sum(s, k=TOPK):\n",
    "    v = np.sort(np.asarray(s, dtype=float))[::-1]\n",
    "    return float(v[:k].sum()) if len(v) else 0.0\n",
    "\n",
    "hot_topk = win_counts.groupby(\"Participant_ID\")[\"bp_count\"].apply(lambda s: topk_sum(s, TOPK)).rename(\"HotspotScore_topK\").reset_index()\n",
    "hot_ent = win_counts.groupby(\"Participant_ID\")[\"bp_count\"].apply(shannon_entropy_norm).rename(\"HotspotEntropy\").reset_index()\n",
    "\n",
    "hotspot_metrics_df = hot_p95.merge(hot_topk, on=\"Participant_ID\").merge(hot_ent, on=\"Participant_ID\")\n",
    "hotspot_metrics_df[\"__JOIN_ID__\"] = hotspot_metrics_df[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "export_df_stats(hotspot_metrics_df, \"hotspot_metrics_df\")\n",
    "hotspot_path = os.path.join(RES_DIR, \"hotspot_metrics_per_patient.tsv\")\n",
    "hotspot_metrics_df.to_csv(hotspot_path, sep=\"\\t\", index=False)\n",
    "\n",
    "hotspot_metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740ecd8",
   "metadata": {
    "id": "1740ecd8"
   },
   "source": [
    "## Clinical / Survival block (LOCKDOWN)\n",
    "\n",
    "Builds `os_df` **without imputation** from `clinical.tsv` and `follow_up.tsv` (when available).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c12a66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "a1c12a66",
    "outputId": "e1092f47-163a-40f2-a2bf-72dcdd20340b"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 13) OS LOCKDOWN (sem imputação) + STAGING (todas as colunas disponíveis)\n",
    "#     + merge opcional de family_history / exposure / pathology_detail (se existirem)\n",
    "# =========================================================\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _find_first(paths):\n",
    "    for p in paths:\n",
    "        hits = glob.glob(p, recursive=True)\n",
    "        if hits:\n",
    "            hits = sorted(list(dict.fromkeys(hits)), key=lambda x:(len(x), x))\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "# busca em /content (Colab) + /mnt/data (sandbox) + Google Drive\n",
    "clinical_path = _find_first([\n",
    "    \"/content/clinical.tsv\",\n",
    "    \"/content/**/clinical.tsv\",\n",
    "    \"/content/**/clinical*.tsv\",\n",
    "    \"/content/drive/MyDrive/**/clinical.tsv\",\n",
    "    \"/mnt/data/clinical.tsv\",\n",
    "    \"/mnt/data/**/clinical.tsv\",\n",
    "])\n",
    "\n",
    "followup_path = _find_first([\n",
    "    \"/content/follow_up.tsv\",\n",
    "    \"/content/**/follow_up.tsv\",\n",
    "    \"/content/**/follow_up*.tsv\",\n",
    "    \"/content/drive/MyDrive/**/follow_up.tsv\",\n",
    "    \"/mnt/data/follow_up.tsv\",\n",
    "    \"/mnt/data/**/follow_up.tsv\",\n",
    "])\n",
    "\n",
    "family_path = _find_first([\n",
    "    \"/content/family_history.tsv\",\n",
    "    \"/content/**/family_history.tsv\",\n",
    "    \"/content/**/family_history*.tsv\",\n",
    "    \"/content/drive/MyDrive/**/family_history.tsv\",\n",
    "    \"/mnt/data/family_history.tsv\",\n",
    "    \"/mnt/data/**/family_history.tsv\",\n",
    "])\n",
    "\n",
    "exposure_path = _find_first([\n",
    "    \"/content/exposure.tsv\",\n",
    "    \"/content/**/exposure.tsv\",\n",
    "    \"/content/**/exposure*.tsv\",\n",
    "    \"/content/drive/MyDrive/**/exposure.tsv\",\n",
    "    \"/mnt/data/exposure.tsv\",\n",
    "    \"/mnt/data/**/exposure.tsv\",\n",
    "])\n",
    "\n",
    "pathology_detail_path = _find_first([\n",
    "    \"/content/pathology_detail.tsv\",\n",
    "    \"/content/**/pathology_detail.tsv\",\n",
    "    \"/content/**/pathology_detail*.tsv\",\n",
    "    \"/content/drive/MyDrive/**/pathology_detail.tsv\",\n",
    "    \"/mnt/data/pathology_detail.tsv\",\n",
    "    \"/mnt/data/**/pathology_detail.tsv\",\n",
    "])\n",
    "\n",
    "if clinical_path is None:\n",
    "    raise FileNotFoundError(\"Não encontrei clinical.tsv. Faça upload no Colab (ou deixe em /mnt/data) e rode novamente.\")\n",
    "\n",
    "df_clin = pd.read_csv(clinical_path, sep=\"\\t\", low_memory=False)\n",
    "print(f\"[OK] clinical.tsv: {clinical_path} | shape={df_clin.shape}\")\n",
    "\n",
    "# follow_up é grande; ler só colunas necessárias\n",
    "df_fu = None\n",
    "if followup_path is not None:\n",
    "    fu_cols = pd.read_csv(followup_path, sep=\"\\t\", nrows=0).columns.tolist()\n",
    "    fu_id_col = next((c for c in [\"cases.submitter_id\",\"cases.case_id\"] if c in fu_cols), None)\n",
    "    fu_time_col = next((c for c in [\"follow_ups.days_to_follow_up\",\"follow_ups.days_to_last_follow_up\",\"follow_ups.days_to_followup\"] if c in fu_cols), None)\n",
    "    if fu_id_col and fu_time_col:\n",
    "        df_fu = pd.read_csv(followup_path, sep=\"\\t\", usecols=[fu_id_col, fu_time_col], low_memory=False)\n",
    "        print(f\"[OK] follow_up.tsv (usecols): {followup_path} | shape={df_fu.shape}\")\n",
    "    else:\n",
    "        print(\"[WARN] follow_up.tsv encontrado, mas sem colunas esperadas de ID/tempo. Ignorando follow_up.\")\n",
    "else:\n",
    "    print(\"[WARN] follow_up.tsv não encontrado.\")\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Identificadores e campos de OS\n",
    "# ----------------------------\n",
    "id_col = pick_col(df_clin, [\"cases.submitter_id\",\"demographic.submitter_id\",\"cases.case_id\"])\n",
    "if id_col is None:\n",
    "    raise ValueError(\"Sem identificador (cases.submitter_id) no clinical.tsv.\")\n",
    "\n",
    "vital_col = pick_col(df_clin, [\"demographic.vital_status\",\"cases.vital_status\"])\n",
    "death_days_col = pick_col(df_clin, [\"demographic.days_to_death\",\"cases.diagnoses.days_to_death\",\"diagnoses.days_to_death\",\"cases.diagnoses.days_to_death\"])\n",
    "age_col = pick_col(df_clin, [\"demographic.age_at_index\",\"demographic.age_at_index\".replace(\"_\",\".\")])\n",
    "sex_col = pick_col(df_clin, [\"demographic.gender\",\"demographic.sex\",\"cases.gender\",\"cases.sex\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Estadiamento (TODAS as colunas plausíveis)\n",
    "# Observação: aqui só anexamos ao os_df; os filtros e análises são feitos depois.\n",
    "# ----------------------------\n",
    "stage_like = []\n",
    "for c in df_clin.columns:\n",
    "    lc = c.lower()\n",
    "    if (\"iss_stage\" in lc) or (\"r_iss\" in lc) or (\"riss\" in lc) or (\"revised\" in lc and \"iss\" in lc):\n",
    "        stage_like.append(c)\n",
    "    elif re.search(r\"(?:^|\\.|_)(stage|staging)(?:$|_|\\.)\", lc):\n",
    "        # inclui *stage* genérico (pode estar vazio; filtraremos depois)\n",
    "        stage_like.append(c)\n",
    "\n",
    "STAGE_COLUMNS_AVAILABLE = sorted(list(dict.fromkeys(stage_like)))\n",
    "\n",
    "# ----------------------------\n",
    "# Base OS\n",
    "# ----------------------------\n",
    "base = df_clin[[id_col]].copy().rename(columns={id_col:\"Participant_ID\"})\n",
    "base[\"Participant_ID\"] = base[\"Participant_ID\"].astype(str)\n",
    "\n",
    "vital = df_clin[vital_col].astype(str).str.strip().str.lower() if vital_col else pd.Series([pd.NA]*len(df_clin))\n",
    "days_to_death = pd.to_numeric(df_clin[death_days_col], errors=\"coerce\") if death_days_col else pd.Series([np.nan]*len(df_clin))\n",
    "\n",
    "if age_col and age_col in df_clin.columns:\n",
    "    base[\"age\"] = pd.to_numeric(df_clin[age_col], errors=\"coerce\")\n",
    "else:\n",
    "    base[\"age\"] = np.nan\n",
    "\n",
    "if sex_col and sex_col in df_clin.columns:\n",
    "    base[\"sex\"] = df_clin[sex_col]\n",
    "else:\n",
    "    base[\"sex\"] = pd.NA\n",
    "\n",
    "# adiciona colunas de estadiamento (raw)\n",
    "if STAGE_COLUMNS_AVAILABLE:\n",
    "    st = df_clin[[id_col] + STAGE_COLUMNS_AVAILABLE].copy()\n",
    "    st = st.rename(columns={id_col:\"Participant_ID\"})\n",
    "    st[\"Participant_ID\"] = st[\"Participant_ID\"].astype(str)\n",
    "\n",
    "    # resolve duplicatas por Participant_ID pegando o primeiro valor não-nulo (determinístico)\n",
    "    def _first_nonnull(s):\n",
    "        s2 = s.dropna()\n",
    "        if len(s2) == 0:\n",
    "            return pd.NA\n",
    "        return s2.iloc[0]\n",
    "\n",
    "    agg_map = {c: _first_nonnull for c in STAGE_COLUMNS_AVAILABLE}\n",
    "    st = st.groupby(\"Participant_ID\", as_index=False).agg(agg_map)\n",
    "    base = base.merge(st, on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "base[\"event\"] = ((vital==\"dead\") | days_to_death.notna()).astype(int)\n",
    "base[\"time\"]  = np.where(base[\"event\"].eq(1), days_to_death, np.nan)\n",
    "\n",
    "# follow-up para censura (sem imputação \"inventada\")\n",
    "if df_fu is not None:\n",
    "    fu_id_col = pick_col(df_fu, [\"cases.submitter_id\",\"cases.case_id\"])\n",
    "    fu_time_col = pick_col(df_fu, [\"follow_ups.days_to_follow_up\",\"follow_ups.days_to_last_follow_up\",\"follow_ups.days_to_followup\"])\n",
    "    if fu_id_col and fu_time_col:\n",
    "        fu_tmp = df_fu[[fu_id_col, fu_time_col]].copy().rename(columns={fu_id_col:\"Participant_ID\"})\n",
    "        fu_tmp[\"Participant_ID\"] = fu_tmp[\"Participant_ID\"].astype(str)\n",
    "        fu_tmp[fu_time_col] = pd.to_numeric(fu_tmp[fu_time_col], errors=\"coerce\")\n",
    "        fu_max = fu_tmp.groupby(\"Participant_ID\")[fu_time_col].max().rename(\"fu_time_max\").reset_index()\n",
    "        base = base.merge(fu_max, on=\"Participant_ID\", how=\"left\")\n",
    "    else:\n",
    "        base[\"fu_time_max\"] = np.nan\n",
    "        print(\"[WARN] follow_up.tsv sem colunas esperadas.\")\n",
    "else:\n",
    "    base[\"fu_time_max\"] = np.nan\n",
    "\n",
    "alive = base[\"event\"].eq(0)\n",
    "base.loc[alive, \"time\"] = base.loc[alive, \"fu_time_max\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Family history (opcional): agrega por paciente\n",
    "# ----------------------------\n",
    "if family_path is not None:\n",
    "    df_fh = pd.read_csv(family_path, sep=\"\\t\", low_memory=False)\n",
    "    print(f\"[OK] family_history.tsv: {family_path} | shape={df_fh.shape}\")\n",
    "    fh_id = pick_col(df_fh, [\"cases.submitter_id\",\"cases.case_id\"])\n",
    "    if fh_id:\n",
    "        fh = df_fh.copy()\n",
    "        fh = fh.rename(columns={fh_id:\"Participant_ID\"})\n",
    "        fh[\"Participant_ID\"] = fh[\"Participant_ID\"].astype(str)\n",
    "\n",
    "        # features simples e determinísticas (sem inventar)\n",
    "        if \"family_histories.relatives_with_cancer_history_count\" in fh.columns:\n",
    "            fh[\"fh_cancer_count\"] = pd.to_numeric(fh[\"family_histories.relatives_with_cancer_history_count\"], errors=\"coerce\")\n",
    "        else:\n",
    "            fh[\"fh_cancer_count\"] = np.nan\n",
    "\n",
    "        if \"family_histories.relative_with_cancer_history\" in fh.columns:\n",
    "            v = fh[\"family_histories.relative_with_cancer_history\"].astype(str).str.lower().str.strip()\n",
    "            fh[\"fh_any_cancer_history\"] = v.isin([\"yes\",\"true\",\"1\"]).astype(int)\n",
    "        else:\n",
    "            fh[\"fh_any_cancer_history\"] = np.nan\n",
    "\n",
    "        fh_agg = fh.groupby(\"Participant_ID\", as_index=False).agg({\n",
    "            \"fh_cancer_count\": \"max\",\n",
    "            \"fh_any_cancer_history\": \"max\"\n",
    "        })\n",
    "        base = base.merge(fh_agg, on=\"Participant_ID\", how=\"left\")\n",
    "else:\n",
    "    print(\"[INFO] family_history.tsv não encontrado (ok).\")\n",
    "\n",
    "# Exposure/pathology_detail existem às vezes vazios; apenas logamos\n",
    "if exposure_path is not None:\n",
    "    try:\n",
    "        df_ex = pd.read_csv(exposure_path, sep=\"\\t\", low_memory=False)\n",
    "        print(f\"[OK] exposure.tsv: {exposure_path} | shape={df_ex.shape} (não usado no OS por padrão)\")\n",
    "    except Exception:\n",
    "        print(\"[WARN] exposure.tsv encontrado, mas falhou leitura (ignorado).\")\n",
    "\n",
    "if pathology_detail_path is not None:\n",
    "    try:\n",
    "        df_pd = pd.read_csv(pathology_detail_path, sep=\"\\t\", low_memory=False)\n",
    "        print(f\"[OK] pathology_detail.tsv: {pathology_detail_path} | shape={df_pd.shape} (não usado no OS por padrão)\")\n",
    "    except Exception:\n",
    "        print(\"[WARN] pathology_detail.tsv encontrado, mas falhou leitura (ignorado).\")\n",
    "\n",
    "# ----------------------------\n",
    "# Final OS\n",
    "# ----------------------------\n",
    "os_df = base.copy()\n",
    "os_df[\"time\"] = pd.to_numeric(os_df[\"time\"], errors=\"coerce\")\n",
    "os_df = os_df[(os_df[\"time\"].notna()) & (os_df[\"time\"] >= 0)]\n",
    "os_df[\"Participant_ID\"] = os_df[\"Participant_ID\"].astype(str)\n",
    "os_df[\"__JOIN_ID__\"] = os_df[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "export_df_stats(os_df, \"os_df\")\n",
    "print(f\"[OK] os_df: n={len(os_df)} | event rate={os_df['event'].mean():.3f}\")\n",
    "print(f\"[OK] Estadiamento disponíveis (clinical): {len(STAGE_COLUMNS_AVAILABLE)} colunas\")\n",
    "os_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nq-SkrxSfE6n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "Nq-SkrxSfE6n",
    "outputId": "ea316ee9-5d7a-4e39-eb4b-baafbb5b46fa"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# (REFAZER INPUT CLÍNICO) OS/CLINICAL LOCKDOWN — auditável\n",
    "# Cria os_df paciente-nível com: age, demographic.gender, time, event, ISS etc.\n",
    "# =========================================================\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "def _find_first_existing(candidates):\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ---- localizar arquivos (prioriza /mnt/data) ----\n",
    "clinical_path = _find_first_existing([\n",
    "    \"/mnt/data/clinical.tsv\",\n",
    "    \"/content/clinical.tsv\",\n",
    "    os.path.join(os.getcwd(), \"clinical.tsv\"),\n",
    "])\n",
    "followup_path = _find_first_existing([\n",
    "    \"/mnt/data/follow_up.tsv\",\n",
    "    \"/content/follow_up.tsv\",\n",
    "    os.path.join(os.getcwd(), \"follow_up.tsv\"),\n",
    "])\n",
    "\n",
    "if clinical_path is None:\n",
    "    raise FileNotFoundError(\"Não achei clinical.tsv em /mnt/data nem /content.\")\n",
    "if followup_path is None:\n",
    "    raise FileNotFoundError(\"Não achei follow_up.tsv em /mnt/data nem /content.\")\n",
    "\n",
    "# ---- diretórios de saída ----\n",
    "OUT_DIR = globals().get(\"OUT_DIR\") or os.path.join(os.getcwd(), \"outputs\")\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or os.path.join(OUT_DIR, \"results\")\n",
    "LOG_DIR = globals().get(\"LOG_DIR\") or os.path.join(OUT_DIR, \"logs\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# ---- ler header do clinical para capturar todos os stage cols ----\n",
    "with open(clinical_path, \"r\") as f:\n",
    "    header = f.readline().strip().split(\"\\t\")\n",
    "\n",
    "stage_cols = [c for c in header if c.startswith(\"diagnoses.\") and c.endswith(\"_stage\")]\n",
    "\n",
    "essential_cols = [\n",
    "    \"cases.submitter_id\",\n",
    "    \"demographic.age_at_index\",\n",
    "    \"demographic.gender\",\n",
    "    \"demographic.vital_status\",\n",
    "    \"demographic.days_to_death\",\n",
    "    \"diagnoses.days_to_last_follow_up\",\n",
    "]\n",
    "usecols = list(dict.fromkeys(essential_cols + stage_cols))\n",
    "\n",
    "clinical_df = pd.read_csv(clinical_path, sep=\"\\t\", usecols=usecols, low_memory=False)\n",
    "\n",
    "# ---- limpeza de NA-like (ex.: '--') sem inventar dado ----\n",
    "na_like = {\"'--\", \"--\", \"NA\", \"N/A\", \"NaN\", \"nan\", \"\", \"None\", \"null\", \"NULL\"}\n",
    "\n",
    "def _clean_obj(s):\n",
    "    # mantém strings reais, troca placeholders por NaN\n",
    "    s = s.astype(str).replace(list(na_like), np.nan)\n",
    "    return s\n",
    "\n",
    "for c in clinical_df.columns:\n",
    "    if clinical_df[c].dtype == object:\n",
    "        clinical_df[c] = _clean_obj(clinical_df[c])\n",
    "\n",
    "clinical_df[\"demographic.age_at_index\"] = pd.to_numeric(clinical_df[\"demographic.age_at_index\"], errors=\"coerce\")\n",
    "clinical_df[\"demographic.days_to_death\"] = pd.to_numeric(clinical_df[\"demographic.days_to_death\"], errors=\"coerce\")\n",
    "clinical_df[\"diagnoses.days_to_last_follow_up\"] = pd.to_numeric(clinical_df[\"diagnoses.days_to_last_follow_up\"], errors=\"coerce\")\n",
    "\n",
    "# ---- agregação paciente-nível (995 pacientes) ----\n",
    "def first_valid(x):\n",
    "    x = x.dropna()\n",
    "    return x.iloc[0] if len(x) else np.nan\n",
    "\n",
    "agg = {\n",
    "    \"demographic.age_at_index\": \"first\",\n",
    "    \"demographic.gender\": first_valid,\n",
    "    \"demographic.vital_status\": first_valid,\n",
    "    \"demographic.days_to_death\": \"max\",\n",
    "    \"diagnoses.days_to_last_follow_up\": \"max\",\n",
    "}\n",
    "for sc in stage_cols:\n",
    "    agg[sc] = first_valid\n",
    "\n",
    "pat_df = clinical_df.groupby(\"cases.submitter_id\", as_index=False).agg(agg)\n",
    "pat_df = pat_df.rename(columns={\"cases.submitter_id\": \"Participant_ID\"})\n",
    "\n",
    "# ---- follow_up: max days_to_follow_up por paciente (chunked) ----\n",
    "usecols_fu = [\"cases.submitter_id\", \"follow_ups.days_to_follow_up\"]\n",
    "max_fu = {}\n",
    "for chunk in pd.read_csv(followup_path, sep=\"\\t\", usecols=usecols_fu, chunksize=200000, low_memory=False):\n",
    "    sid = chunk[\"cases.submitter_id\"].astype(str)\n",
    "    days = pd.to_numeric(chunk[\"follow_ups.days_to_follow_up\"], errors=\"coerce\")\n",
    "    m = days.groupby(sid).max()\n",
    "    for k, v in m.items():\n",
    "        if pd.isna(v):\n",
    "            continue\n",
    "        prev = max_fu.get(k)\n",
    "        if prev is None or v > prev:\n",
    "            max_fu[k] = float(v)\n",
    "\n",
    "pat_df[\"follow_ups.days_to_follow_up_max\"] = pat_df[\"Participant_ID\"].map(max_fu)\n",
    "\n",
    "# ---- construir OS (time/event) ----\n",
    "pat_df[\"follow_up_time\"] = pat_df[[\"follow_ups.days_to_follow_up_max\", \"diagnoses.days_to_last_follow_up\"]].max(axis=1, skipna=True)\n",
    "\n",
    "pat_df[\"event\"] = (\n",
    "    (pat_df[\"demographic.vital_status\"].astype(str).str.lower() == \"dead\") |\n",
    "    (pat_df[\"demographic.days_to_death\"].notna())\n",
    ").astype(int)\n",
    "\n",
    "pat_df[\"time\"] = np.where(\n",
    "    pat_df[\"event\"] == 1,\n",
    "    pat_df[\"demographic.days_to_death\"],\n",
    "    pat_df[\"follow_up_time\"]\n",
    ")\n",
    "\n",
    "# compatibilidade com o resto do pipeline\n",
    "pat_df[\"__JOIN_ID__\"] = pat_df[\"Participant_ID\"].astype(str)\n",
    "pat_df[\"age\"] = pat_df[\"demographic.age_at_index\"]\n",
    "\n",
    "# ---- define os_df final (mantém demographic.gender no nome original) ----\n",
    "os_df = pat_df.copy()\n",
    "\n",
    "# ---- auditoria rápida ----\n",
    "audit = {\n",
    "    \"n_patients\": int(os_df.shape[0]),\n",
    "    \"n_dead\": int(os_df[\"event\"].sum()),\n",
    "    \"n_alive\": int((1 - os_df[\"event\"]).sum()),\n",
    "    \"age_non_null\": int(os_df[\"age\"].notna().sum()),\n",
    "    \"gender_non_null\": int(os_df[\"demographic.gender\"].notna().sum()),\n",
    "    \"iss_non_null\": int(os_df[\"diagnoses.iss_stage\"].notna().sum()) if \"diagnoses.iss_stage\" in os_df.columns else 0\n",
    "}\n",
    "\n",
    "audit_path = os.path.join(LOG_DIR, \"clinical_input_audit.txt\")\n",
    "with open(audit_path, \"w\") as f:\n",
    "    for k, v in audit.items():\n",
    "        f.write(f\"{k}\\t{v}\\n\")\n",
    "\n",
    "out_os = os.path.join(RES_DIR, \"os_df_patient_level.tsv\")\n",
    "os_df.to_csv(out_os, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"[OK] os_df reconstruído (paciente-nível).\")\n",
    "print(\"[OK] salvo:\", out_os)\n",
    "print(\"[OK] audit:\", audit_path)\n",
    "print(\"Colunas-chave presentes?\",\n",
    "      {\"age\": \"age\" in os_df.columns,\n",
    "       \"demographic.gender\": \"demographic.gender\" in os_df.columns,\n",
    "       \"time\": \"time\" in os_df.columns,\n",
    "       \"event\": \"event\" in os_df.columns,\n",
    "       \"ISS\": \"diagnoses.iss_stage\" in os_df.columns})\n",
    "os_df[[\"Participant_ID\",\"__JOIN_ID__\",\"time\",\"event\",\"age\",\"demographic.gender\",\"diagnoses.iss_stage\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vY6NzW_qfQTa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vY6NzW_qfQTa",
    "outputId": "40c746d9-1772-4eb7-c8e1-ca333cf145e4"
   },
   "outputs": [],
   "source": [
    "print(\"os_df shape:\", os_df.shape)\n",
    "print(\"\\nGender:\")\n",
    "print(os_df[\"demographic.gender\"].value_counts(dropna=False))\n",
    "\n",
    "if \"diagnoses.iss_stage\" in os_df.columns:\n",
    "    print(\"\\nISS:\")\n",
    "    print(os_df[\"diagnoses.iss_stage\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nEventos:\")\n",
    "print(os_df[\"event\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1a578",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "16f1a578",
    "outputId": "3034c029-e49c-427d-ab34-309b6975a866"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 14) MERGE FEATURES ↔ OS (SEM NA→0) — ROBUSTO A NOMES DE COLUNAS\n",
    "# =========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- diretório de resultados (compatível com variações) ---\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or globals().get(\"OUT_RESULTS_DIR\")\n",
    "if RES_DIR is None:\n",
    "    RES_DIR = os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# --- colunas obrigatórias ---\n",
    "req = [\"Participant_ID\", \"time\", \"event\"]\n",
    "missing = [c for c in req if c not in os_df.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"os_df sem colunas obrigatórias: {missing}\")\n",
    "\n",
    "# --- tenta achar a coluna de idade e sexo/gênero sem forçar nome ---\n",
    "age_candidates = [\"age\", \"Age\", \"age_at_index\", \"cases.demographic.age_at_index\"]\n",
    "sex_candidates = [\"demographic.gender\", \"gender\", \"sex\", \"Sex\", \"demographic.sex\", \"cases.demographic.gender\", \"cases.demographic.sex\"]\n",
    "\n",
    "age_col = next((c for c in age_candidates if c in os_df.columns), None)\n",
    "sex_col = next((c for c in sex_candidates if c in os_df.columns), None)\n",
    "\n",
    "# __JOIN_ID__ é opcional (depende de como você montou o join)\n",
    "join_col = \"__JOIN_ID__\" if \"__JOIN_ID__\" in os_df.columns else None\n",
    "\n",
    "base_cols = [\"Participant_ID\"]\n",
    "if join_col: base_cols.append(join_col)\n",
    "base_cols += [\"time\", \"event\"]\n",
    "if age_col: base_cols.append(age_col)\n",
    "if sex_col: base_cols.append(sex_col)\n",
    "\n",
    "base = os_df[base_cols].copy()\n",
    "\n",
    "# padroniza nomes (sem criar valores)\n",
    "rename_map = {}\n",
    "if age_col and age_col != \"age\":\n",
    "    rename_map[age_col] = \"age\"\n",
    "if sex_col and sex_col != \"sex\":\n",
    "    rename_map[sex_col] = \"sex\"\n",
    "base = base.rename(columns=rename_map)\n",
    "\n",
    "# --- merges: não depende de __JOIN_ID__ nos dfs de métricas ---\n",
    "def safe_drop(df, col):\n",
    "    return df.drop(columns=[col]) if col in df.columns else df\n",
    "\n",
    "features_df = (\n",
    "    base\n",
    "    .merge(safe_drop(bp_metrics_df, \"__JOIN_ID__\"), on=\"Participant_ID\", how=\"left\")\n",
    "    .merge(safe_drop(hotspot_metrics_df, \"__JOIN_ID__\"), on=\"Participant_ID\", how=\"left\")\n",
    ")\n",
    "\n",
    "export_df_stats(features_df, \"features_df_merged\")\n",
    "\n",
    "merged_path = os.path.join(RES_DIR, \"survival_features_merged.tsv\")\n",
    "features_df.to_csv(merged_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[OK] salvo: {merged_path}\")\n",
    "print(\"[INFO] colunas base usadas:\", base_cols)\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aTBEF_Q5m8hZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "aTBEF_Q5m8hZ",
    "outputId": "8a623beb-e43e-49fa-e07a-1cc9525f8596"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 15A) SEX DIFFERENCES — Mann-Whitney U + FDR (BH) + export\n",
    "#     (Reproducible for Supplementary Table)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# -------------------------\n",
    "# Config de saída\n",
    "# -------------------------\n",
    "# Usa RES_DIR (do notebook) se existir; senão, cai em /content/results\n",
    "_RES_DIR = None\n",
    "try:\n",
    "    _RES_DIR = Path(RES_DIR)  # noqa: F821  (RES_DIR vem de célula anterior no notebook)\n",
    "    _RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "except Exception:\n",
    "    _RES_DIR = Path(\"/content/results\")\n",
    "    _RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 1) normaliza sexo/gênero para male/female\n",
    "# -------------------------\n",
    "def normalize_sex(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    v = str(x).strip().lower()\n",
    "    male_set   = {\"male\",\"m\",\"masc\",\"masculino\",\"homem\",\"man\"}\n",
    "    female_set = {\"female\",\"f\",\"fem\",\"feminino\",\"mulher\",\"woman\"}\n",
    "    if v in male_set:\n",
    "        return \"male\"\n",
    "    if v in female_set:\n",
    "        return \"female\"\n",
    "    return np.nan  # ignora outros rótulos/valores\n",
    "\n",
    "df_sex = features_df.copy()\n",
    "\n",
    "if \"sex\" not in df_sex.columns:\n",
    "    raise RuntimeError(\"features_df não contém a coluna 'sex'. Verifique o merge/renomeação no pipeline.\")\n",
    "\n",
    "df_sex[\"sex_norm\"] = df_sex[\"sex\"].apply(normalize_sex)\n",
    "\n",
    "# -------------------------\n",
    "# 2) métricas a testar (ajuste se quiser incluir mais)\n",
    "# -------------------------\n",
    "sex_metrics = [\n",
    "    \"BP_total\",\n",
    "    \"BP_unique\",\n",
    "    \"HotspotCount_P95\",\n",
    "    \"HotspotScore_topK\",\n",
    "    \"HotspotEntropy\",\n",
    "]\n",
    "\n",
    "# mantém só métricas existentes no dataframe\n",
    "sex_metrics = [m for m in sex_metrics if m in df_sex.columns]\n",
    "if not sex_metrics:\n",
    "    raise RuntimeError(\"Nenhuma das métricas esperadas foi encontrada em features_df.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) descritivas (mean±sd e mediana [Q1–Q3]) por sexo\n",
    "# -------------------------\n",
    "def _q1(x): return x.quantile(0.25)\n",
    "def _q3(x): return x.quantile(0.75)\n",
    "\n",
    "desc_rows = []\n",
    "for metric in sex_metrics:\n",
    "    tmp = df_sex[[\"sex_norm\", metric]].dropna().copy()\n",
    "    tmp[metric] = pd.to_numeric(tmp[metric], errors=\"coerce\")\n",
    "    tmp = tmp.dropna()\n",
    "\n",
    "    male = tmp.loc[tmp[\"sex_norm\"] == \"male\", metric]\n",
    "    female = tmp.loc[tmp[\"sex_norm\"] == \"female\", metric]\n",
    "\n",
    "    desc_rows.append({\n",
    "        \"metric\": metric,\n",
    "        \"n_male\": int(male.shape[0]),\n",
    "        \"n_female\": int(female.shape[0]),\n",
    "        \"mean_male\": float(male.mean()) if male.shape[0] else np.nan,\n",
    "        \"sd_male\": float(male.std(ddof=1)) if male.shape[0] > 1 else np.nan,\n",
    "        \"mean_female\": float(female.mean()) if female.shape[0] else np.nan,\n",
    "        \"sd_female\": float(female.std(ddof=1)) if female.shape[0] > 1 else np.nan,\n",
    "        \"median_male\": float(male.median()) if male.shape[0] else np.nan,\n",
    "        \"q1_male\": float(_q1(male)) if male.shape[0] else np.nan,\n",
    "        \"q3_male\": float(_q3(male)) if male.shape[0] else np.nan,\n",
    "        \"median_female\": float(female.median()) if female.shape[0] else np.nan,\n",
    "        \"q1_female\": float(_q1(female)) if female.shape[0] else np.nan,\n",
    "        \"q3_female\": float(_q3(female)) if female.shape[0] else np.nan,\n",
    "    })\n",
    "\n",
    "desc_sex = pd.DataFrame(desc_rows)\n",
    "desc_path = _RES_DIR / \"supp_table_sex_descriptives_mean_sd_median_iqr.tsv\"\n",
    "desc_sex.to_csv(desc_path, sep=\"\\t\", index=False)\n",
    "print(f\"[OK] Descritivas por sexo (mean±sd e mediana[Q1–Q3]) salvas em: {desc_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) BH-FDR (Benjamini–Hochberg) sem depender de statsmodels\n",
    "# -------------------------\n",
    "def bh_fdr(pvals):\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    n = len(pvals)\n",
    "    order = np.argsort(pvals)\n",
    "    ranked = pvals[order]\n",
    "    q = ranked * n / (np.arange(n) + 1)\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    q = np.clip(q, 0, 1)\n",
    "    out = np.empty_like(q)\n",
    "    out[order] = q\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# 5) Mann-Whitney U + efeito (rank-biserial)\n",
    "# -------------------------\n",
    "rows = []\n",
    "for metric in sex_metrics:\n",
    "    tmp = df_sex[[\"sex_norm\", metric]].dropna().copy()\n",
    "    tmp[metric] = pd.to_numeric(tmp[metric], errors=\"coerce\")\n",
    "    tmp = tmp.dropna()\n",
    "\n",
    "    male = tmp.loc[tmp[\"sex_norm\"] == \"male\", metric].values\n",
    "    female = tmp.loc[tmp[\"sex_norm\"] == \"female\", metric].values\n",
    "\n",
    "    # requer ao menos 5 por grupo (ajuste se quiser)\n",
    "    if (len(male) < 5) or (len(female) < 5):\n",
    "        rows.append({\n",
    "            \"metric\": metric,\n",
    "            \"n_male\": int(len(male)),\n",
    "            \"n_female\": int(len(female)),\n",
    "            \"U\": np.nan,\n",
    "            \"p_value\": np.nan,\n",
    "            \"effect_rank_biserial\": np.nan,\n",
    "            \"median_male\": np.nan if len(male)==0 else float(np.median(male)),\n",
    "            \"median_female\": np.nan if len(female)==0 else float(np.median(female)),\n",
    "            \"note\": \"insuficiente_n\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # mannwhitneyu: U é para o primeiro grupo (male)\n",
    "    U, p = mannwhitneyu(male, female, alternative=\"two-sided\", method=\"auto\")\n",
    "\n",
    "    # rank-biserial: [-1, 1]\n",
    "    # positivo => male tende a ser maior\n",
    "    n1, n2 = len(male), len(female)\n",
    "    r_rb = (2.0 * U) / (n1 * n2) - 1.0\n",
    "\n",
    "    rows.append({\n",
    "        \"metric\": metric,\n",
    "        \"n_male\": int(n1),\n",
    "        \"n_female\": int(n2),\n",
    "        \"median_male\": float(np.median(male)),\n",
    "        \"median_female\": float(np.median(female)),\n",
    "        \"U\": float(U),\n",
    "        \"p_value\": float(p),\n",
    "        \"effect_rank_biserial\": float(r_rb),\n",
    "        \"note\": \"\"\n",
    "    })\n",
    "\n",
    "res_sex = pd.DataFrame(rows)\n",
    "\n",
    "# FDR apenas nas linhas com p válido\n",
    "mask = res_sex[\"p_value\"].notna()\n",
    "res_sex.loc[mask, \"q_value_bh\"] = bh_fdr(res_sex.loc[mask, \"p_value\"].values)\n",
    "res_sex.loc[~mask, \"q_value_bh\"] = np.nan\n",
    "\n",
    "# export\n",
    "mw_path = _RES_DIR / \"supp_table_sex_mannwhitney.tsv\"\n",
    "res_sex.sort_values(\"p_value\", na_position=\"last\").to_csv(mw_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[OK] Mann-Whitney por sexo + BH-FDR salvo em: {mw_path}\")\n",
    "\n",
    "# preview (top 10 por p)\n",
    "display(res_sex.sort_values(\"p_value\", na_position=\"last\").head(10))\n",
    "\n",
    "# Opcional: tabela combinada (descritivas + p/q) para facilitar citação\n",
    "try:\n",
    "    combined = desc_sex.merge(res_sex[[\"metric\", \"p_value\", \"q_value_bh\", \"effect_rank_biserial\"]], on=\"metric\", how=\"left\")\n",
    "    comb_path = _RES_DIR / \"supp_table_sex_combined.tsv\"\n",
    "    combined.to_csv(comb_path, sep=\"\\t\", index=False)\n",
    "    print(f\"[OK] Tabela combinada (descritivas + p/q) salva em: {comb_path}\")\n",
    "except Exception as _e:\n",
    "    print(\"[WARN] Could not generate combined table:\", _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e75d6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "25e75d6d",
    "outputId": "2c9d41cc-0b84-4417-acdf-4fdacd89e7e2"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 15) COX (BREAKPOINTS/HOTSPOTS) — ROBUSTO A SEX/GENDER\n",
    "# =========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# diretório resultados\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or globals().get(\"OUT_RESULTS_DIR\")\n",
    "if RES_DIR is None:\n",
    "    RES_DIR = os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# validações\n",
    "for c in [\"time\",\"event\"]:\n",
    "    if c not in features_df.columns:\n",
    "        raise RuntimeError(f\"features_df sem coluna obrigatória: {c}\")\n",
    "\n",
    "# escolhe lista de métricas automaticamente (tudo numérico exceto colunas-base)\n",
    "base_exclude = {\"Participant_ID\",\"__JOIN_ID__\",\"time\",\"event\",\"age\",\"sex\"}\n",
    "metrics = [c for c in features_df.columns\n",
    "           if c not in base_exclude and pd.api.types.is_numeric_dtype(features_df[c])]\n",
    "\n",
    "if len(metrics) == 0:\n",
    "    raise RuntimeError(\"Nenhuma métrica numérica encontrada em features_df para rodar Cox.\")\n",
    "\n",
    "# prepara covariáveis\n",
    "covars = []\n",
    "if \"age\" in features_df.columns:\n",
    "    covars.append(\"age\")\n",
    "\n",
    "# sexo opcional: cria dummies se existir\n",
    "df_sex = None\n",
    "sex_dummy_cols = []\n",
    "if \"sex\" in features_df.columns:\n",
    "    df_sex = features_df[[\"sex\"]].copy()\n",
    "    df_sex[\"sex\"] = df_sex[\"sex\"].astype(str)\n",
    "    dummies = pd.get_dummies(df_sex[\"sex\"], prefix=\"sex\", drop_first=True)\n",
    "    sex_dummy_cols = dummies.columns.tolist()\n",
    "    # anexa dummies ao dataframe\n",
    "    features_for_models = pd.concat([features_df.drop(columns=[\"sex\"]), dummies], axis=1)\n",
    "else:\n",
    "    features_for_models = features_df.copy()\n",
    "\n",
    "covars += sex_dummy_cols\n",
    "\n",
    "rows = []\n",
    "cph = CoxPHFitter()\n",
    "\n",
    "for m in metrics:\n",
    "    cols = [\"time\",\"event\", m] + covars\n",
    "    dfm = features_for_models[cols].copy()\n",
    "\n",
    "    # SEM imputar: remove NA nas colunas usadas\n",
    "    dfm = dfm.dropna(subset=[m,\"time\",\"event\"] + covars)\n",
    "\n",
    "    # evita modelos fracos\n",
    "    if len(dfm) < 50 or dfm[\"event\"].sum() < 10:\n",
    "        continue\n",
    "\n",
    "    # HR por +1 SD (padronização) — só na métrica\n",
    "    sd = dfm[m].std()\n",
    "    if not np.isfinite(sd) or sd == 0:\n",
    "        continue\n",
    "    dfm[m] = (dfm[m] - dfm[m].mean()) / sd\n",
    "\n",
    "    try:\n",
    "        cph.fit(dfm, duration_col=\"time\", event_col=\"event\")\n",
    "        s = cph.summary.loc[m]\n",
    "\n",
    "        # C-index (opcional, no mesmo set)\n",
    "        risk = cph.predict_partial_hazard(dfm).values.ravel()\n",
    "        cidx = concordance_index(dfm[\"time\"].values, -risk, dfm[\"event\"].values)\n",
    "\n",
    "        rows.append({\n",
    "            \"metric\": m,\n",
    "            \"N\": int(dfm.shape[0]),\n",
    "            \"events\": int(dfm[\"event\"].sum()),\n",
    "            \"HR_per_1SD\": float(np.exp(s[\"coef\"])),\n",
    "            \"CI95_low\": float(np.exp(s[\"coef lower 95%\"])),\n",
    "            \"CI95_high\": float(np.exp(s[\"coef upper 95%\"])),\n",
    "            \"p\": float(s[\"p\"]),\n",
    "            \"c_index_in_sample\": float(cidx)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"metric\": m,\n",
    "            \"N\": int(dfm.shape[0]),\n",
    "            \"events\": int(dfm[\"event\"].sum()),\n",
    "            \"HR_per_1SD\": np.nan,\n",
    "            \"CI95_low\": np.nan,\n",
    "            \"CI95_high\": np.nan,\n",
    "            \"p\": np.nan,\n",
    "            \"c_index_in_sample\": np.nan,\n",
    "            \"error\": str(e)[:180]\n",
    "        })\n",
    "\n",
    "res = pd.DataFrame(rows)\n",
    "if res.empty:\n",
    "    raise RuntimeError(\"Nenhum modelo Cox foi ajustado (poucos eventos ou métricas inválidas).\")\n",
    "\n",
    "# FDR BH\n",
    "res = res.sort_values(\"p\", na_position=\"last\").reset_index(drop=True)\n",
    "pvals = res[\"p\"].values.astype(float)\n",
    "mask = np.isfinite(pvals)\n",
    "q = np.full_like(pvals, np.nan, dtype=float)\n",
    "if mask.sum() > 0:\n",
    "    pv = pvals[mask]\n",
    "    order = np.argsort(pv)\n",
    "    ranked = pv[order]\n",
    "    mtests = len(ranked)\n",
    "    bh = ranked * mtests / (np.arange(1, mtests+1))\n",
    "    bh = np.minimum.accumulate(bh[::-1])[::-1]\n",
    "    q_tmp = np.empty_like(ranked)\n",
    "    q_tmp[order] = np.clip(bh, 0, 1)\n",
    "    q[mask] = q_tmp\n",
    "res[\"q_FDR_BH\"] = q\n",
    "\n",
    "out_path = os.path.join(RES_DIR, \"cox_results_breakpoints_hotspots.tsv\")\n",
    "res.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "print(f\"[OK] Cox salvo: {out_path}\")\n",
    "res.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pp9kT5Q8gEft",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Pp9kT5Q8gEft",
    "outputId": "e937418c-3979-4c62-9c57-9ec14fa3d89f"
   },
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or globals().get(\"OUT_RESULTS_DIR\")\n",
    "if RES_DIR is None:\n",
    "    RES_DIR = os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, \"cox_results_breakpoints_hotspots.tsv\")\n",
    "print(\"[CHECK] procurando:\", cox_path, \"| existe?\", os.path.exists(cox_path))\n",
    "\n",
    "if os.path.exists(cox_path):\n",
    "    cox_df = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "    print(\"[CHECK] cox_df shape:\", cox_df.shape)\n",
    "    print(\"[CHECK] colunas:\", list(cox_df.columns))\n",
    "    print(\"[CHECK] p non-null:\", cox_df[\"p\"].notna().sum() if \"p\" in cox_df.columns else \"NA\")\n",
    "    print(\"[CHECK] q non-null:\", cox_df[\"q_FDR_BH\"].notna().sum() if \"q_FDR_BH\" in cox_df.columns else \"NA\")\n",
    "    display(cox_df.head(10))\n",
    "else:\n",
    "    print(\"[WARN] Não existe arquivo de Cox. A célula 15 não salvou output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c39ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c7c39ba",
    "outputId": "ca53ddad-1249-4e67-af18-0c0d4e4eb767"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 15b) KM PLOTS (High vs Low por mediana) — métricas top\n",
    "# =========================================================\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "\n",
    "sig = []\n",
    "if \"cox_df\" in globals() and isinstance(cox_df, pd.DataFrame) and not cox_df.empty:\n",
    "    if \"q_FDR_BH\" in cox_df.columns:\n",
    "        sig = cox_df[cox_df[\"q_FDR_BH\"] <= 0.10][\"metric\"].tolist()\n",
    "    if not sig:\n",
    "        sig = cox_df.head(3)[\"metric\"].tolist()\n",
    "\n",
    "if not sig:\n",
    "    print(\"[INFO] Sem métricas para KM (cox_df vazio).\")\n",
    "else:\n",
    "    km_dir = os.path.join(RES_DIR, \"km_plots\")\n",
    "    os.makedirs(km_dir, exist_ok=True)\n",
    "\n",
    "    for m in sig:\n",
    "        dfm = features_df[[\"time\",\"event\", m]].dropna(subset=[m,\"time\",\"event\"]).copy()\n",
    "        if len(dfm) < 50:\n",
    "            print(f\"[WARN] {m}: n<50. Pulando KM.\")\n",
    "            continue\n",
    "        med = dfm[m].median()\n",
    "        dfm[\"group\"] = np.where(dfm[m] >= med, \"High\", \"Low\")\n",
    "\n",
    "        fig = plt.figure(figsize=(7,5))\n",
    "        ax = plt.gca()\n",
    "\n",
    "        kmf_low = KaplanMeierFitter()\n",
    "        kmf_high = KaplanMeierFitter()\n",
    "\n",
    "        low = dfm[\"group\"].eq(\"Low\")\n",
    "        high = dfm[\"group\"].eq(\"High\")\n",
    "\n",
    "        kmf_low.fit(dfm.loc[low,\"time\"], event_observed=dfm.loc[low,\"event\"], label=\"Low\")\n",
    "        kmf_high.fit(dfm.loc[high,\"time\"], event_observed=dfm.loc[high,\"event\"], label=\"High\")\n",
    "        kmf_low.plot_survival_function(ax=ax)\n",
    "        kmf_high.plot_survival_function(ax=ax)\n",
    "\n",
    "        lr = logrank_test(dfm.loc[high,\"time\"], dfm.loc[low,\"time\"],\n",
    "                          event_observed_A=dfm.loc[high,\"event\"], event_observed_B=dfm.loc[low,\"event\"])\n",
    "\n",
    "        ax.set_title(f\"KM — {m} (median split)\\nlog-rank p={lr.p_value:.3g}\")\n",
    "        ax.set_xlabel(\"Time (days)\")\n",
    "        ax.set_ylabel(\"Survival probability\")\n",
    "\n",
    "        add_at_risk_counts(kmf_low, kmf_high, ax=ax)\n",
    "        outp = os.path.join(km_dir, f\"KM_{m}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outp, dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] KM plots → {km_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16da81",
   "metadata": {
    "id": "be16da81"
   },
   "source": [
    "## Clustering (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2f917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46c2f917",
    "outputId": "77c0bab7-ed49-4bb8-c1b1-65dacb45dcc9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 16) CLUSTERING (OPCIONAL) — determinístico + export\n",
    "# =========================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_features = [\"BP_total\",\"BP_unique\",\"BP_entropy_chr\",\"BP_gini_chr\",\"HotspotCount_P95\",\"HotspotScore_topK\",\"HotspotEntropy\"]\n",
    "\n",
    "Xdf = features_df[[\"Participant_ID\"] + cluster_features].dropna().copy()\n",
    "if len(Xdf) < 50:\n",
    "    print(f\"[WARN] n={len(Xdf)} insuficiente para clustering.\")\n",
    "else:\n",
    "    X = np.log1p(Xdf[cluster_features].values.astype(float))\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "    best_k, best_s = None, -1\n",
    "    for k in range(2, 7):\n",
    "        labels = AgglomerativeClustering(n_clusters=k, linkage=\"ward\").fit_predict(Xs)\n",
    "        s = silhouette_score(Xs, labels)\n",
    "        if s > best_s:\n",
    "            best_s, best_k = s, k\n",
    "\n",
    "    labels = AgglomerativeClustering(n_clusters=best_k, linkage=\"ward\").fit_predict(Xs)\n",
    "    clusters_df = Xdf[[\"Participant_ID\"]].copy()\n",
    "    clusters_df[\"cluster\"] = labels.astype(int)\n",
    "    clusters_df[\"__JOIN_ID__\"] = clusters_df[\"Participant_ID\"].astype(\"string\")\n",
    "\n",
    "    clus_path = os.path.join(RES_DIR, \"cluster_assignments.tsv\")\n",
    "    clusters_df.to_csv(clus_path, sep=\"\\t\", index=False)\n",
    "    export_df_stats(clusters_df, \"clusters_df\")\n",
    "\n",
    "    print(f\"[OK] clustering k={best_k}, silhouette={best_s:.3f}\")\n",
    "    clusters_df[\"cluster\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2cca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17e2cca6",
    "outputId": "6fdb844d-04ce-4fe4-da3a-ca01b64dd465"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 17) SURVIVAL POR CLUSTER (se clusters_df existir)\n",
    "# =========================================================\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "\n",
    "if \"clusters_df\" not in globals() or clusters_df is None or clusters_df.empty:\n",
    "    print(\"[INFO] clusters_df não disponível.\")\n",
    "else:\n",
    "    dfc = os_df.merge(clusters_df[[\"Participant_ID\",\"cluster\"]], on=\"Participant_ID\", how=\"inner\")\n",
    "    if dfc[\"cluster\"].nunique() < 2:\n",
    "        print(\"[INFO] Apenas 1 cluster após join.\")\n",
    "    else:\n",
    "        res = multivariate_logrank_test(dfc[\"time\"], dfc[\"cluster\"], dfc[\"event\"])\n",
    "        outp = os.path.join(RES_DIR, \"cluster_survival_logrank.txt\")\n",
    "        with open(outp, \"w\") as f:\n",
    "            f.write(f\"multivariate_logrank_test p={res.p_value}\\n\")\n",
    "            f.write(str(res.summary))\n",
    "        print(f\"[OK] log-rank por cluster → {outp}\")\n",
    "        print(res.summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6cd949",
   "metadata": {
    "id": "5d6cd949"
   },
   "source": [
    "# End — main outputs\n",
    "\n",
    "- `processed/file_metadata.tsv`\n",
    "- `processed/cnv_segments.parquet`\n",
    "- `results/cnv_recurrence_by_cytoband.tsv`\n",
    "- `results/cnv_recurrence_by_bins_1Mb.tsv`\n",
    "- `results/cox_results_breakpoints_hotspots.tsv`\n",
    "- `results/cluster_assignments.tsv` (if executed)\n",
    "- `results/top*_recurrent_regions_by_bins_1Mb.tsv` (if executed)\n",
    "- `results/top*_recurrent_regions_cox_table.tsv` (if executed)\n",
    "- `results/recurrent_cnv_km_plots/` (if executed)\n",
    "- `results/prediction_dataset_top*_bins1Mb.tsv` (if executed)\n",
    "- `results/pred_survival_*` and `results/pred_iss_*` (if executed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XIpo5v60sszg",
   "metadata": {
    "id": "XIpo5v60sszg"
   },
   "source": [
    "## Recurrent CNV module (legacy-compatible): top CNVs + adjusted Cox + KM + ISS stratification\n",
    "\n",
    "This block provides a **legacy-compatible** analysis set:\n",
    "- identifies **top recurrent CNVs** (exact coordinates) and exports recurrence tables;\n",
    "- runs **multivariable Cox** per CNV (adjusted for age and sex);\n",
    "- generates **Kaplan–Meier** plots for selected CNVs;\n",
    "- optional: **ISS stratification** when ISS is available in `clinical.tsv`;\n",
    "- optional: a **circos-style** visualization (only if the library is available; execution does not fail otherwise).\n",
    "\n",
    "> Note: exact-coordinate recurrence depends on segmentation breakpoints; therefore the pipeline also keeps the more robust recurrence summaries by **cytoband** and by **1Mb bins**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YJw4PGTlsszg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "YJw4PGTlsszg",
    "outputId": "a9946c62-cf4e-4fad-f899-481d42ae2126"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 18) RECURRENT CNV: selecionar TOP regiões por bins (1Mb)\n",
    "#     (AGORA filtrando CNVs \"plotáveis\" p/ KM: ambos grupos >= KM_MIN_GROUP)\n",
    "# =========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROC_DIR = globals().get(\"PROC_DIR\", os.path.join(BASE_OUT_DIR, \"processed\"))\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "BIN_SIZE = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "KM_MIN_GROUP = int(globals().get(\"KM_MIN_GROUP\", 20))\n",
    "\n",
    "freq_path  = os.path.join(RES_DIR, f\"cnv_recurrence_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "top_path   = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "edges_path = os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "\n",
    "if not os.path.exists(freq_path):\n",
    "    raise FileNotFoundError(\"Falta cnv_recurrence_by_bins_1Mb.tsv. Rode a etapa de recorrência antes.\")\n",
    "\n",
    "freq = pd.read_csv(freq_path, sep=\"\\t\")\n",
    "\n",
    "# N da coorte CNV (mesma base usada em Cox/KM)\n",
    "if not os.path.exists(edges_path):\n",
    "    raise FileNotFoundError(f\"Falta {edges_path}. Rode a etapa de overlaps (edges) antes.\")\n",
    "_edges = pd.read_csv(edges_path, sep=\"\\t\", usecols=[\"Participant_ID\"])\n",
    "COHORT_N_CNV = _edges[\"Participant_ID\"].astype(str).nunique()\n",
    "\n",
    "print(f\"[INFO] COHORT_N_CNV (edges) = {COHORT_N_CNV}\")\n",
    "print(f\"[INFO] KM_MIN_GROUP = {KM_MIN_GROUP} | TOP_K = {TOP_K}\")\n",
    "\n",
    "# Filtra CNVs \"plotáveis\": garante n1>=KM_MIN_GROUP e n0>=KM_MIN_GROUP\n",
    "freq_ok = freq[\n",
    "    (freq[\"n_patients\"] >= KM_MIN_GROUP) &\n",
    "    (freq[\"n_patients\"] <= (COHORT_N_CNV - KM_MIN_GROUP))\n",
    "].copy()\n",
    "\n",
    "if freq_ok.empty:\n",
    "    print(\"[WARN] Nenhuma região passou o filtro de grupos (KM_MIN_GROUP).\")\n",
    "    print(\"[WARN] Vou manter a lista original (sem filtro). Isso pode gerar 0 PNGs no KM.\")\n",
    "    freq_use = freq\n",
    "else:\n",
    "    freq_use = freq_ok\n",
    "    print(f\"[OK] Regiões elegíveis p/ KM: {len(freq_ok)} / {len(freq)}\")\n",
    "\n",
    "top = (freq_use\n",
    "       .sort_values([\"n_patients\"], ascending=False)\n",
    "       .head(TOP_K)\n",
    "       .copy())\n",
    "\n",
    "top.to_csv(top_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[OK] Top regiões (filtradas p/ KM) exportadas: {top_path}\")\n",
    "display(top.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m6BpXjitYrww",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "m6BpXjitYrww",
    "outputId": "bbdd122b-c3a3-453f-9741-850acb81ca89"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 18b) RECURRENT CNV: selecionar TOP CNVs por SegmentID exato (chr:start-end)\n",
    "#     (filtra \"plotáveis\" p/ KM: ambos grupos >= KM_MIN_GROUP)\n",
    "# =========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "KM_MIN_GROUP = int(globals().get(\"KM_MIN_GROUP\", 20))\n",
    "\n",
    "freq_path = os.path.join(RES_DIR, \"cnv_recurrence_by_segmentid_exact__by_patient.tsv\")\n",
    "top_path  = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_segments_by_segmentid_exact.tsv\")\n",
    "\n",
    "if not os.path.exists(freq_path):\n",
    "    raise FileNotFoundError(\"Falta cnv_recurrence_by_segmentid_exact__by_patient.tsv. Rode a etapa 8.5 antes.\")\n",
    "\n",
    "if \"df_cnvs_hc\" not in globals():\n",
    "    raise RuntimeError(\"df_cnvs_hc não está em memória. Rode a etapa 8 antes.\")\n",
    "\n",
    "freq = pd.read_csv(freq_path, sep=\"\\t\")\n",
    "\n",
    "COHORT_N_CNV = int(df_cnvs_hc[\"Participant_ID\"].astype(str).nunique())\n",
    "print(f\"[INFO] COHORT_N_CNV (df_cnvs_hc) = {COHORT_N_CNV}\")\n",
    "print(f\"[INFO] KM_MIN_GROUP = {KM_MIN_GROUP} | TOP_K = {TOP_K}\")\n",
    "\n",
    "freq_ok = freq[\n",
    "    (freq[\"n_patients\"] >= KM_MIN_GROUP) &\n",
    "    (freq[\"n_patients\"] <= (COHORT_N_CNV - KM_MIN_GROUP))\n",
    "].copy()\n",
    "\n",
    "if freq_ok.empty:\n",
    "    print(\"[WARN] Nenhum SegmentID passou o filtro de grupos (KM_MIN_GROUP).\")\n",
    "    print(\"[WARN] Vou manter a lista original (sem filtro). Isso pode gerar 0 PNGs no KM.\")\n",
    "    freq_use = freq\n",
    "else:\n",
    "    freq_use = freq_ok\n",
    "    print(f\"[OK] Segmentos elegíveis p/ KM: {len(freq_ok)} / {len(freq)}\")\n",
    "\n",
    "top = (freq_use\n",
    "       .sort_values([\"n_patients\"], ascending=False)\n",
    "       .head(TOP_K)\n",
    "       .copy())\n",
    "\n",
    "# parse coordenadas\n",
    "m = top[\"SegmentID_exact\"].astype(str).str.extract(r\"(?P<Chromosome>[^:]+):(?P<Start>\\d+)-(?P<End>\\d+)\")\n",
    "top[\"Chromosome\"] = m[\"Chromosome\"]\n",
    "top[\"Start\"] = pd.to_numeric(m[\"Start\"], errors=\"coerce\")\n",
    "top[\"End\"] = pd.to_numeric(m[\"End\"], errors=\"coerce\")\n",
    "\n",
    "# nome de variável canônico\n",
    "top[\"var\"] = (\n",
    "    \"CNVEXACT__\" +\n",
    "    top[\"CNV_Type_Ajustado\"].astype(str).str.replace(\" \", \"_\") +\n",
    "    \"__\" +\n",
    "    top[\"SegmentID_exact\"].astype(str).str.replace(\":\", \"_\").str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "top.to_csv(top_path, sep=\"\\t\", index=False)\n",
    "print(f\"[OK] Top SegmentIDs (filtrados p/ KM) exportados: {top_path}\")\n",
    "display(top.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HxoILtczsszg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "id": "HxoILtczsszg",
    "outputId": "d8c0517c-2639-4f4a-9458-8f9fbb2a0b97"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 19) RECURRENT CNV: COX por presença/ausência das TOP CNVs (bins 1Mb)\n",
    "#     - Ajuste idade/sexo (se existir)\n",
    "#     - Sem imputação clínica (apenas 0/1 para presença/ausência de CNV nas features)\n",
    "# =========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "PROC_DIR = globals().get(\"PROC_DIR\", os.path.join(BASE_OUT_DIR, \"processed\"))\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "BIN_SIZE = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "\n",
    "edges_path = os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "top_path   = os.path.join(RES_DIR,  f\"top{TOP_K}_recurrent_regions_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\")\n",
    "\n",
    "if not os.path.exists(edges_path) or not os.path.exists(top_path):\n",
    "    raise FileNotFoundError(\"Faltam arquivos de entrada (edges/top). Rode a célula 18 antes.\")\n",
    "\n",
    "edges = pd.read_csv(edges_path, sep=\"\\t\")\n",
    "top   = pd.read_csv(top_path, sep=\"\\t\")\n",
    "\n",
    "# valida OS\n",
    "if \"os_df\" not in globals() or not isinstance(os_df, pd.DataFrame) or os_df.empty:\n",
    "    raise RuntimeError(\"os_df não está em memória. Rode a célula 'OS LOCKDOWN' (13) antes.\")\n",
    "\n",
    "df_surv = os_df.copy()\n",
    "df_surv[\"Participant_ID\"] = df_surv[\"Participant_ID\"].astype(str)\n",
    "\n",
    "# restringe à coorte com CNV disponível (não assumir ausência fora da coorte)\n",
    "cnv_patients = set(edges[\"Participant_ID\"].astype(str).unique())\n",
    "df_surv = df_surv[df_surv[\"Participant_ID\"].isin(cnv_patients)].copy()\n",
    "\n",
    "if df_surv.shape[0] < 30:\n",
    "    raise RuntimeError(f\"Coorte OS∩CNV muito pequena: {df_surv.shape[0]} pacientes.\")\n",
    "\n",
    "# identifica colunas de tempo/evento\n",
    "time_col = pick_col(df_surv, [\"time\", \"OS_time\", \"OS_days\", \"os_time\", \"days_to_death_or_last_followup\"])\n",
    "event_col = pick_col(df_surv, [\"event\", \"OS_event\", \"os_event\", \"vital_event\"])\n",
    "if time_col is None or event_col is None:\n",
    "    raise RuntimeError(f\"Não achei colunas de tempo/evento em os_df. Colunas: {list(df_surv.columns)[:50]}\")\n",
    "\n",
    "# covariáveis candidatas (sem imputar)\n",
    "age_col = pick_col(df_surv, [\"age\", \"Age\", \"age_at_index\", \"cases.demographic.age_at_index\"])\n",
    "sex_col = pick_col(df_surv, [\"sex\", \"Sex\", \"gender\", \"cases.demographic.gender\"])\n",
    "\n",
    "# cria variáveis binárias CNV__<type>__<BinID> para TOP regiões\n",
    "top = top.copy()\n",
    "top[\"var\"] = (\n",
    "    \"CNV__\" +\n",
    "    top[\"CNV_Type_Ajustado\"].astype(str).str.replace(\" \", \"_\") +\n",
    "    \"__\" +\n",
    "    top[\"BinID\"].astype(str).str.replace(\":\", \"_\").str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "keep = top[[\"BinID\",\"CNV_Type_Ajustado\",\"var\",\"n_patients\",\"Chromosome\",\"BinStart\",\"BinEnd\"]].copy()\n",
    "\n",
    "# mapeia edges → apenas TOP regiões\n",
    "edges2 = edges.merge(keep[[\"BinID\",\"CNV_Type_Ajustado\",\"var\"]], on=[\"BinID\",\"CNV_Type_Ajustado\"], how=\"inner\")\n",
    "edges2 = edges2.drop_duplicates([\"Participant_ID\",\"var\"]).copy()\n",
    "edges2[\"val\"] = 1\n",
    "\n",
    "X = (edges2\n",
    "    .pivot(index=\"Participant_ID\", columns=\"var\", values=\"val\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# monta base de modelagem (OS + features CNV)\n",
    "df_model_base = df_surv.merge(X, on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "# Preencher AUSÊNCIA de CNV (paciente sem ocorrência no Top-K) com 0\n",
    "cnv_cols = [c for c in df_model_base.columns if c.startswith(\"CNV__\")]\n",
    "if cnv_cols:\n",
    "    df_model_base[cnv_cols] = df_model_base[cnv_cols].fillna(0).astype(int)\n",
    "\n",
    "# normaliza idade/sexo se existirem (sem preencher missing)\n",
    "covars = []\n",
    "rename_map = {}\n",
    "if age_col and age_col != \"age\":\n",
    "    rename_map[age_col] = \"age\"\n",
    "if sex_col and sex_col != \"sex\":\n",
    "    rename_map[sex_col] = \"sex\"\n",
    "if rename_map:\n",
    "    df_model_base = df_model_base.rename(columns=rename_map)\n",
    "\n",
    "if \"age\" in df_model_base.columns:\n",
    "    covars.append(\"age\")\n",
    "\n",
    "if \"sex\" in df_model_base.columns:\n",
    "    # dummies de sexo sem inventar valores\n",
    "    df_model_base[\"sex\"] = df_model_base[\"sex\"].astype(str)\n",
    "    sex_dum = pd.get_dummies(df_model_base[\"sex\"], prefix=\"sex\", drop_first=True)\n",
    "    df_model_base = pd.concat([df_model_base.drop(columns=[\"sex\"]), sex_dum], axis=1)\n",
    "    covars.extend(sex_dum.columns.tolist())\n",
    "\n",
    "# expõe variáveis para KM/estadiamento (células 20/21)\n",
    "globals()[\"df_model_base\"] = df_model_base\n",
    "globals()[\"time_col\"] = time_col\n",
    "globals()[\"event_col\"] = event_col\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Cox por variável (recurrent CNV)\n",
    "# ---------------------------------------------------------\n",
    "cph = CoxPHFitter()\n",
    "rows = []\n",
    "\n",
    "for _, r in keep.iterrows():\n",
    "    var = r[\"var\"]\n",
    "    cols = [time_col, event_col, var] + covars\n",
    "    tmp = df_model_base[cols].copy().dropna()  # sem imputação\n",
    "\n",
    "    n_events = int(tmp[event_col].sum())\n",
    "    if tmp.shape[0] < 30 or n_events < 10:\n",
    "        rows.append({\n",
    "            \"Region_BinID\": r[\"BinID\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"BinStart\": int(r[\"BinStart\"]),\n",
    "            \"BinEnd\": int(r[\"BinEnd\"]),\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": np.nan, \"CI95_low\": np.nan, \"CI95_high\": np.nan, \"p\": np.nan,\n",
    "            \"note\": \"insuf_eventos_ou_N\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cph.fit(tmp, duration_col=time_col, event_col=event_col)\n",
    "        s = cph.summary.loc[var]\n",
    "        hr = float(np.exp(s[\"coef\"]))\n",
    "        lo = float(np.exp(s[\"coef lower 95%\"]))\n",
    "        hi = float(np.exp(s[\"coef upper 95%\"]))\n",
    "        p  = float(s[\"p\"])\n",
    "        rows.append({\n",
    "            \"Region_BinID\": r[\"BinID\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"BinStart\": int(r[\"BinStart\"]),\n",
    "            \"BinEnd\": int(r[\"BinEnd\"]),\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": hr, \"CI95_low\": lo, \"CI95_high\": hi, \"p\": p\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"Region_BinID\": r[\"BinID\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"BinStart\": int(r[\"BinStart\"]),\n",
    "            \"BinEnd\": int(r[\"BinEnd\"]),\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": np.nan, \"CI95_low\": np.nan, \"CI95_high\": np.nan, \"p\": np.nan,\n",
    "            \"error\": str(e)[:200]\n",
    "        })\n",
    "\n",
    "cox_tab = pd.DataFrame(rows)\n",
    "\n",
    "# BH-FDR (somente para p não-NaN)\n",
    "pvals = cox_tab[\"p\"].values\n",
    "mask = np.isfinite(pvals)\n",
    "q = np.full_like(pvals, np.nan, dtype=float)\n",
    "if mask.sum() > 0:\n",
    "    p = pvals[mask]\n",
    "    order = np.argsort(p)\n",
    "    ranked = p[order]\n",
    "    m = len(ranked)\n",
    "    qvals = ranked * m / (np.arange(m) + 1)\n",
    "    qvals = np.minimum.accumulate(qvals[::-1])[::-1]\n",
    "    q[mask] = qvals[np.argsort(order)]\n",
    "cox_tab[\"q_BH\"] = q\n",
    "\n",
    "def prognosis(hr):\n",
    "    if not np.isfinite(hr):\n",
    "        return \"\"\n",
    "    return \"Adverse (HR>1)\" if hr > 1 else \"Favorable (HR<1)\"\n",
    "\n",
    "cox_tab[\"Prognosis_from_HR\"] = cox_tab[\"HR\"].apply(prognosis)\n",
    "cox_tab = cox_tab.sort_values(\"p\", na_position=\"last\")\n",
    "\n",
    "out_cox = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_cox_table.tsv\")\n",
    "cox_tab.to_csv(out_cox, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[OK] Tabela Cox (bins) salva: {out_cox}\")\n",
    "display(cox_tab.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V8aDbeg1Yrww",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "V8aDbeg1Yrww",
    "outputId": "76075da3-a18f-478a-8da0-7d3e1d80ae11"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 19b) RECURRENT CNV: COX por presença/ausência das TOP CNVs (SegmentID exato)\n",
    "#     - Ajuste idade/sexo (se existir)\n",
    "#     - Sem imputação clínica (apenas 0/1 para presença/ausência de CNV nas features)\n",
    "# =========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "top_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_segments_by_segmentid_exact.tsv\")\n",
    "if not os.path.exists(top_path):\n",
    "    raise FileNotFoundError(\"Falta lista TOP SegmentID. Rode a célula 18b antes.\")\n",
    "\n",
    "if \"df_cnvs_hc\" not in globals() or not isinstance(df_cnvs_hc, pd.DataFrame) or df_cnvs_hc.empty:\n",
    "    raise RuntimeError(\"df_cnvs_hc não está em memória. Rode a etapa 8 antes.\")\n",
    "if \"os_df\" not in globals() or not isinstance(os_df, pd.DataFrame) or os_df.empty:\n",
    "    raise RuntimeError(\"os_df não está em memória. Rode a célula 'OS LOCKDOWN' (13) antes.\")\n",
    "\n",
    "top = pd.read_csv(top_path, sep=\"\\t\")\n",
    "\n",
    "# survival base\n",
    "df_surv = os_df.copy()\n",
    "df_surv[\"Participant_ID\"] = df_surv[\"Participant_ID\"].astype(str)\n",
    "\n",
    "# restringe à coorte com CNV disponível (não assumir ausência fora da coorte)\n",
    "cnv_patients = set(df_cnvs_hc[\"Participant_ID\"].astype(str).unique())\n",
    "df_surv = df_surv[df_surv[\"Participant_ID\"].isin(cnv_patients)].copy()\n",
    "\n",
    "if df_surv.shape[0] < 30:\n",
    "    raise RuntimeError(f\"Coorte OS∩CNV muito pequena: {df_surv.shape[0]} pacientes.\")\n",
    "\n",
    "# tempo/evento\n",
    "time_col_exact = pick_col(df_surv, [\"time\", \"OS_time\", \"OS_days\", \"os_time\", \"days_to_death_or_last_followup\"])\n",
    "event_col_exact = pick_col(df_surv, [\"event\", \"OS_event\", \"os_event\", \"vital_event\"])\n",
    "if time_col_exact is None or event_col_exact is None:\n",
    "    raise RuntimeError(f\"Não achei colunas de tempo/evento em os_df. Colunas: {list(df_surv.columns)[:50]}\")\n",
    "\n",
    "# covariáveis candidatas (sem imputar)\n",
    "age_col = pick_col(df_surv, [\"age\", \"Age\", \"age_at_index\", \"cases.demographic.age_at_index\"])\n",
    "sex_col = pick_col(df_surv, [\"sex\", \"Sex\", \"gender\", \"cases.demographic.gender\"])\n",
    "\n",
    "# prepara CNVs com SegmentID exato\n",
    "df_seg = df_cnvs_hc.copy()\n",
    "df_seg[\"Participant_ID\"] = df_seg[\"Participant_ID\"].astype(str)\n",
    "df_seg[\"Chromosome\"] = df_seg[\"Chromosome\"].astype(str)\n",
    "df_seg[\"Start\"] = pd.to_numeric(df_seg[\"Start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_seg[\"End\"]   = pd.to_numeric(df_seg[\"End\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "df_seg = df_seg.dropna(subset=[\"Participant_ID\",\"Chromosome\",\"Start\",\"End\",\"CNV_Type_Ajustado\"])\n",
    "\n",
    "df_seg[\"SegmentID_exact\"] = (\n",
    "    df_seg[\"Chromosome\"] + \":\" +\n",
    "    df_seg[\"Start\"].astype(str) + \"-\" +\n",
    "    df_seg[\"End\"].astype(str)\n",
    ")\n",
    "\n",
    "# define var canônica (igual à 18b)\n",
    "df_seg[\"var\"] = (\n",
    "    \"CNVEXACT__\" +\n",
    "    df_seg[\"CNV_Type_Ajustado\"].astype(str).str.replace(\" \", \"_\") +\n",
    "    \"__\" +\n",
    "    df_seg[\"SegmentID_exact\"].astype(str).str.replace(\":\", \"_\").str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "# keep (TOP segmentos) com var\n",
    "keep = top[[\"SegmentID_exact\",\"CNV_Type_Ajustado\",\"var\",\"n_patients\",\"Chromosome\",\"Start\",\"End\"]].copy()\n",
    "\n",
    "# edges exatos (presença/ausência por paciente)\n",
    "edges_exact = df_seg.merge(\n",
    "    keep[[\"SegmentID_exact\",\"CNV_Type_Ajustado\"]],\n",
    "    on=[\"SegmentID_exact\",\"CNV_Type_Ajustado\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "edges_exact = edges_exact.drop_duplicates([\"Participant_ID\",\"var\"]).copy()\n",
    "edges_exact[\"val\"] = 1\n",
    "\n",
    "X_exact = (edges_exact\n",
    "           .pivot(index=\"Participant_ID\", columns=\"var\", values=\"val\")\n",
    "           .fillna(0)\n",
    "           .astype(int)\n",
    "           .reset_index())\n",
    "\n",
    "# base modelagem\n",
    "df_model_base_exact = df_surv.merge(X_exact, on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "cnv_cols = [c for c in df_model_base_exact.columns if c.startswith(\"CNVEXACT__\")]\n",
    "if cnv_cols:\n",
    "    df_model_base_exact[cnv_cols] = df_model_base_exact[cnv_cols].fillna(0).astype(int)\n",
    "\n",
    "# normaliza idade/sexo (sem preencher missing)\n",
    "covars = []\n",
    "rename_map = {}\n",
    "if age_col and age_col != \"age\":\n",
    "    rename_map[age_col] = \"age\"\n",
    "if sex_col and sex_col != \"sex\":\n",
    "    rename_map[sex_col] = \"sex\"\n",
    "if rename_map:\n",
    "    df_model_base_exact = df_model_base_exact.rename(columns=rename_map)\n",
    "\n",
    "if \"age\" in df_model_base_exact.columns:\n",
    "    covars.append(\"age\")\n",
    "\n",
    "if \"sex\" in df_model_base_exact.columns:\n",
    "    df_model_base_exact[\"sex\"] = df_model_base_exact[\"sex\"].astype(str)\n",
    "    sex_dum = pd.get_dummies(df_model_base_exact[\"sex\"], prefix=\"sex\", drop_first=True)\n",
    "    df_model_base_exact = pd.concat([df_model_base_exact.drop(columns=[\"sex\"]), sex_dum], axis=1)\n",
    "    covars.extend(sex_dum.columns.tolist())\n",
    "\n",
    "# expõe para KM/estadiamento (SegmentID exato)\n",
    "globals()[\"df_model_base_exact\"] = df_model_base_exact\n",
    "globals()[\"time_col_exact\"] = time_col_exact\n",
    "globals()[\"event_col_exact\"] = event_col_exact\n",
    "\n",
    "# Cox por variável\n",
    "cph = CoxPHFitter()\n",
    "rows = []\n",
    "\n",
    "for _, r in keep.iterrows():\n",
    "    var = r[\"var\"]\n",
    "    cols = [time_col_exact, event_col_exact, var] + covars\n",
    "    tmp = df_model_base_exact[cols].copy().dropna()\n",
    "\n",
    "    n_events = int(tmp[event_col_exact].sum())\n",
    "    if tmp.shape[0] < 30 or n_events < 10:\n",
    "        rows.append({\n",
    "            \"SegmentID_exact\": r[\"SegmentID_exact\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"Start\": int(r[\"Start\"]) if pd.notna(r[\"Start\"]) else np.nan,\n",
    "            \"End\": int(r[\"End\"]) if pd.notna(r[\"End\"]) else np.nan,\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": np.nan, \"CI95_low\": np.nan, \"CI95_high\": np.nan, \"p\": np.nan,\n",
    "            \"note\": \"insuf_eventos_ou_N\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cph.fit(tmp, duration_col=time_col_exact, event_col=event_col_exact)\n",
    "        s = cph.summary.loc[var]\n",
    "        hr = float(np.exp(s[\"coef\"]))\n",
    "        lo = float(np.exp(s[\"coef lower 95%\"]))\n",
    "        hi = float(np.exp(s[\"coef upper 95%\"]))\n",
    "        p  = float(s[\"p\"])\n",
    "        rows.append({\n",
    "            \"SegmentID_exact\": r[\"SegmentID_exact\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"Start\": int(r[\"Start\"]) if pd.notna(r[\"Start\"]) else np.nan,\n",
    "            \"End\": int(r[\"End\"]) if pd.notna(r[\"End\"]) else np.nan,\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": hr, \"CI95_low\": lo, \"CI95_high\": hi, \"p\": p\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\n",
    "            \"SegmentID_exact\": r[\"SegmentID_exact\"],\n",
    "            \"Chromosome\": r[\"Chromosome\"],\n",
    "            \"Start\": int(r[\"Start\"]) if pd.notna(r[\"Start\"]) else np.nan,\n",
    "            \"End\": int(r[\"End\"]) if pd.notna(r[\"End\"]) else np.nan,\n",
    "            \"CNV_Type\": r[\"CNV_Type_Ajustado\"],\n",
    "            \"Patients_with_CNV\": int(r[\"n_patients\"]),\n",
    "            \"Cohort_N_used\": int(tmp.shape[0]),\n",
    "            \"Events_used\": n_events,\n",
    "            \"HR\": np.nan, \"CI95_low\": np.nan, \"CI95_high\": np.nan, \"p\": np.nan,\n",
    "            \"error\": str(e)[:200]\n",
    "        })\n",
    "\n",
    "cox_tab = pd.DataFrame(rows)\n",
    "\n",
    "# BH-FDR\n",
    "pvals = cox_tab[\"p\"].values\n",
    "mask = np.isfinite(pvals)\n",
    "q = np.full_like(pvals, np.nan, dtype=float)\n",
    "if mask.sum() > 0:\n",
    "    p = pvals[mask]\n",
    "    order = np.argsort(p)\n",
    "    ranked = p[order]\n",
    "    m = len(ranked)\n",
    "    qvals = ranked * m / (np.arange(m) + 1)\n",
    "    qvals = np.minimum.accumulate(qvals[::-1])[::-1]\n",
    "    q[mask] = qvals[np.argsort(order)]\n",
    "cox_tab[\"q_BH\"] = q\n",
    "\n",
    "def prognosis(hr):\n",
    "    if not np.isfinite(hr):\n",
    "        return \"\"\n",
    "    return \"Adverse (HR>1)\" if hr > 1 else \"Favorable (HR<1)\"\n",
    "\n",
    "cox_tab[\"Prognosis_from_HR\"] = cox_tab[\"HR\"].apply(prognosis)\n",
    "cox_tab = cox_tab.sort_values(\"p\", na_position=\"last\")\n",
    "\n",
    "out_cox = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_segments_cox_table.tsv\")\n",
    "cox_tab.to_csv(out_cox, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"[OK] Tabela Cox (SegmentID exato) salva: {out_cox}\")\n",
    "display(cox_tab.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oZP6ElSwsszh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZP6ElSwsszh",
    "outputId": "034005a7-f249-4451-d0bf-66ea722f1ca5"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 20) RECURRENT CNV: KM plots para TOP CNVs — ROBUSTO (sem crash) + filtro de prevalência\n",
    "# =========================================================\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "km_dir = os.path.join(RES_DIR, \"recurrent_cnv_km_plots\")\n",
    "os.makedirs(km_dir, exist_ok=True)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "N_KM  = int(globals().get(\"TOP_KM_PLOTS\", min(10, TOP_K)))\n",
    "\n",
    "# critério mínimo de tamanho de grupo (ajuste se quiser)\n",
    "MIN_GROUP = int(globals().get(\"KM_MIN_GROUP\", 20))\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_cox_table.tsv\")\n",
    "if not os.path.exists(cox_path):\n",
    "    raise FileNotFoundError(\"Tabela Cox não encontrada. Rode a célula 19 antes.\")\n",
    "\n",
    "cox_tab = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "\n",
    "if \"df_model_base\" not in globals():\n",
    "    raise RuntimeError(\"df_model_base não está em memória. Rode a célula 19 antes (na mesma sessão).\")\n",
    "if \"time_col\" not in globals() or \"event_col\" not in globals():\n",
    "    raise RuntimeError(\"time_col/event_col não estão em memória. Rode a célula 19 antes (na mesma sessão).\")\n",
    "\n",
    "# Base survival (garante time/event presentes)\n",
    "df_base = df_model_base.dropna(subset=[time_col, event_col]).copy()\n",
    "\n",
    "def safe_name(s):\n",
    "    s = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", str(s))\n",
    "    return s[:180]\n",
    "\n",
    "def make_var(binid, cnv_type):\n",
    "    return \"CNV__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(binid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "# adiciona var + n_present/n_absent em cox_tab (para filtrar CNVs viáveis para KM)\n",
    "counts = []\n",
    "N_total = df_base.shape[0]\n",
    "\n",
    "for i, r in cox_tab.iterrows():\n",
    "    binid = r.get(\"Region_BinID\", r.get(\"Region(BinID)\", None))\n",
    "    cnv_type = r.get(\"CNV_Type\", None)\n",
    "    if binid is None or cnv_type is None:\n",
    "        continue\n",
    "\n",
    "    var = make_var(binid, cnv_type)\n",
    "    if var not in df_base.columns:\n",
    "        continue\n",
    "\n",
    "    # garante 0/1; se tiver NaN, trata como 0 (ausente) apenas para variável binária de CNV\n",
    "    v = pd.to_numeric(df_base[var], errors=\"coerce\").fillna(0).astype(int)\n",
    "    n1 = int((v == 1).sum())\n",
    "    n0 = int((v == 0).sum())\n",
    "\n",
    "    counts.append((i, var, n1, n0))\n",
    "\n",
    "if not counts:\n",
    "    # salva vazio e sai sem quebrar\n",
    "    km_tab = pd.DataFrame(columns=[\"Region_BinID\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "    out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary.tsv\")\n",
    "    km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "    print(\"[INFO] Nenhuma variável CNV encontrada em df_model_base para KM.\")\n",
    "    print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "    km_tab\n",
    "\n",
    "else:\n",
    "    counts_df = pd.DataFrame(counts, columns=[\"row_idx\",\"var\",\"n_present\",\"n_absent\"]).set_index(\"row_idx\")\n",
    "    cox2 = cox_tab.join(counts_df, how=\"left\")\n",
    "\n",
    "    # filtra CNVs com grupos mínimos\n",
    "    cox2[\"min_group\"] = cox2[[\"n_present\",\"n_absent\"]].min(axis=1)\n",
    "    viable = cox2.dropna(subset=[\"n_present\",\"n_absent\"]).copy()\n",
    "    viable = viable[(viable[\"n_present\"] >= MIN_GROUP) & (viable[\"n_absent\"] >= MIN_GROUP)].copy()\n",
    "\n",
    "    if viable.empty:\n",
    "        km_tab = pd.DataFrame(columns=[\"Region_BinID\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "        out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary.tsv\")\n",
    "        km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "\n",
    "        print(f\"[INFO] Nenhuma CNV passou o filtro de KM (MIN_GROUP={MIN_GROUP}).\")\n",
    "        print(\"Isso é comum quando a CNV é MUITO rara ou MUITO frequente na coorte.\")\n",
    "        print(\"[DICA] Você pode: (a) reduzir KM_MIN_GROUP, (b) usar citobanda em vez de bin 1Mb, ou (c) avaliar apenas Gains/Losses.\")\n",
    "        print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "        km_tab\n",
    "\n",
    "    else:\n",
    "        # escolhe melhores por p (Cox) e, em empate, por maior min_group e frequência\n",
    "        viable[\"p_rank\"] = pd.to_numeric(viable.get(\"p\", pd.Series(np.nan, index=viable.index)), errors=\"coerce\").fillna(1.0)\n",
    "\n",
    "        # tenta usar Patients_with_CNV se existir\n",
    "        freq_col = \"Patients_with_CNV\" if \"Patients_with_CNV\" in viable.columns else None\n",
    "\n",
    "        sort_cols = [\"p_rank\", \"min_group\"]\n",
    "        ascending = [True, False]\n",
    "        if freq_col:\n",
    "            sort_cols.append(freq_col)\n",
    "            ascending.append(False)\n",
    "\n",
    "        chosen = viable.sort_values(sort_cols, ascending=ascending).head(N_KM)\n",
    "\n",
    "        kmf = KaplanMeierFitter()\n",
    "        km_results = []\n",
    "\n",
    "        for _, r in chosen.iterrows():\n",
    "            binid = r.get(\"Region_BinID\", r.get(\"Region(BinID)\", None))\n",
    "            cnv_type = r.get(\"CNV_Type\", None)\n",
    "            if binid is None or cnv_type is None:\n",
    "                continue\n",
    "\n",
    "            var = make_var(binid, cnv_type)\n",
    "            if var not in df_base.columns:\n",
    "                print(f\"[WARN] Variável CNV não encontrada para KM: {var}\")\n",
    "                continue\n",
    "\n",
    "            dfp = df_base[[time_col, event_col, var]].dropna().copy()\n",
    "            dfp[var] = pd.to_numeric(dfp[var], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "            g1 = dfp[dfp[var] == 1]\n",
    "            g0 = dfp[dfp[var] == 0]\n",
    "\n",
    "            # guarda tamanhos reais\n",
    "            n1, n0 = int(g1.shape[0]), int(g0.shape[0])\n",
    "\n",
    "            if n1 < MIN_GROUP or n0 < MIN_GROUP:\n",
    "                print(f\"[WARN] Grupo pequeno para KM: {binid} {cnv_type} | n1={n1} n0={n0}\")\n",
    "                continue\n",
    "\n",
    "            lr = logrank_test(g1[time_col], g0[time_col], event_observed_A=g1[event_col], event_observed_B=g0[event_col])\n",
    "            p_lr = float(lr.p_value)\n",
    "\n",
    "            plt.figure()\n",
    "            kmf.fit(g0[time_col], event_observed=g0[event_col], label=\"CNV absent\")\n",
    "            ax = kmf.plot()\n",
    "            kmf.fit(g1[time_col], event_observed=g1[event_col], label=\"CNV present\")\n",
    "            kmf.plot(ax=ax)\n",
    "            ax.set_title(f\"{binid} | {cnv_type} | logrank p={p_lr:.3g}\")\n",
    "            ax.set_xlabel(time_col)\n",
    "            ax.set_ylabel(\"Survival probability\")\n",
    "\n",
    "            fig_path = os.path.join(km_dir, f\"KM__{safe_name(binid)}__{safe_name(cnv_type)}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_path, dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "            km_results.append({\n",
    "                \"Region_BinID\": binid,\n",
    "                \"CNV_Type\": cnv_type,\n",
    "                \"n_present\": n1,\n",
    "                \"n_absent\": n0,\n",
    "                \"logrank_p\": p_lr,\n",
    "                \"plot\": os.path.basename(fig_path)\n",
    "            })\n",
    "\n",
    "        # agora: se nada gerou KM, não quebra\n",
    "        if len(km_results) == 0:\n",
    "            km_tab = pd.DataFrame(columns=[\"Region_BinID\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "            out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary.tsv\")\n",
    "            km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "            print(f\"[INFO] Após filtros, nenhuma CNV gerou KM (MIN_GROUP={MIN_GROUP}).\")\n",
    "            print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "            km_tab\n",
    "        else:\n",
    "            km_tab = pd.DataFrame(km_results).sort_values(\"logrank_p\", na_position=\"last\")\n",
    "            out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary.tsv\")\n",
    "            km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "\n",
    "            print(f\"[OK] KM plots em: {km_dir}\")\n",
    "            print(f\"[OK] Resumo KM: {out_km}\")\n",
    "            km_tab.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B5AFK5FLYrwx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5AFK5FLYrwx",
    "outputId": "76c433b5-a4e7-4c65-889c-a7621708f09a"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 20b) RECURRENT CNV: KM plots para TOP CNVs (SegmentID exato) — ROBUSTO + filtro de prevalência\n",
    "# =========================================================\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "km_dir = os.path.join(RES_DIR, \"recurrent_cnv_km_plots_segment_exact\")\n",
    "os.makedirs(km_dir, exist_ok=True)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "N_KM  = int(globals().get(\"TOP_KM_PLOTS\", min(10, TOP_K)))\n",
    "MIN_GROUP = int(globals().get(\"KM_MIN_GROUP\", 20))\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_segments_cox_table.tsv\")\n",
    "if not os.path.exists(cox_path):\n",
    "    raise FileNotFoundError(\"Tabela Cox (SegmentID exato) não encontrada. Rode a célula 19b antes.\")\n",
    "\n",
    "cox_tab = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "\n",
    "if \"df_model_base_exact\" not in globals():\n",
    "    raise RuntimeError(\"df_model_base_exact não está em memória. Rode a célula 19b antes (na mesma sessão).\")\n",
    "if \"time_col_exact\" not in globals() or \"event_col_exact\" not in globals():\n",
    "    raise RuntimeError(\"time_col_exact/event_col_exact não estão em memória. Rode a célula 19b antes (na mesma sessão).\")\n",
    "\n",
    "df_base = df_model_base_exact.dropna(subset=[time_col_exact, event_col_exact]).copy()\n",
    "\n",
    "def safe_name(s):\n",
    "    s = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", str(s))\n",
    "    return s[:180]\n",
    "\n",
    "def make_var_exact(segid, cnv_type):\n",
    "    return \"CNVEXACT__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(segid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "# adiciona contagens (para filtrar CNVs viáveis para KM)\n",
    "counts = []\n",
    "N_total = df_base.shape[0]\n",
    "\n",
    "for i, r in cox_tab.iterrows():\n",
    "    segid = r.get(\"SegmentID_exact\", None)\n",
    "    cnv_type = r.get(\"CNV_Type\", None)\n",
    "    if segid is None or cnv_type is None:\n",
    "        continue\n",
    "\n",
    "    var = make_var_exact(segid, cnv_type)\n",
    "    if var not in df_base.columns:\n",
    "        continue\n",
    "\n",
    "    v = pd.to_numeric(df_base[var], errors=\"coerce\").fillna(0).astype(int)\n",
    "    n1 = int((v == 1).sum())\n",
    "    n0 = int((v == 0).sum())\n",
    "    counts.append((i, var, n1, n0))\n",
    "\n",
    "if not counts:\n",
    "    km_tab = pd.DataFrame(columns=[\"SegmentID_exact\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "    out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary_segment_exact.tsv\")\n",
    "    km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "    print(\"[INFO] Nenhuma variável CNVEXACT encontrada em df_model_base_exact para KM.\")\n",
    "    print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "    km_tab\n",
    "else:\n",
    "    counts_df = pd.DataFrame(counts, columns=[\"row_idx\",\"var\",\"n_present\",\"n_absent\"]).set_index(\"row_idx\")\n",
    "    cox2 = cox_tab.join(counts_df, how=\"left\")\n",
    "\n",
    "    cox2[\"min_group\"] = cox2[[\"n_present\",\"n_absent\"]].min(axis=1)\n",
    "    viable = cox2.dropna(subset=[\"n_present\",\"n_absent\"]).copy()\n",
    "    viable = viable[(viable[\"n_present\"] >= MIN_GROUP) & (viable[\"n_absent\"] >= MIN_GROUP)].copy()\n",
    "\n",
    "    if viable.empty:\n",
    "        km_tab = pd.DataFrame(columns=[\"SegmentID_exact\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "        out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary_segment_exact.tsv\")\n",
    "        km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "\n",
    "        print(f\"[INFO] Nenhuma CNVEXACT passou o filtro de KM (MIN_GROUP={MIN_GROUP}).\")\n",
    "        print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "        km_tab\n",
    "    else:\n",
    "        viable[\"p_rank\"] = pd.to_numeric(viable.get(\"p\", pd.Series(np.nan, index=viable.index)), errors=\"coerce\").fillna(1.0)\n",
    "\n",
    "        sort_cols = [\"p_rank\", \"min_group\"]\n",
    "        ascending = [True, False]\n",
    "        if \"Patients_with_CNV\" in viable.columns:\n",
    "            sort_cols.append(\"Patients_with_CNV\")\n",
    "            ascending.append(False)\n",
    "\n",
    "        chosen = viable.sort_values(sort_cols, ascending=ascending).head(N_KM)\n",
    "\n",
    "        kmf = KaplanMeierFitter()\n",
    "        km_results = []\n",
    "\n",
    "        for _, r in chosen.iterrows():\n",
    "            segid = r.get(\"SegmentID_exact\", None)\n",
    "            cnv_type = r.get(\"CNV_Type\", None)\n",
    "            if segid is None or cnv_type is None:\n",
    "                continue\n",
    "\n",
    "            var = make_var_exact(segid, cnv_type)\n",
    "            if var not in df_base.columns:\n",
    "                print(f\"[WARN] Variável CNVEXACT não encontrada para KM: {var}\")\n",
    "                continue\n",
    "\n",
    "            dfp = df_base[[time_col_exact, event_col_exact, var]].dropna().copy()\n",
    "            dfp[var] = pd.to_numeric(dfp[var], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "            g1 = dfp[dfp[var] == 1]\n",
    "            g0 = dfp[dfp[var] == 0]\n",
    "\n",
    "            n1, n0 = int(g1.shape[0]), int(g0.shape[0])\n",
    "            if n1 < MIN_GROUP or n0 < MIN_GROUP:\n",
    "                print(f\"[WARN] Grupo pequeno para KM: {segid} {cnv_type} | n1={n1} n0={n0}\")\n",
    "                continue\n",
    "\n",
    "            lr = logrank_test(g1[time_col_exact], g0[time_col_exact], event_observed_A=g1[event_col_exact], event_observed_B=g0[event_col_exact])\n",
    "            p_lr = float(lr.p_value)\n",
    "\n",
    "            plt.figure()\n",
    "            kmf.fit(g0[time_col_exact], event_observed=g0[event_col_exact], label=\"CNV absent\")\n",
    "            ax = kmf.plot()\n",
    "            kmf.fit(g1[time_col_exact], event_observed=g1[event_col_exact], label=\"CNV present\")\n",
    "            kmf.plot(ax=ax)\n",
    "            ax.set_title(f\"{segid} | {cnv_type} | logrank p={p_lr:.3g}\")\n",
    "            ax.set_xlabel(time_col_exact)\n",
    "            ax.set_ylabel(\"Survival probability\")\n",
    "\n",
    "            fig_path = os.path.join(km_dir, f\"KM__{safe_name(segid)}__{safe_name(cnv_type)}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_path, dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "            km_results.append({\n",
    "                \"SegmentID_exact\": segid,\n",
    "                \"CNV_Type\": cnv_type,\n",
    "                \"n_present\": n1,\n",
    "                \"n_absent\": n0,\n",
    "                \"logrank_p\": p_lr,\n",
    "                \"plot\": os.path.basename(fig_path)\n",
    "            })\n",
    "\n",
    "        if len(km_results) == 0:\n",
    "            km_tab = pd.DataFrame(columns=[\"SegmentID_exact\",\"CNV_Type\",\"n_present\",\"n_absent\",\"logrank_p\",\"plot\"])\n",
    "            out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary_segment_exact.tsv\")\n",
    "            km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "            print(f\"[INFO] Após filtros, nenhuma CNVEXACT gerou KM (MIN_GROUP={MIN_GROUP}).\")\n",
    "            print(\"[OK] Resumo KM vazio salvo:\", out_km)\n",
    "            km_tab\n",
    "        else:\n",
    "            km_tab = pd.DataFrame(km_results).sort_values(\"logrank_p\", na_position=\"last\")\n",
    "            out_km = os.path.join(RES_DIR, f\"top{TOP_K}_km_summary_segment_exact.tsv\")\n",
    "            km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "\n",
    "            print(f\"[OK] KM plots (SegmentID exato) em: {km_dir}\")\n",
    "            print(f\"[OK] Resumo KM: {out_km}\")\n",
    "            km_tab.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1VWsr4-Ysszh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VWsr4-Ysszh",
    "outputId": "255577c7-9bf6-4654-8f7f-a33a75f9db3c"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 21) RECURRENT CNV: Estratificação por STAGING (TODAS as colunas disponíveis)\n",
    "#    - Cox por estágio (I/II/III etc.) para as TOP regiões (sem forçar CNVs do banner)\n",
    "#    - Associação CNV↔estágio (qui-quadrado) para TOP regiões (opcional, se SciPy disponível)\n",
    "# =========================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "BIN_SIZE = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "MIN_STAGE_N = int(globals().get(\"MIN_STAGE_N\", 50))      # mínimo de amostras com estágio válido\n",
    "MAX_LEVELS = int(globals().get(\"MAX_STAGE_LEVELS\", 10))  # evita variáveis \"bagunçadas\"\n",
    "MIN_EVENTS = int(globals().get(\"MIN_EVENTS_STAGE\", 8))   # mínimo de eventos por estágio para Cox\n",
    "\n",
    "BAD_LEVELS = set([\n",
    "    \"\", \"nan\", \"none\", \"null\",\n",
    "    \"unknown\", \"not reported\", \"not applicable\", \"na\", \"n/a\",\n",
    "    \"unspecified\", \"missing\", \"not available\"\n",
    "])\n",
    "\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_cox_table.tsv\")\n",
    "if not os.path.exists(cox_path):\n",
    "    raise FileNotFoundError(\"Tabela Cox não encontrada. Rode a célula 19 antes.\")\n",
    "\n",
    "if \"df_model_base\" not in globals() or \"time_col\" not in globals() or \"event_col\" not in globals():\n",
    "    raise RuntimeError(\"Variáveis de modelo não estão em memória. Rode a célula 19 antes (na mesma sessão).\")\n",
    "\n",
    "df_base = df_model_base.copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Detecta colunas de estadiamento (TODAS)\n",
    "# Prioridade:\n",
    "# 1) STAGE_COLUMNS_AVAILABLE (do clinical, anexadas no os_df)\n",
    "# 2) qualquer coluna do df_base com 'iss_stage' ou termina em 'stage'\n",
    "# ---------------------------------------------------------\n",
    "stage_candidates = []\n",
    "if \"STAGE_COLUMNS_AVAILABLE\" in globals():\n",
    "    stage_candidates.extend([c for c in STAGE_COLUMNS_AVAILABLE if c in df_base.columns])\n",
    "\n",
    "stage_candidates.extend([c for c in df_base.columns if re.search(r\"(iss_stage|r_iss|riss|revised.*iss)\", str(c), re.I)])\n",
    "stage_candidates.extend([c for c in df_base.columns if re.search(r\"(?:^|\\.|_)(stage|staging)(?:$|_|\\.)\", str(c), re.I)])\n",
    "\n",
    "# remove duplicatas, mantém ordem\n",
    "stage_candidates = list(dict.fromkeys(stage_candidates))\n",
    "\n",
    "# filtra colunas com dados suficientes e níveis razoáveis\n",
    "usable = []\n",
    "stage_profile_rows = []\n",
    "\n",
    "for c in stage_candidates:\n",
    "    s = df_base[c]\n",
    "    nn = int(s.notna().sum())\n",
    "    if nn < MIN_STAGE_N:\n",
    "        continue\n",
    "    # normaliza como string pra medir níveis\n",
    "    lv = s.dropna().astype(str).str.strip()\n",
    "    lv_low = lv.str.lower()\n",
    "    lv = lv[~lv_low.isin(BAD_LEVELS)]\n",
    "    nlev = int(lv.nunique())\n",
    "    if nlev < 2 or nlev > MAX_LEVELS:\n",
    "        continue\n",
    "    usable.append(c)\n",
    "    stage_profile_rows.append({\"stage_col\": c, \"n_nonnull\": nn, \"n_levels\": nlev, \"levels\": \", \".join(sorted(lv.unique().tolist())[:MAX_LEVELS])})\n",
    "\n",
    "stage_profile = pd.DataFrame(stage_profile_rows).sort_values([\"n_nonnull\",\"n_levels\"], ascending=[False, True])\n",
    "profile_path = os.path.join(RES_DIR, f\"recurrent_cnv_stage_columns_profile.tsv\")\n",
    "stage_profile.to_csv(profile_path, sep=\"\\t\", index=False)\n",
    "print(f\"[OK] Perfil de estadiamento salvo: {profile_path}\")\n",
    "\n",
    "if not usable:\n",
    "    print(\"[WARN] Nenhuma coluna de estadiamento com dados suficientes (>=MIN_STAGE_N e níveis <=MAX_LEVELS). Pulando.\")\n",
    "else:\n",
    "    print(f\"[OK] Colunas de estadiamento usadas: {usable}\")\n",
    "\n",
    "    # lista de regiões / tipos (para reconstruir varname)\n",
    "    cox_tab = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "    cox_tab = cox_tab.dropna(subset=[\"Region_BinID\",\"CNV_Type\"]).copy()\n",
    "\n",
    "    covars = [c for c in df_base.columns if c.startswith(\"sex_\")] + ([\"age\"] if \"age\" in df_base.columns else [])\n",
    "    cph = CoxPHFitter()\n",
    "\n",
    "    # tenta SciPy para qui-quadrado\n",
    "    try:\n",
    "        from scipy.stats import chi2_contingency\n",
    "        HAVE_SCIPY = True\n",
    "    except Exception:\n",
    "        HAVE_SCIPY = False\n",
    "\n",
    "    for stage_col in usable:\n",
    "        # prepara coluna\n",
    "        df_stage_base = df_base.copy()\n",
    "        df_stage_base[\"_stage_\"] = df_stage_base[stage_col].astype(str).str.strip()\n",
    "        df_stage_base.loc[df_stage_base[stage_col].isna(), \"_stage_\"] = np.nan\n",
    "        df_stage_base.loc[df_stage_base[\"_stage_\"].astype(str).str.lower().isin(BAD_LEVELS), \"_stage_\"] = np.nan\n",
    "\n",
    "        # distribuição\n",
    "        dist = (df_stage_base[\"_stage_\"].value_counts(dropna=False)\n",
    "                .rename_axis(\"level\")\n",
    "                .reset_index(name=\"n\"))\n",
    "        dist_path = os.path.join(RES_DIR, f\"recurrent_cnv_{re.sub(r'[^A-Za-z0-9]+','_',stage_col)}__distribution.tsv\")\n",
    "        dist.to_csv(dist_path, sep=\"\\t\", index=False)\n",
    "\n",
    "        rows = []\n",
    "        chi_rows = []\n",
    "\n",
    "        # Cox por nível\n",
    "        levels = sorted(df_stage_base[\"_stage_\"].dropna().unique().tolist())\n",
    "        for level in levels:\n",
    "            df_s = df_stage_base[df_stage_base[\"_stage_\"] == level].copy()\n",
    "            if df_s.shape[0] < 30 or int(df_s[event_col].sum()) < MIN_EVENTS:\n",
    "                continue\n",
    "\n",
    "            for _, r in cox_tab.iterrows():\n",
    "                binid = r[\"Region_BinID\"]\n",
    "                cnv_type = r[\"CNV_Type\"]\n",
    "                var = \"CNV__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(binid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "                if var not in df_s.columns:\n",
    "                    continue\n",
    "\n",
    "                cols = [time_col, event_col, var] + covars\n",
    "                tmp = df_s[cols].dropna()\n",
    "                if tmp.shape[0] < 30 or int(tmp[event_col].sum()) < MIN_EVENTS:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    cph.fit(tmp, duration_col=time_col, event_col=event_col)\n",
    "                    s = cph.summary.loc[var]\n",
    "                    rows.append({\n",
    "                        \"stage_col\": stage_col,\n",
    "                        \"stage_level\": level,\n",
    "                        \"Region_BinID\": binid,\n",
    "                        \"CNV_Type\": cnv_type,\n",
    "                        \"N_used\": int(tmp.shape[0]),\n",
    "                        \"Events_used\": int(tmp[event_col].sum()),\n",
    "                        \"HR\": float(np.exp(s[\"coef\"])),\n",
    "                        \"CI95_low\": float(np.exp(s[\"coef lower 95%\"])),\n",
    "                        \"CI95_high\": float(np.exp(s[\"coef upper 95%\"])),\n",
    "                        \"p\": float(s[\"p\"]),\n",
    "                    })\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Qui-quadrado (CNV presence vs nível) – para TOP regiões apenas\n",
    "            if HAVE_SCIPY:\n",
    "                for _, r in cox_tab.iterrows():\n",
    "                    binid = r[\"Region_BinID\"]\n",
    "                    cnv_type = r[\"CNV_Type\"]\n",
    "                    var = \"CNV__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(binid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "                    if var not in df_stage_base.columns:\n",
    "                        continue\n",
    "                    # tabela 2xK (presença vs nível) calculada no dataset inteiro do stage_col\n",
    "                    tmp2 = df_stage_base[[var, \"_stage_\"]].dropna()\n",
    "                    if tmp2[\"_stage_\"].nunique() < 2:\n",
    "                        continue\n",
    "                    ct = pd.crosstab(tmp2[\"_stage_\"], tmp2[var])\n",
    "                    # garante colunas 0 e 1\n",
    "                    if 0 not in ct.columns:\n",
    "                        ct[0] = 0\n",
    "                    if 1 not in ct.columns:\n",
    "                        ct[1] = 0\n",
    "                    ct = ct[[0,1]]\n",
    "                    if ct.values.sum() < 50:\n",
    "                        continue\n",
    "                    try:\n",
    "                        chi2, pval, dof, exp = chi2_contingency(ct.values)\n",
    "                        chi_rows.append({\n",
    "                            \"stage_col\": stage_col,\n",
    "                            \"Region_BinID\": binid,\n",
    "                            \"CNV_Type\": cnv_type,\n",
    "                            \"chi2\": float(chi2),\n",
    "                            \"dof\": int(dof),\n",
    "                            \"p\": float(pval),\n",
    "                            \"n_used\": int(ct.values.sum())\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        # salva outputs por stage_col\n",
    "        slug = re.sub(r\"[^A-Za-z0-9]+\", \"_\", stage_col).strip(\"_\")\n",
    "        if rows:\n",
    "            stage_tab = pd.DataFrame(rows).sort_values([\"stage_level\",\"p\"], na_position=\"last\")\n",
    "            out_stage = os.path.join(RES_DIR, f\"top{TOP_K}_cox_by_{slug}.tsv\")\n",
    "            stage_tab.to_csv(out_stage, sep=\"\\t\", index=False)\n",
    "            print(f\"[OK] Cox estratificado ({stage_col}) salvo: {out_stage}\")\n",
    "        else:\n",
    "            print(f\"[WARN] Sem modelos Cox suficientes para {stage_col} (N/eventos baixos).\")\n",
    "\n",
    "        if HAVE_SCIPY and chi_rows:\n",
    "            chi_tab = pd.DataFrame(chi_rows).sort_values(\"p\", na_position=\"last\")\n",
    "            out_chi = os.path.join(RES_DIR, f\"top{TOP_K}_chi2_by_{slug}.tsv\")\n",
    "            chi_tab.to_csv(out_chi, sep=\"\\t\", index=False)\n",
    "            print(f\"[OK] Qui-quadrado CNV↔estágio ({stage_col}) salvo: {out_chi}\")\n",
    "        elif not HAVE_SCIPY:\n",
    "            print(\"[INFO] SciPy não disponível; pulando qui-quadrado (ok).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yONDdNhvYrwy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yONDdNhvYrwy",
    "outputId": "2baac4f7-3f4b-4ed2-9384-e514315ae3e3"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 21b) RECURRENT CNV: Estratificação por STAGING (SegmentID exato)\n",
    "#    - Cox por estágio (I/II/III etc.) para TOP SegmentIDs\n",
    "#    - Associação CNV↔estágio (qui-quadrado) para TOP SegmentIDs (opcional, se SciPy disponível)\n",
    "# =========================================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "MIN_STAGE_N = int(globals().get(\"MIN_STAGE_N\", 50))\n",
    "MAX_LEVELS = int(globals().get(\"MAX_STAGE_LEVELS\", 10))\n",
    "MIN_EVENTS = int(globals().get(\"MIN_EVENTS_STAGE\", 8))\n",
    "\n",
    "BAD_LEVELS = set([\n",
    "    \"\", \"nan\", \"none\", \"null\",\n",
    "    \"unknown\", \"not reported\", \"not applicable\", \"na\", \"n/a\",\n",
    "    \"unspecified\", \"missing\", \"not available\"\n",
    "])\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_segments_cox_table.tsv\")\n",
    "if not os.path.exists(cox_path):\n",
    "    raise FileNotFoundError(\"Tabela Cox (SegmentID exato) não encontrada. Rode a célula 19b antes.\")\n",
    "\n",
    "if \"df_model_base_exact\" not in globals() or \"time_col_exact\" not in globals() or \"event_col_exact\" not in globals():\n",
    "    raise RuntimeError(\"Variáveis de modelo exato não estão em memória. Rode a célula 19b antes (na mesma sessão).\")\n",
    "\n",
    "df_base = df_model_base_exact.copy()\n",
    "\n",
    "# Detecta colunas de estadiamento\n",
    "stage_candidates = []\n",
    "if \"STAGE_COLUMNS_AVAILABLE\" in globals():\n",
    "    stage_candidates.extend([c for c in STAGE_COLUMNS_AVAILABLE if c in df_base.columns])\n",
    "\n",
    "stage_candidates.extend([c for c in df_base.columns if re.search(r\"(iss_stage|r_iss|riss|revised.*iss)\", str(c), re.I)])\n",
    "stage_candidates.extend([c for c in df_base.columns if re.search(r\"(?:^|\\.|_)(stage|staging)(?:$|_|\\.)\", str(c), re.I)])\n",
    "\n",
    "stage_candidates = list(dict.fromkeys(stage_candidates))\n",
    "\n",
    "usable = []\n",
    "stage_profile_rows = []\n",
    "\n",
    "for c in stage_candidates:\n",
    "    s = df_base[c]\n",
    "    nn = int(s.notna().sum())\n",
    "    if nn < MIN_STAGE_N:\n",
    "        continue\n",
    "    lv = s.dropna().astype(str).str.strip()\n",
    "    lv_low = lv.str.lower()\n",
    "    lv = lv[~lv_low.isin(BAD_LEVELS)]\n",
    "    nlev = int(lv.nunique())\n",
    "    if nlev < 2 or nlev > MAX_LEVELS:\n",
    "        continue\n",
    "    usable.append(c)\n",
    "    stage_profile_rows.append({\"stage_col\": c, \"n_nonnull\": nn, \"n_levels\": nlev, \"levels\": \", \".join(sorted(lv.unique().tolist())[:MAX_LEVELS])})\n",
    "\n",
    "stage_profile = pd.DataFrame(stage_profile_rows).sort_values([\"n_nonnull\",\"n_levels\"], ascending=[False, True])\n",
    "profile_path = os.path.join(RES_DIR, f\"recurrent_cnv_stage_columns_profile__segment_exact.tsv\")\n",
    "stage_profile.to_csv(profile_path, sep=\"\\t\", index=False)\n",
    "print(f\"[OK] Perfil de estadiamento (SegmentID exato) salvo: {profile_path}\")\n",
    "\n",
    "if not usable:\n",
    "    print(\"[WARN] Nenhuma coluna de estadiamento com dados suficientes (>=MIN_STAGE_N e níveis <=MAX_LEVELS). Pulando.\")\n",
    "else:\n",
    "    print(f\"[OK] Colunas de estadiamento usadas: {usable}\")\n",
    "\n",
    "    cox_tab = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "    cox_tab = cox_tab.dropna(subset=[\"SegmentID_exact\",\"CNV_Type\"]).copy()\n",
    "\n",
    "    covars = [c for c in df_base.columns if c.startswith(\"sex_\")] + ([\"age\"] if \"age\" in df_base.columns else [])\n",
    "    cph = CoxPHFitter()\n",
    "\n",
    "    try:\n",
    "        from scipy.stats import chi2_contingency\n",
    "        HAVE_SCIPY = True\n",
    "    except Exception:\n",
    "        HAVE_SCIPY = False\n",
    "\n",
    "    def make_var_exact(segid, cnv_type):\n",
    "        return \"CNVEXACT__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(segid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "    for stage_col in usable:\n",
    "        df_stage_base = df_base.copy()\n",
    "        df_stage_base[\"_stage_\"] = df_stage_base[stage_col].astype(str).str.strip()\n",
    "        df_stage_base.loc[df_stage_base[stage_col].isna(), \"_stage_\"] = np.nan\n",
    "        df_stage_base.loc[df_stage_base[\"_stage_\"].astype(str).str.lower().isin(BAD_LEVELS), \"_stage_\"] = np.nan\n",
    "\n",
    "        dist = (df_stage_base[\"_stage_\"].value_counts(dropna=False)\n",
    "                .rename_axis(\"level\")\n",
    "                .reset_index(name=\"n\"))\n",
    "        dist_path = os.path.join(RES_DIR, f\"recurrent_cnv_{re.sub(r'[^A-Za-z0-9]+','_',stage_col)}__distribution__segment_exact.tsv\")\n",
    "        dist.to_csv(dist_path, sep=\"\\t\", index=False)\n",
    "\n",
    "        rows = []\n",
    "        chi_rows = []\n",
    "\n",
    "        levels = sorted(df_stage_base[\"_stage_\"].dropna().unique().tolist())\n",
    "        for level in levels:\n",
    "            df_s = df_stage_base[df_stage_base[\"_stage_\"] == level].copy()\n",
    "            if df_s.shape[0] < 30 or int(df_s[event_col_exact].sum()) < MIN_EVENTS:\n",
    "                continue\n",
    "\n",
    "            for _, r in cox_tab.iterrows():\n",
    "                segid = r[\"SegmentID_exact\"]\n",
    "                cnv_type = r[\"CNV_Type\"]\n",
    "                var = make_var_exact(segid, cnv_type)\n",
    "                if var not in df_s.columns:\n",
    "                    continue\n",
    "\n",
    "                cols = [time_col_exact, event_col_exact, var] + covars\n",
    "                tmp = df_s[cols].dropna()\n",
    "                if tmp.shape[0] < 30 or int(tmp[event_col_exact].sum()) < MIN_EVENTS:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    cph.fit(tmp, duration_col=time_col_exact, event_col=event_col_exact)\n",
    "                    s = cph.summary.loc[var]\n",
    "                    rows.append({\n",
    "                        \"stage_col\": stage_col,\n",
    "                        \"stage_level\": level,\n",
    "                        \"SegmentID_exact\": segid,\n",
    "                        \"CNV_Type\": cnv_type,\n",
    "                        \"N_used\": int(tmp.shape[0]),\n",
    "                        \"Events_used\": int(tmp[event_col_exact].sum()),\n",
    "                        \"HR\": float(np.exp(s[\"coef\"])),\n",
    "                        \"CI95_low\": float(np.exp(s[\"coef lower 95%\"])),\n",
    "                        \"CI95_high\": float(np.exp(s[\"coef upper 95%\"])),\n",
    "                        \"p\": float(s[\"p\"]),\n",
    "                    })\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Qui-quadrado (presença vs nível) – no dataset inteiro do stage_col\n",
    "            if HAVE_SCIPY:\n",
    "                for _, r in cox_tab.iterrows():\n",
    "                    segid = r[\"SegmentID_exact\"]\n",
    "                    cnv_type = r[\"CNV_Type\"]\n",
    "                    var = make_var_exact(segid, cnv_type)\n",
    "                    if var not in df_stage_base.columns:\n",
    "                        continue\n",
    "                    tmp2 = df_stage_base[[var, \"_stage_\"]].dropna()\n",
    "                    if tmp2[\"_stage_\"].nunique() < 2:\n",
    "                        continue\n",
    "                    ct = pd.crosstab(tmp2[\"_stage_\"], tmp2[var])\n",
    "                    if 0 not in ct.columns:\n",
    "                        ct[0] = 0\n",
    "                    if 1 not in ct.columns:\n",
    "                        ct[1] = 0\n",
    "                    ct = ct[[0,1]]\n",
    "                    if ct.values.sum() < 50:\n",
    "                        continue\n",
    "                    try:\n",
    "                        chi2, pval, dof, exp = chi2_contingency(ct.values)\n",
    "                        chi_rows.append({\n",
    "                            \"stage_col\": stage_col,\n",
    "                            \"SegmentID_exact\": segid,\n",
    "                            \"CNV_Type\": cnv_type,\n",
    "                            \"chi2\": float(chi2),\n",
    "                            \"dof\": int(dof),\n",
    "                            \"p\": float(pval),\n",
    "                            \"n_used\": int(ct.values.sum())\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        slug = re.sub(r\"[^A-Za-z0-9]+\", \"_\", stage_col).strip(\"_\")\n",
    "        if rows:\n",
    "            stage_tab = pd.DataFrame(rows).sort_values([\"stage_level\",\"p\"], na_position=\"last\")\n",
    "            out_stage = os.path.join(RES_DIR, f\"top{TOP_K}_cox_by_{slug}__segment_exact.tsv\")\n",
    "            stage_tab.to_csv(out_stage, sep=\"\\t\", index=False)\n",
    "            print(f\"[OK] Cox estratificado (SegmentID exato) ({stage_col}) salvo: {out_stage}\")\n",
    "        else:\n",
    "            print(f\"[WARN] Sem modelos Cox suficientes para {stage_col} (N/eventos baixos).\")\n",
    "\n",
    "        if HAVE_SCIPY and chi_rows:\n",
    "            chi_tab = pd.DataFrame(chi_rows).sort_values(\"p\", na_position=\"last\")\n",
    "            out_chi = os.path.join(RES_DIR, f\"top{TOP_K}_chi2_by_{slug}__segment_exact.tsv\")\n",
    "            chi_tab.to_csv(out_chi, sep=\"\\t\", index=False)\n",
    "            print(f\"[OK] Qui-quadrado CNV↔estágio (SegmentID exato) ({stage_col}) salvo: {out_chi}\")\n",
    "        elif not HAVE_SCIPY:\n",
    "            print(\"[INFO] SciPy não disponível — pulando qui-quadrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axOl1En5So6D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "axOl1En5So6D",
    "outputId": "4b8bd38b-b61e-42aa-d212-d8a05da60182"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 22) CHECKLIST PUBLICÁVEL (sanity checks) — sem dados inventados\n",
    "# =========================================================\n",
    "import os, sys, platform\n",
    "import pandas as pd\n",
    "\n",
    "print(\"[ENV] python:\", sys.version.split()[0], \"| platform:\", platform.platform())\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"[ENV] numpy:\", np.__version__)\n",
    "except: pass\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"[ENV] pandas:\", pd.__version__)\n",
    "except: pass\n",
    "try:\n",
    "    import pyranges as pr\n",
    "    print(\"[ENV] pyranges:\", pr.__version__)\n",
    "except: pass\n",
    "try:\n",
    "    import lifelines\n",
    "    print(\"[ENV] lifelines:\", lifelines.__version__)\n",
    "except: pass\n",
    "\n",
    "# diretórios\n",
    "RAW_DIR  = globals().get(\"RAW_DIR\",  os.path.join(BASE_OUT_DIR, \"raw\"))\n",
    "PROC_DIR = globals().get(\"PROC_DIR\", os.path.join(BASE_OUT_DIR, \"processed\"))\n",
    "RES_DIR  = globals().get(\"RES_DIR\",  os.path.join(BASE_OUT_DIR, \"results\"))\n",
    "LOG_DIR  = globals().get(\"LOG_DIR\",  os.path.join(BASE_OUT_DIR, \"logs\"))\n",
    "\n",
    "print(\"[PATHS]\", \"RAW:\", RAW_DIR)\n",
    "print(\"[PATHS]\", \"PROC:\", PROC_DIR)\n",
    "print(\"[PATHS]\", \"RES:\", RES_DIR)\n",
    "print(\"[PATHS]\", \"LOG:\", LOG_DIR)\n",
    "\n",
    "# 1) dados em memória (se o usuário executou as etapas)\n",
    "if \"df_cnvs_hc\" in globals() and isinstance(df_cnvs_hc, pd.DataFrame):\n",
    "    print(\"[DATA] df_cnvs_hc:\", df_cnvs_hc.shape, \"| pacientes:\", df_cnvs_hc[\"Participant_ID\"].nunique())\n",
    "    if \"CNV_Type_Ajustado\" in df_cnvs_hc.columns:\n",
    "        print(\"[DATA] CNV_Type_Ajustado counts:\")\n",
    "        display(df_cnvs_hc[\"CNV_Type_Ajustado\"].value_counts().head(10))\n",
    "else:\n",
    "    print(\"[WARN] df_cnvs_hc não está em memória (rode etapas 6–8).\")\n",
    "\n",
    "if \"os_df\" in globals() and isinstance(os_df, pd.DataFrame):\n",
    "    print(\"[DATA] os_df:\", os_df.shape, \"| pacientes:\", os_df[\"Participant_ID\"].nunique())\n",
    "else:\n",
    "    print(\"[WARN] os_df não está em memória (rode etapa 13).\")\n",
    "\n",
    "# 2) arquivos esperados (publicação/recurrent CNV)\n",
    "BIN_SIZE = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "\n",
    "expected = [\n",
    "    os.path.join(PROC_DIR, \"cnv_patient_cytoband_overlaps.tsv\"),\n",
    "    os.path.join(RES_DIR,  \"cnv_recurrence_by_cytoband.tsv\"),\n",
    "    os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_SIZE//1_000_000}Mb.tsv\"),\n",
    "    os.path.join(RES_DIR,  f\"cnv_recurrence_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\"),\n",
    "    os.path.join(RES_DIR,  f\"top{TOP_K}_recurrent_regions_by_bins_{BIN_SIZE//1_000_000}Mb.tsv\"),\n",
    "    os.path.join(RES_DIR,  f\"top{TOP_K}_recurrent_regions_cox_table.tsv\"),\n",
    "    os.path.join(RES_DIR,  f\"top{TOP_K}_km_summary.tsv\"),\n",
    "]\n",
    "for p in expected:\n",
    "    print(\"[FILE]\", (\"OK\" if os.path.exists(p) else \"MISSING\"), \"→\", p)\n",
    "\n",
    "# 3) garante que não houve imputação silenciosa: procura por colunas de tempo/evento não-numéricas\n",
    "if \"os_df\" in globals() and isinstance(os_df, pd.DataFrame):\n",
    "    tc = pick_col(os_df, [\"time\",\"OS_time\",\"OS_days\"])\n",
    "    ec = pick_col(os_df, [\"event\",\"OS_event\",\"OS_status\"])\n",
    "    if tc and ec:\n",
    "        print(\"[CHECK] time dtype:\", os_df[tc].dtype, \"| event dtype:\", os_df[ec].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ZcjLTrLc9jY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZcjLTrLc9jY",
    "outputId": "4a85e1e6-a731-4d97-e118-b9b9d0f61aa1"
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# achar RES_DIR automaticamente\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or globals().get(\"OUT_RESULTS_DIR\")\n",
    "if RES_DIR is None:\n",
    "    RES_DIR = os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "merged_path = os.path.join(RES_DIR, \"survival_features_merged.tsv\")\n",
    "\n",
    "if \"df_surv\" in globals() and isinstance(df_surv, pd.DataFrame) and {\"time\",\"event\"}.issubset(df_surv.columns):\n",
    "    df_ml = df_surv.copy()\n",
    "elif os.path.exists(merged_path):\n",
    "    df_ml = pd.read_csv(merged_path, sep=\"\\t\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não achei df_surv em memória nem survival_features_merged.tsv em RES_DIR.\")\n",
    "\n",
    "# colunas essenciais\n",
    "for c in [\"Participant_ID\",\"time\",\"event\"]:\n",
    "    if c not in df_ml.columns:\n",
    "        raise RuntimeError(f\"Dataset survival sem coluna obrigatória: {c}\")\n",
    "\n",
    "# remove linhas inválidas\n",
    "df_ml = df_ml.dropna(subset=[\"time\",\"event\"]).copy()\n",
    "df_ml = df_ml[df_ml[\"time\"] >= 0].copy()\n",
    "\n",
    "# define features: tudo que é numérico e NÃO é time/event/ID\n",
    "drop_cols = {\"Participant_ID\",\"time\",\"event\"}\n",
    "num_cols = [c for c in df_ml.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(df_ml[c])]\n",
    "if len(num_cols) < 3:\n",
    "    raise RuntimeError(f\"Poucas features numéricas detectadas: {num_cols}\")\n",
    "\n",
    "# tira colunas constantes\n",
    "good = []\n",
    "for c in num_cols:\n",
    "    if df_ml[c].nunique(dropna=True) > 1:\n",
    "        good.append(c)\n",
    "num_cols = good\n",
    "\n",
    "# treino/teste estratificado por evento\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df_ml.index, test_size=0.25, random_state=42,\n",
    "    stratify=df_ml[\"event\"].astype(int)\n",
    ")\n",
    "train_df = df_ml.loc[train_idx].copy()\n",
    "test_df  = df_ml.loc[test_idx].copy()\n",
    "\n",
    "print(\"[OK] Dataset:\", df_ml.shape, \"| Eventos:\", int(df_ml[\"event\"].sum()))\n",
    "print(\"[OK] Features numéricas usadas:\", len(num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iJGLe3ZgpNPE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJGLe3ZgpNPE",
    "outputId": "43ae358b-b6d8-46c1-83d6-0df96258ee81"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SUPPL FIG) Pearson Correlation: CNV Burden vs Clinical\n",
    "# (gera heatmap + matriz TSV em results/)\n",
    "# =========================================================\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# resolve pasta de resultados sem depender de uma única variável\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or os.path.join(globals().get(\"BASE_OUT_DIR\", \".\"), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# checagens mínimas\n",
    "for _v in [\"df_cnvs_hc\", \"os_df\"]:\n",
    "    if _v not in globals():\n",
    "        raise RuntimeError(f\"Variável obrigatória não encontrada em memória: {_v}. Rode as etapas anteriores.\")\n",
    "\n",
    "# --- CNV burden por cromossomo (contagem de segmentos HC) ---\n",
    "burden = (\n",
    "    df_cnvs_hc.groupby([\"Participant_ID\",\"Chromosome\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# padroniza nomes chr*\n",
    "def _chr_name(c):\n",
    "    c = str(c)\n",
    "    return \"chr\" + c.replace(\"chr\",\"\")\n",
    "burden.columns = [_chr_name(c) for c in burden.columns]\n",
    "\n",
    "# ordena cromossomos canônicos\n",
    "ordered = [f\"chr{i}\" for i in range(1,23)] + [\"chrX\",\"chrY\",\"chrM\"]\n",
    "cols = [c for c in ordered if c in burden.columns] + [c for c in burden.columns if c not in ordered]\n",
    "burden = burden[cols]\n",
    "\n",
    "burden[\"Total_CNVs\"] = burden.sum(axis=1)\n",
    "\n",
    "# --- merge com clinical/os_df (somente colunas numéricas úteis) ---\n",
    "clin = os_df.set_index(\"Participant_ID\").copy()\n",
    "\n",
    "# indicadores opcionais\n",
    "if \"demographic.gender\" in clin.columns:\n",
    "    g = clin[\"demographic.gender\"].astype(str).str.lower()\n",
    "    clin[\"demographic.gender_male\"] = g.isin([\"male\",\"m\"]).astype(int)\n",
    "\n",
    "# “High burden” (top quartil)\n",
    "thr = burden[\"Total_CNVs\"].quantile(0.75)\n",
    "clin[\"Is_High_CNV_Burden\"] = (burden[\"Total_CNVs\"] >= thr).astype(int)\n",
    "\n",
    "# candidatos clínicos (usa apenas os que existirem)\n",
    "cand = [\n",
    "    \"age\",\n",
    "    \"time\",\"event\",\n",
    "    \"demographic.days_to_death\",\n",
    "    \"diagnoses.days_to_last_follow_up\",\n",
    "    \"follow_ups.days_to_follow_up_max\",\n",
    "    \"demographic.gender_male\",\n",
    "    \"Is_High_CNV_Burden\",\n",
    "]\n",
    "cand = [c for c in cand if c in clin.columns]\n",
    "\n",
    "corr_df = burden.join(clin[cand], how=\"inner\")\n",
    "corr_df = corr_df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "corr = corr_df.corr(method=\"pearson\")\n",
    "\n",
    "# --- plot heatmap com anotações ---\n",
    "# plot (mesmo estilo vermelho/azul do “clássico”)\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(corr.values, aspect=\"auto\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "  # <-- aqui\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "ax.set_xticks(range(len(corr.columns)))\n",
    "ax.set_xticklabels(corr.columns, rotation=90, fontsize=7)\n",
    "ax.set_yticks(range(len(corr.index)))\n",
    "ax.set_yticklabels(corr.index, fontsize=7)\n",
    "\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(corr.shape[1]):\n",
    "        ax.text(j, i, f\"{corr.values[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=6)\n",
    "\n",
    "ax.set_title(\"Pearson Correlation Matrix: CNV Burden vs. Clinical Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = os.path.join(RES_DIR, \"pearson_corr_cnv_burden_vs_clinical.png\")\n",
    "plt.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "out_tsv = os.path.join(RES_DIR, \"pearson_corr_cnv_burden_vs_clinical.tsv\")\n",
    "corr.to_csv(out_tsv, sep=\"\\t\")\n",
    "\n",
    "print(\"[OK] Heatmap:\", out_png)\n",
    "print(\"[OK] Matrix TSV:\", out_tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hPkYkZsPpNPE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "hPkYkZsPpNPE",
    "outputId": "3d547393-7b0e-4798-ccd4-4b249bacef9d"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# MAIN FIG) KM para TOP CNVs (presença/ausência) — estilo banner\n",
    "# (gera plots + km_logrank_summary.tsv em results/)\n",
    "# =========================================================\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\") or globals().get(\"RESULTS_DIR\") or os.path.join(globals().get(\"BASE_OUT_DIR\", \".\"), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "KM_MIN_GROUP = int(globals().get(\"KM_MIN_GROUP\", 20))\n",
    "\n",
    "# precisa ter df_model_base/time_col/event_col gerados no bloco recurrent CNV\n",
    "for _v in [\"df_model_base\", \"time_col\", \"event_col\"]:\n",
    "    if _v not in globals():\n",
    "        raise RuntimeError(f\"Variável obrigatória não encontrada em memória: {_v}. Rode as etapas recurrent CNV antes.\")\n",
    "\n",
    "df_base = df_model_base.dropna(subset=[time_col, event_col]).copy()\n",
    "\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_cox_table.tsv\")\n",
    "if not os.path.exists(cox_path):\n",
    "    raise FileNotFoundError(f\"Não achei {cox_path}. Rode a célula do COX recurrent CNV antes.\")\n",
    "\n",
    "cox_tab = pd.read_csv(cox_path, sep=\"\\t\")\n",
    "\n",
    "km_dir = os.path.join(RES_DIR, \"KM_TOPCNV_PRESENCE\")\n",
    "os.makedirs(km_dir, exist_ok=True)\n",
    "\n",
    "def safe_name(s):\n",
    "    s = re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", str(s))\n",
    "    return s[:180]\n",
    "\n",
    "def make_var(binid, cnv_type):\n",
    "    return \"CNV__\" + str(cnv_type).replace(\" \", \"_\") + \"__\" + str(binid).replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "km_results = []\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "for _, r in cox_tab.head(TOP_K).iterrows():\n",
    "    binid = r.get(\"Region_BinID\")\n",
    "    cnv_type = r.get(\"CNV_Type\")\n",
    "    if pd.isna(binid) or pd.isna(cnv_type):\n",
    "        continue\n",
    "\n",
    "    var = make_var(binid, cnv_type)\n",
    "    if var not in df_base.columns:\n",
    "        continue\n",
    "\n",
    "    d = df_base[[time_col, event_col, var]].dropna().copy()\n",
    "    d[var] = pd.to_numeric(d[var], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    g0 = d[d[var]==0]\n",
    "    g1 = d[d[var]==1]\n",
    "\n",
    "    if g0.shape[0] < KM_MIN_GROUP or g1.shape[0] < KM_MIN_GROUP:\n",
    "        continue\n",
    "\n",
    "    lr = logrank_test(\n",
    "        g1[time_col], g0[time_col],\n",
    "        event_observed_A=g1[event_col],\n",
    "        event_observed_B=g0[event_col]\n",
    "    )\n",
    "    p_lr = float(lr.p_value)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    kmf.fit(g0[time_col], event_observed=g0[event_col], label=f\"No CNV (N={g0.shape[0]})\")\n",
    "    ax = kmf.plot()\n",
    "    kmf.fit(g1[time_col], event_observed=g1[event_col], label=f\"With CNV (N={g1.shape[0]})\")\n",
    "    kmf.plot(ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Kaplan–Meier survival para {binid} ({cnv_type})\")\n",
    "    ax.set_xlabel(\"Time (days)\")\n",
    "    ax.set_ylabel(\"Survival probability\")\n",
    "    ax.text(0.62, 0.74, f\"Log-rank p: {p_lr:.4f}\", transform=ax.transAxes,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"gray\"))\n",
    "\n",
    "    fig_path = os.path.join(km_dir, f\"KM__{safe_name(binid)}__{safe_name(cnv_type)}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    km_results.append({\n",
    "        \"Region_BinID\": binid,\n",
    "        \"CNV_Type\": cnv_type,\n",
    "        \"N_no_CNV\": int(g0.shape[0]),\n",
    "        \"N_with_CNV\": int(g1.shape[0]),\n",
    "        \"Events_no_CNV\": int(pd.to_numeric(g0[event_col], errors=\"coerce\").fillna(0).sum()),\n",
    "        \"Events_with_CNV\": int(pd.to_numeric(g1[event_col], errors=\"coerce\").fillna(0).sum()),\n",
    "        \"logrank_p\": p_lr,\n",
    "        \"plot\": os.path.basename(fig_path)\n",
    "    })\n",
    "\n",
    "km_tab = pd.DataFrame(km_results)\n",
    "out_km = os.path.join(RES_DIR, \"km_logrank_summary.tsv\")\n",
    "\n",
    "if km_tab.empty:\n",
    "    km_tab = pd.DataFrame(columns=[\"Region_BinID\",\"CNV_Type\",\"N_no_CNV\",\"N_with_CNV\",\"Events_no_CNV\",\"Events_with_CNV\",\"logrank_p\",\"plot\"])\n",
    "    km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "    print(f\"[INFO] Nenhuma CNV em TOP{TOP_K} passou KM_MIN_GROUP={KM_MIN_GROUP}. Resumo vazio salvo:\", out_km)\n",
    "else:\n",
    "    km_tab = km_tab.sort_values(\"logrank_p\", na_position=\"last\")\n",
    "    km_tab.to_csv(out_km, sep=\"\\t\", index=False)\n",
    "    print(\"[OK] KM plots em:\", km_dir)\n",
    "    print(\"[OK] Resumo KM:\", out_km)\n",
    "    display(km_tab.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zrhR65DbBEVM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zrhR65DbBEVM",
    "outputId": "69196772-27eb-4923-e459-b37ff08a262c"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# FIND + PREVIEW: KM plots gerados\n",
    "# =========================================================\n",
    "import os, glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", None)\n",
    "if RES_DIR is None:\n",
    "    # fallback: tenta achar o results mais recente\n",
    "    cand = sorted(glob.glob(\"outputs/run_*/results\"), reverse=True)\n",
    "    if not cand:\n",
    "        raise RuntimeError(\"Não achei RES_DIR nem outputs/run_*/results.\")\n",
    "    RES_DIR = cand[0]\n",
    "\n",
    "km_dir = os.path.join(RES_DIR, \"KM_TOPCNV_PRESENCE\")\n",
    "print(\"[INFO] RES_DIR =\", RES_DIR)\n",
    "print(\"[INFO] KM dir  =\", km_dir)\n",
    "\n",
    "pngs = sorted(glob.glob(os.path.join(km_dir, \"*.png\")))\n",
    "print(\"[OK] PNGs encontrados:\", len(pngs))\n",
    "for p in pngs[:15]:\n",
    "    print(\" -\", os.path.basename(p))\n",
    "\n",
    "# preview dos primeiros 6\n",
    "for p in pngs[:6]:\n",
    "    display(Image(filename=p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h7ZdcDDNClQy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "h7ZdcDDNClQy",
    "outputId": "5bbf0304-3d8a-49e9-85a7-87471b53eeba"
   },
   "outputs": [],
   "source": [
    "import os, glob, pandas as pd\n",
    "\n",
    "RES_DIR = globals().get(\"RES_DIR\", None)\n",
    "if RES_DIR is None:\n",
    "    cand = sorted(glob.glob(\"outputs/run_*/results\"), reverse=True)\n",
    "    RES_DIR = cand[0] if cand else None\n",
    "print(\"[INFO] RES_DIR =\", RES_DIR)\n",
    "\n",
    "TOP_K = int(globals().get(\"TOP_K_TOPCNV\", 10))\n",
    "cox_path = os.path.join(RES_DIR, f\"top{TOP_K}_recurrent_regions_cox_table.tsv\")\n",
    "print(\"[INFO] cox_path =\", cox_path, \"| exists:\", os.path.exists(cox_path))\n",
    "\n",
    "cox_tab = pd.read_csv(cox_path, sep=\"\\t\") if os.path.exists(cox_path) else pd.DataFrame()\n",
    "print(\"\\n[INFO] Cox columns:\", list(cox_tab.columns))\n",
    "display(cox_tab.head(3))\n",
    "\n",
    "# mostra como os CNV features existem no df_model_base\n",
    "cnv_cols = [c for c in df_model_base.columns if str(c).startswith(\"CNV__\")]\n",
    "print(\"\\n[INFO] CNV feature cols em df_model_base:\", len(cnv_cols))\n",
    "print(\"Exemplos:\", cnv_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxo5SCp3Zxtt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "mxo5SCp3Zxtt",
    "outputId": "8e610fc1-9cbc-4972-ae2e-58265a9841c5"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 22.9) PREDICTION DATASET: global features (BP/hotspots/age/sex) + locus TOP-K (1Mb bins)\n",
    "# - Builds df_pred used by the \"23) Prediction models\" section\n",
    "# - If the TOP-K file is missing, it is generated from the patient-bin overlap table\n",
    "# Outputs are saved in RES_DIR\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- directories ---\n",
    "RES_DIR  = globals().get(\"RES_DIR\") or os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "PROC_DIR = globals().get(\"PROC_DIR\") or os.path.join(globals().get(\"BASE_OUT_DIR\", os.getcwd()), \"processed\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# --- parameters ---\n",
    "BIN_SIZE   = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "BIN_MB     = BIN_SIZE // 1_000_000\n",
    "TOP_K_PRED = int(globals().get(\"TOP_K_PRED\", 50))  # increase to 100 if needed\n",
    "\n",
    "# --- base features ---\n",
    "merged_path = os.path.join(RES_DIR, \"survival_features_merged.tsv\")\n",
    "if \"features_df\" in globals() and isinstance(features_df, pd.DataFrame) and not features_df.empty:\n",
    "    base = features_df.copy()\n",
    "elif os.path.exists(merged_path):\n",
    "    base = pd.read_csv(merged_path, sep=\"\\t\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Missing base features: neither `features_df` in memory nor `survival_features_merged.tsv` in RES_DIR.\")\n",
    "\n",
    "# (optional) bring ISS from os_df (survival_features_merged.tsv may not include it)\n",
    "if \"os_df\" in globals() and isinstance(os_df, pd.DataFrame) and \"diagnoses.iss_stage\" in os_df.columns:\n",
    "    base = base.merge(os_df[[\"Participant_ID\", \"diagnoses.iss_stage\"]], on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "# --- locus table inputs ---\n",
    "edges_path = os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_MB}Mb.tsv\")\n",
    "top_path   = os.path.join(RES_DIR,  f\"top{TOP_K_PRED}_recurrent_regions_by_bins_{BIN_MB}Mb.tsv\")\n",
    "\n",
    "if not os.path.exists(edges_path):\n",
    "    raise FileNotFoundError(f\"Missing overlap table: {edges_path}. Run the CNV x bin overlap export step first.\")\n",
    "\n",
    "# read edges with safer dtypes (avoid mixed-type warnings)\n",
    "edges = pd.read_csv(edges_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# detect required columns robustly\n",
    "cols = {c.lower(): c for c in edges.columns}\n",
    "participant_col = cols.get(\"participant_id\") or cols.get(\"participant\") or cols.get(\"patient_id\")\n",
    "bin_col = cols.get(\"binid\") or cols.get(\"bin_id\") or cols.get(\"bin\")\n",
    "type_col = cols.get(\"cnv_type_ajustado\") or cols.get(\"cnv_type\") or cols.get(\"type\")\n",
    "\n",
    "missing = [x for x in [participant_col, bin_col, type_col] if x is None]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Could not detect required columns in overlap table. Found columns: {list(edges.columns)[:40]}\")\n",
    "\n",
    "# normalize dtypes\n",
    "edges[participant_col] = edges[participant_col].astype(str)\n",
    "edges[bin_col] = edges[bin_col].astype(str)\n",
    "edges[type_col] = edges[type_col].astype(str)\n",
    "\n",
    "# --- build TOP-K if missing ---\n",
    "if not os.path.exists(top_path):\n",
    "    n_total = int(edges[participant_col].nunique())\n",
    "    freq = (\n",
    "        edges.groupby([bin_col, type_col])[participant_col]\n",
    "        .nunique()\n",
    "        .reset_index(name=\"n_patients\")\n",
    "    )\n",
    "    freq[\"pct_patients\"] = (freq[\"n_patients\"] / max(n_total, 1)) * 100.0\n",
    "    freq = freq.sort_values([\"pct_patients\", \"n_patients\"], ascending=False).head(TOP_K_PRED).copy()\n",
    "\n",
    "    # standardize column names for downstream\n",
    "    freq = freq.rename(columns={bin_col: \"BinID\", type_col: \"CNV_Type\"})\n",
    "    freq.to_csv(top_path, sep=\"\\t\", index=False)\n",
    "    print(\"[OK] TOP-K recurrent regions file generated:\", top_path)\n",
    "else:\n",
    "    freq = pd.read_csv(top_path, sep=\"\\t\")\n",
    "    # accept either legacy or new column naming\n",
    "    if \"BinID\" not in freq.columns:\n",
    "        if bin_col in freq.columns:\n",
    "            freq = freq.rename(columns={bin_col: \"BinID\"})\n",
    "    if \"CNV_Type\" not in freq.columns:\n",
    "        if type_col in freq.columns:\n",
    "            freq = freq.rename(columns={type_col: \"CNV_Type\"})\n",
    "    print(\"[OK] TOP-K recurrent regions file found:\", top_path)\n",
    "\n",
    "# --- make safe variable names: CNV__<TYPE>__<BinID> ---\n",
    "def _slug(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\":\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "freq = freq.copy()\n",
    "freq[\"var\"] = \"CNV__\" + freq[\"CNV_Type\"].map(_slug) + \"__\" + freq[\"BinID\"].map(_slug)\n",
    "\n",
    "keep = freq[[\"BinID\", \"CNV_Type\", \"var\"]].drop_duplicates()\n",
    "\n",
    "# align edges column names to expected \"BinID\"/\"CNV_Type\"\n",
    "edges2 = edges.rename(columns={bin_col: \"BinID\", type_col: \"CNV_Type\", participant_col: \"Participant_ID\"}).copy()\n",
    "\n",
    "edges2 = (\n",
    "    edges2.merge(keep, on=[\"BinID\", \"CNV_Type\"], how=\"inner\")\n",
    "         .drop_duplicates([\"Participant_ID\", \"var\"])\n",
    ")\n",
    "edges2[\"val\"] = 1\n",
    "\n",
    "X_locus = (\n",
    "    edges2.pivot(index=\"Participant_ID\", columns=\"var\", values=\"val\")\n",
    "    .fillna(0).astype(int)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_pred = base.merge(X_locus, on=\"Participant_ID\", how=\"left\")\n",
    "locus_cols = [c for c in df_pred.columns if c.startswith(\"CNV__\")]\n",
    "df_pred[locus_cols] = df_pred[locus_cols].fillna(0).astype(int)\n",
    "\n",
    "# --- save dataset for auditability ---\n",
    "pred_path = os.path.join(RES_DIR, f\"prediction_dataset_top{TOP_K_PRED}_bins{BIN_MB}Mb.tsv\")\n",
    "df_pred.to_csv(pred_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"[OK] df_pred:\", df_pred.shape, \"| locus cols:\", len(locus_cols))\n",
    "print(\"[OK] saved:\", pred_path)\n",
    "df_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8vIr7OxSscad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "8vIr7OxSscad",
    "outputId": "dd67128d-15fb-41ce-d602-b0643239eaf1"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# PREDICTION DATASET: global features (BP/hotspots/age/sex) + locus TOP-K (1Mb bins)\n",
    "# - Builds df_pred for the predictive models section\n",
    "# - If the TOP-K file is missing, it is generated from the patient-bin overlap table\n",
    "# Outputs are saved in RES_DIR\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- directories ---\n",
    "RES_DIR  = globals().get(\"RES_DIR\") or os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "PROC_DIR = globals().get(\"PROC_DIR\") or os.path.join(globals().get(\"BASE_OUT_DIR\", os.getcwd()), \"processed\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# --- parameters ---\n",
    "BIN_SIZE   = int(globals().get(\"BIN_SIZE\", 1_000_000))\n",
    "BIN_MB     = BIN_SIZE // 1_000_000\n",
    "TOP_K_PRED = int(globals().get(\"TOP_K_PRED\", 50))  # increase to 100 if needed\n",
    "\n",
    "# --- base features ---\n",
    "merged_path = os.path.join(RES_DIR, \"survival_features_merged.tsv\")\n",
    "if \"features_df\" in globals() and isinstance(features_df, pd.DataFrame) and not features_df.empty:\n",
    "    base = features_df.copy()\n",
    "elif os.path.exists(merged_path):\n",
    "    base = pd.read_csv(merged_path, sep=\"\\t\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Missing base features: neither `features_df` in memory nor `survival_features_merged.tsv` in RES_DIR.\")\n",
    "\n",
    "# (optional) bring ISS from os_df (survival_features_merged.tsv may not include it)\n",
    "if \"os_df\" in globals() and isinstance(os_df, pd.DataFrame) and \"diagnoses.iss_stage\" in os_df.columns:\n",
    "    base = base.merge(os_df[[\"Participant_ID\", \"diagnoses.iss_stage\"]], on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "# --- locus table inputs ---\n",
    "edges_path = os.path.join(PROC_DIR, f\"cnv_patient_bin_overlaps_{BIN_MB}Mb.tsv\")\n",
    "top_path   = os.path.join(RES_DIR,  f\"top{TOP_K_PRED}_recurrent_regions_by_bins_{BIN_MB}Mb.tsv\")\n",
    "\n",
    "if not os.path.exists(edges_path):\n",
    "    raise FileNotFoundError(f\"Missing overlap table: {edges_path}. Run the CNV x bin overlap export step first.\")\n",
    "\n",
    "# read edges with safer dtypes (avoid mixed-type warnings)\n",
    "edges = pd.read_csv(edges_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "# detect required columns robustly\n",
    "cols = {c.lower(): c for c in edges.columns}\n",
    "participant_col = cols.get(\"participant_id\") or cols.get(\"participant\") or cols.get(\"patient_id\")\n",
    "bin_col = cols.get(\"binid\") or cols.get(\"bin_id\") or cols.get(\"bin\")\n",
    "type_col = cols.get(\"cnv_type_ajustado\") or cols.get(\"cnv_type\") or cols.get(\"type\")\n",
    "\n",
    "missing = [x for x in [participant_col, bin_col, type_col] if x is None]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Could not detect required columns in overlap table. Found columns: {list(edges.columns)[:40]}\")\n",
    "\n",
    "# normalize dtypes\n",
    "edges[participant_col] = edges[participant_col].astype(str)\n",
    "edges[bin_col] = edges[bin_col].astype(str)\n",
    "edges[type_col] = edges[type_col].astype(str)\n",
    "\n",
    "# --- build TOP-K if missing ---\n",
    "if not os.path.exists(top_path):\n",
    "    n_total = int(edges[participant_col].nunique())\n",
    "    freq = (\n",
    "        edges.groupby([bin_col, type_col])[participant_col]\n",
    "        .nunique()\n",
    "        .reset_index(name=\"n_patients\")\n",
    "    )\n",
    "    freq[\"pct_patients\"] = (freq[\"n_patients\"] / max(n_total, 1)) * 100.0\n",
    "    freq = freq.sort_values([\"pct_patients\", \"n_patients\"], ascending=False).head(TOP_K_PRED).copy()\n",
    "\n",
    "    # standardize column names for downstream\n",
    "    freq = freq.rename(columns={bin_col: \"BinID\", type_col: \"CNV_Type\"})\n",
    "    freq.to_csv(top_path, sep=\"\\t\", index=False)\n",
    "    print(\"[OK] TOP-K recurrent regions file generated:\", top_path)\n",
    "else:\n",
    "    freq = pd.read_csv(top_path, sep=\"\\t\")\n",
    "    # accept either legacy or new column naming\n",
    "    if \"BinID\" not in freq.columns:\n",
    "        if bin_col in freq.columns:\n",
    "            freq = freq.rename(columns={bin_col: \"BinID\"})\n",
    "    if \"CNV_Type\" not in freq.columns:\n",
    "        if type_col in freq.columns:\n",
    "            freq = freq.rename(columns={type_col: \"CNV_Type\"})\n",
    "    print(\"[OK] TOP-K recurrent regions file found:\", top_path)\n",
    "\n",
    "# --- make safe variable names: CNV__<TYPE>__<BinID> ---\n",
    "def _slug(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.replace(\":\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "freq = freq.copy()\n",
    "freq[\"var\"] = \"CNV__\" + freq[\"CNV_Type\"].map(_slug) + \"__\" + freq[\"BinID\"].map(_slug)\n",
    "\n",
    "keep = freq[[\"BinID\", \"CNV_Type\", \"var\"]].drop_duplicates()\n",
    "\n",
    "# align edges column names to expected \"BinID\"/\"CNV_Type\"\n",
    "edges2 = edges.rename(columns={bin_col: \"BinID\", type_col: \"CNV_Type\", participant_col: \"Participant_ID\"}).copy()\n",
    "\n",
    "edges2 = (\n",
    "    edges2.merge(keep, on=[\"BinID\", \"CNV_Type\"], how=\"inner\")\n",
    "         .drop_duplicates([\"Participant_ID\", \"var\"])\n",
    ")\n",
    "edges2[\"val\"] = 1\n",
    "\n",
    "X_locus = (\n",
    "    edges2.pivot(index=\"Participant_ID\", columns=\"var\", values=\"val\")\n",
    "    .fillna(0).astype(int)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_pred = base.merge(X_locus, on=\"Participant_ID\", how=\"left\")\n",
    "locus_cols = [c for c in df_pred.columns if c.startswith(\"CNV__\")]\n",
    "df_pred[locus_cols] = df_pred[locus_cols].fillna(0).astype(int)\n",
    "\n",
    "# --- save dataset for auditability ---\n",
    "pred_path = os.path.join(RES_DIR, f\"prediction_dataset_top{TOP_K_PRED}_bins{BIN_MB}Mb.tsv\")\n",
    "df_pred.to_csv(pred_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"[OK] df_pred:\", df_pred.shape, \"| locus cols:\", len(locus_cols))\n",
    "print(\"[OK] saved:\", pred_path)\n",
    "df_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QAuxFZmyBaz1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAuxFZmyBaz1",
    "outputId": "a95fd57a-e056-42c5-f0ce-2a8956df0807"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 23) PREDICTION MODELS (OPTIONAL / REPORTABLE)  [ANTI-LEAK + STABILITY]\n",
    "#  - Survival risk prediction (penalized Cox) -> test C-index (no leakage)\n",
    "#  - ISS stage prediction (multiclass) -> macro-F1 / accuracy\n",
    "#  Outputs saved in RES_DIR\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# A) Ensure RES_DIR exists\n",
    "# ---------------------------------------------------------\n",
    "RES_DIR = globals().get(\"RES_DIR\") or os.path.join(globals().get(\"OUT_DIR\", os.getcwd()), \"results\")\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Get df_pred (preferred) or df_ml\n",
    "# ---------------------------------------------------------\n",
    "if \"df_pred\" in globals() and isinstance(df_pred, pd.DataFrame) and not df_pred.empty:\n",
    "    df_model = df_pred.copy()\n",
    "elif \"df_ml\" in globals() and isinstance(df_ml, pd.DataFrame) and not df_ml.empty:\n",
    "    df_model = df_ml.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"Missing `df_pred` and `df_ml`. Run the prediction dataset build cell before this block.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Required survival columns\n",
    "# ---------------------------------------------------------\n",
    "req = [\"Participant_ID\", \"time\", \"event\"]\n",
    "for c in req:\n",
    "    if c not in df_model.columns:\n",
    "        raise RuntimeError(f\"Dataset is missing required column: {c}\")\n",
    "\n",
    "df_model = df_model.dropna(subset=[\"time\", \"event\"]).copy()\n",
    "df_model = df_model[df_model[\"time\"] >= 0].copy()\n",
    "df_model[\"event\"] = df_model[\"event\"].astype(int)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Numeric features (exclude ID/time/event)\n",
    "# ---------------------------------------------------------\n",
    "drop_cols = {\"Participant_ID\", \"time\", \"event\"}\n",
    "feat_cols = [\n",
    "    c for c in df_model.columns\n",
    "    if c not in drop_cols and pd.api.types.is_numeric_dtype(df_model[c])\n",
    "]\n",
    "\n",
    "# remove constants\n",
    "feat_cols = [c for c in feat_cols if df_model[c].nunique(dropna=True) > 1]\n",
    "if len(feat_cols) < 5:\n",
    "    raise RuntimeError(f\"Too few numeric features detected: {feat_cols}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Survival HOLDOUT split (stratified by event)\n",
    "# ---------------------------------------------------------\n",
    "train_idx, test_idx = train_test_split(\n",
    "    df_model.index, test_size=0.25, random_state=42,\n",
    "    stratify=df_model[\"event\"].values\n",
    ")\n",
    "tr = df_model.loc[train_idx].copy()\n",
    "te = df_model.loc[test_idx].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) ANTI-LEAKAGE: remove post-baseline / follow-up / outcome-derived fields\n",
    "# ---------------------------------------------------------\n",
    "leak_patterns = [\n",
    "    \"days_to_death\", \"death\", \"deceased\",\n",
    "    \"days_to_last_follow_up\", \"last_follow_up\",\n",
    "    \"follow_up_time\", \"follow_ups\", \"follow_up\", \"days_to_follow_up\",\n",
    "    \"time_to\", \"os_time\", \"pfs_time\",\n",
    "    \"overall_survival\", \"progression_free\"\n",
    "]\n",
    "\n",
    "feat_before = feat_cols[:]\n",
    "\n",
    "# 4.1 drop by name patterns\n",
    "feat_cols = [c for c in feat_cols if not any(p in c.lower() for p in leak_patterns)]\n",
    "removed_by_pattern = sorted(list(set(feat_before) - set(feat_cols)))\n",
    "\n",
    "# 4.2 near-zero variance in TRAIN\n",
    "var_series = tr[feat_cols].var(numeric_only=True)\n",
    "near_zero = var_series[var_series.fillna(0) < 1e-12].index.tolist()\n",
    "feat_cols = [c for c in feat_cols if c not in near_zero]\n",
    "\n",
    "# 4.3 highly correlated with time in TRAIN (often proxy/leakage)\n",
    "tmp = tr[feat_cols + [\"time\"]].copy()\n",
    "corr = tmp.corr(numeric_only=True)[\"time\"].drop(\"time\")\n",
    "high_corr = corr[abs(corr) > 0.95].index.tolist()\n",
    "feat_cols = [c for c in feat_cols if c not in high_corr]\n",
    "\n",
    "# 4.4 too many missing values in TRAIN\n",
    "na_rate = tr[feat_cols].isna().mean()\n",
    "too_many_na = na_rate[na_rate > 0.60].index.tolist()\n",
    "feat_cols = [c for c in feat_cols if c not in too_many_na]\n",
    "\n",
    "print(\"\\n[ANTI-LEAK SUMMARY]\")\n",
    "print(\"Features before:\", len(feat_before))\n",
    "print(\"Removed by pattern:\", len(removed_by_pattern), \"| ex:\", removed_by_pattern[:8])\n",
    "print(\"Removed near-zero var:\", len(near_zero), \"| ex:\", near_zero[:8])\n",
    "print(\"Removed corr(|time|)>0.95:\", len(high_corr), \"| ex:\", high_corr[:8])\n",
    "print(\"Removed NA>60%:\", len(too_many_na), \"| ex:\", too_many_na[:8])\n",
    "print(\"Features after anti-leak:\", len(feat_cols))\n",
    "\n",
    "if len(feat_cols) < 5:\n",
    "    raise RuntimeError(\"Too few features after anti-leakage filters. Review patterns/filters or feature set.\")\n",
    "\n",
    "# keep a copy for ISS BEFORE Cox-only stabilization\n",
    "feat_cols_iss = feat_cols[:]  # ISS uses the full anti-leak feature set\n",
    "\n",
    "# save final anti-leak feature list (Methods / auditability)\n",
    "feat_list_path = os.path.join(RES_DIR, \"pred_features_used_survival.txt\")\n",
    "with open(feat_list_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(feat_cols))\n",
    "print(\"[OK] survival features saved:\", feat_list_path)\n",
    "\n",
    "# =========================================================\n",
    "# 4B) EXTRA STABILITY FOR COX (SURVIVAL ONLY)\n",
    "# - remove very rare binary features (low prevalence)\n",
    "# - remove high collinearity among features\n",
    "# - remove simple complete separation patterns (binary)\n",
    "# =========================================================\n",
    "\n",
    "# 1) remove very rare binary features in TRAIN (e.g., <3% ones)\n",
    "rare_thresh = 0.03\n",
    "rare = []\n",
    "for c in feat_cols:\n",
    "    s = tr[c]\n",
    "    vals = set(pd.Series(s.dropna().unique()).tolist())\n",
    "    if vals.issubset({0, 1}):\n",
    "        freq = float(s.mean())\n",
    "        if freq < rare_thresh:\n",
    "            rare.append(c)\n",
    "\n",
    "feat_cols = [c for c in feat_cols if c not in rare]\n",
    "print(f\"[COX-STAB] removed rare binaries (<{rare_thresh*100:.1f}% ones):\", len(rare))\n",
    "\n",
    "# 2) remove high collinearity among features (TRAIN)\n",
    "Xcorr = tr[feat_cols].corr(numeric_only=True).abs()\n",
    "upper = Xcorr.where(np.triu(np.ones(Xcorr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "feat_cols = [c for c in feat_cols if c not in to_drop]\n",
    "print(\"[COX-STAB] removed by collinearity > 0.95:\", len(to_drop))\n",
    "\n",
    "# 3) remove binary features with (almost) complete separation wrt event\n",
    "sep = []\n",
    "for c in feat_cols:\n",
    "    s = tr[c]\n",
    "    vals = set(pd.Series(s.dropna().unique()).tolist())\n",
    "    if vals.issubset({0, 1}):\n",
    "        e1 = tr.loc[tr[\"event\"] == 1, c].dropna()\n",
    "        e0 = tr.loc[tr[\"event\"] == 0, c].dropna()\n",
    "        if len(e1) > 0 and len(e0) > 0:\n",
    "            # if one group has almost no 1s or almost all 1s while the other is opposite\n",
    "            p1 = float(e1.mean())\n",
    "            p0 = float(e0.mean())\n",
    "            if (p1 < 0.01 and p0 > 0.99) or (p0 < 0.01 and p1 > 0.99) or (p1 > 0.99 and p0 < 0.01) or (p0 > 0.99 and p1 < 0.01):\n",
    "                sep.append(c)\n",
    "\n",
    "feat_cols = [c for c in feat_cols if c not in sep]\n",
    "print(\"[COX-STAB] removed by separation (binary):\", len(sep), \"| ex:\", sep[:8])\n",
    "\n",
    "print(\"[COX-STAB] survival features after stabilization:\", len(feat_cols))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Penalized Cox (CV for penalizer/l1_ratio) + TEST C-index\n",
    "# ---------------------------------------------------------\n",
    "def fit_eval_cox(train_df, test_df, feats, penalizer, l1_ratio):\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    sca = StandardScaler()\n",
    "\n",
    "    Xtr = sca.fit_transform(imp.fit_transform(train_df[feats]))\n",
    "    Xte = sca.transform(imp.transform(test_df[feats]))\n",
    "\n",
    "    tr2 = train_df[[\"time\", \"event\"]].copy()\n",
    "    te2 = test_df[[\"time\", \"event\"]].copy()\n",
    "    tr2[feats] = Xtr\n",
    "    te2[feats] = Xte\n",
    "\n",
    "    try:\n",
    "        cph = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
    "    except TypeError:\n",
    "        cph = CoxPHFitter(penalizer=penalizer)\n",
    "\n",
    "    cph.fit(tr2, duration_col=\"time\", event_col=\"event\")\n",
    "    risk = cph.predict_partial_hazard(te2).values.ravel()\n",
    "    cidx = concordance_index(te2[\"time\"].values, -risk, te2[\"event\"].values)\n",
    "    return float(cidx), cph, risk\n",
    "\n",
    "# avoid unpenalized Cox (often unstable)\n",
    "pen_grid = [0.05, 0.1, 0.5, 1.0, 5.0]\n",
    "l1_grid  = [0.0, 0.5]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_event = tr[\"event\"].values\n",
    "\n",
    "best = {\"mean_cv_cindex\": -np.inf, \"penalizer\": None, \"l1_ratio\": None}\n",
    "rows = []\n",
    "\n",
    "for pen in pen_grid:\n",
    "    for l1 in l1_grid:\n",
    "        scores = []\n",
    "        for tr_i, va_i in cv.split(tr.index, y_event):\n",
    "            df_tr = tr.iloc[tr_i]\n",
    "            df_va = tr.iloc[va_i]\n",
    "            try:\n",
    "                cidx, _, _ = fit_eval_cox(df_tr, df_va, feat_cols, penalizer=pen, l1_ratio=l1)\n",
    "                scores.append(cidx)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if scores:\n",
    "            m = float(np.mean(scores))\n",
    "            rows.append({\"penalizer\": pen, \"l1_ratio\": l1, \"mean_cv_cindex\": m, \"n_folds_used\": len(scores)})\n",
    "            if m > best[\"mean_cv_cindex\"]:\n",
    "                best.update({\"mean_cv_cindex\": m, \"penalizer\": pen, \"l1_ratio\": l1})\n",
    "\n",
    "cv_df = pd.DataFrame(rows).sort_values(\"mean_cv_cindex\", ascending=False)\n",
    "cv_path = os.path.join(RES_DIR, \"pred_survival_cox_cv_grid.tsv\")\n",
    "cv_df.to_csv(cv_path, sep=\"\\t\", index=False)\n",
    "\n",
    "cidx_test, cph_final, risk_test = fit_eval_cox(tr, te, feat_cols, best[\"penalizer\"], best[\"l1_ratio\"])\n",
    "\n",
    "coef = cph_final.summary.reset_index().rename(columns={\"index\": \"feature\"})\n",
    "coef_path = os.path.join(RES_DIR, \"pred_survival_cox_coefficients.tsv\")\n",
    "coef.to_csv(coef_path, sep=\"\\t\", index=False)\n",
    "\n",
    "risk_out = te[[\"Participant_ID\", \"time\", \"event\"]].copy()\n",
    "risk_out[\"risk_score\"] = risk_test\n",
    "risk_path = os.path.join(RES_DIR, \"pred_survival_test_risk_scores.tsv\")\n",
    "risk_out.to_csv(risk_path, sep=\"\\t\", index=False)\n",
    "\n",
    "perf_path = os.path.join(RES_DIR, \"pred_survival_performance.tsv\")\n",
    "pd.DataFrame([{\n",
    "    \"model\": \"Penalized Cox (ANTI-LEAK)\",\n",
    "    \"n_train\": int(tr.shape[0]),\n",
    "    \"n_test\": int(te.shape[0]),\n",
    "    \"events_train\": int(tr[\"event\"].sum()),\n",
    "    \"events_test\": int(te[\"event\"].sum()),\n",
    "    \"best_penalizer\": best[\"penalizer\"],\n",
    "    \"best_l1_ratio\": best[\"l1_ratio\"],\n",
    "    \"cv_mean_cindex\": float(best[\"mean_cv_cindex\"]),\n",
    "    \"test_cindex\": float(cidx_test),\n",
    "    \"n_features\": int(len(feat_cols))\n",
    "}]).to_csv(perf_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"\\n[OK] Survival prediction (ANTI-LEAK)\")\n",
    "print(\" - CV mean C-index:\", round(best[\"mean_cv_cindex\"], 4))\n",
    "print(\" - TEST C-index   :\", round(cidx_test, 4))\n",
    "print(\" - cv:\", cv_path)\n",
    "print(\" - coef:\", coef_path)\n",
    "print(\" - risk:\", risk_path)\n",
    "print(\" - perf:\", perf_path)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) ISS prediction (if available) using FULL anti-leak feature set\n",
    "# ---------------------------------------------------------\n",
    "iss_candidates = [\"diagnoses.iss_stage\", \"iss_stage\", \"ISS\", \"ISS_stage\"]\n",
    "iss_col = next((c for c in iss_candidates if c in df_model.columns), None)\n",
    "\n",
    "if iss_col is None:\n",
    "    print(\"\\n[INFO] ISS stage not found. Skipping ISS classifier.\")\n",
    "else:\n",
    "    df_iss = df_model.dropna(subset=[iss_col]).copy()\n",
    "    y = df_iss[iss_col].astype(str).values\n",
    "    X = df_iss[feat_cols_iss].copy()\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sca\", StandardScaler()),\n",
    "        (\"sel\", SelectKBest(score_func=mutual_info_classif, k=min(50, X_tr.shape[1]))),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=20000,\n",
    "            solver=\"lbfgs\",\n",
    "            class_weight=\"balanced\",\n",
    "            multi_class=\"multinomial\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # light tuning\n",
    "    k_grid = [25, 50, 100]\n",
    "    C_grid = [0.1, 1.0, 10.0]\n",
    "    cv2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    best2 = {\"cv_macro_f1\": -np.inf, \"k\": None, \"C\": None}\n",
    "    for k in k_grid:\n",
    "        for C in C_grid:\n",
    "            pipe.set_params(sel__k=min(k, X_tr.shape[1]), clf__C=C)\n",
    "            f1s = []\n",
    "            for a, b in cv2.split(X_tr, y_tr):\n",
    "                pipe.fit(X_tr.iloc[a], y_tr[a])\n",
    "                pred = pipe.predict(X_tr.iloc[b])\n",
    "                f1s.append(f1_score(y_tr[b], pred, average=\"macro\"))\n",
    "            m = float(np.mean(f1s))\n",
    "            if m > best2[\"cv_macro_f1\"]:\n",
    "                best2.update({\"cv_macro_f1\": m, \"k\": k, \"C\": C})\n",
    "\n",
    "    pipe.set_params(sel__k=min(best2[\"k\"], X_tr.shape[1]), clf__C=best2[\"C\"])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    f1m = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    labels = np.unique(y)  # stable label ordering\n",
    "    cm = confusion_matrix(y_te, y_pred, labels=labels)\n",
    "\n",
    "    iss_perf_path = os.path.join(RES_DIR, \"pred_iss_performance.tsv\")\n",
    "    pd.DataFrame([{\n",
    "        \"model\": \"LogReg + SelectKBest (ANTI-LEAK features)\",\n",
    "        \"iss_col\": iss_col,\n",
    "        \"best_k\": best2[\"k\"],\n",
    "        \"best_C\": best2[\"C\"],\n",
    "        \"cv_macro_f1\": best2[\"cv_macro_f1\"],\n",
    "        \"test_accuracy\": float(acc),\n",
    "        \"test_macro_f1\": float(f1m),\n",
    "        \"n_train\": int(len(y_tr)),\n",
    "        \"n_test\": int(len(y_te)),\n",
    "        \"n_features\": int(X.shape[1])\n",
    "    }]).to_csv(iss_perf_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    iss_rep_path = os.path.join(RES_DIR, \"pred_iss_classification_report.txt\")\n",
    "    with open(iss_rep_path, \"w\") as f:\n",
    "        f.write(classification_report(y_te, y_pred, zero_division=0))\n",
    "\n",
    "    iss_cm_path = os.path.join(RES_DIR, \"pred_iss_confusion_matrix.tsv\")\n",
    "    pd.DataFrame(cm, index=labels, columns=labels).to_csv(iss_cm_path, sep=\"\\t\")\n",
    "\n",
    "    print(\"\\n[OK] ISS prediction (ANTI-LEAK features)\")\n",
    "    print(\" - TEST accuracy:\", round(float(acc), 4), \"| macro-F1:\", round(float(f1m), 4))\n",
    "    print(\" - report:\", iss_rep_path)\n",
    "    print(\" - cm:\", iss_cm_path)\n",
    "    print(\" - perf:\", iss_perf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NDM4crTLjFBN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "NDM4crTLjFBN",
    "outputId": "fe9bde19-7a4f-4f95-f379-bced0e567aa9"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ✅ FINAL) ZIPAR TUDO DO AMBIENTE DE TRABALHO + BAIXAR (COLAB)\n",
    "# =========================================================\n",
    "import os, zipfile, pathlib, datetime\n",
    "\n",
    "# 1) Define \"roots\" do seu projeto (prioriza as pastas do pipeline)\n",
    "roots = []\n",
    "for k in [\"BASE_OUT_DIR\", \"OUT_DIR\", \"RES_DIR\", \"LOG_DIR\", \"RAW_DIR\", \"PROC_DIR\", \"PROCESSED_DIR\", \"RESULTS_DIR\"]:\n",
    "    v = globals().get(k)\n",
    "    if isinstance(v, str) and v and os.path.exists(v):\n",
    "        roots.append(os.path.abspath(v))\n",
    "\n",
    "# Fallback: se nada estiver definido, usa /content (Colab) ou cwd\n",
    "if not roots:\n",
    "    roots = [\"/content\"] if os.path.exists(\"/content\") else [os.getcwd()]\n",
    "\n",
    "# Remove duplicados mantendo ordem\n",
    "seen = set()\n",
    "roots = [r for r in roots if not (r in seen or seen.add(r))]\n",
    "\n",
    "# 2) Nome do zip\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_path = os.path.join(\"/content\" if os.path.exists(\"/content\") else os.getcwd(),\n",
    "                        f\"workspace_backup_{ts}.zip\")\n",
    "\n",
    "# 3) O que excluir (para não explodir tamanho/tempo por cache/drive)\n",
    "EXCLUDE_DIRS = {\n",
    "    \"/content/drive\",              # evita zipar o Drive inteiro\n",
    "    \"/content/sample_data\",\n",
    "}\n",
    "EXCLUDE_PARTS = {\".ipynb_checkpoints\", \"__pycache__\", \".cache\", \".config\", \".git\"}\n",
    "\n",
    "def should_skip(path: str) -> bool:\n",
    "    ap = os.path.abspath(path)\n",
    "    for ed in EXCLUDE_DIRS:\n",
    "        if ap.startswith(os.path.abspath(ed) + os.sep) or ap == os.path.abspath(ed):\n",
    "            return True\n",
    "    parts = set(pathlib.Path(ap).parts)\n",
    "    return len(parts.intersection(EXCLUDE_PARTS)) > 0\n",
    "\n",
    "# 4) Zipar\n",
    "count = 0\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as zf:\n",
    "    for root in roots:\n",
    "        root = os.path.abspath(root)\n",
    "        if should_skip(root):\n",
    "            continue\n",
    "        for dirpath, dirnames, filenames in os.walk(root):\n",
    "            # filtra diretórios excluídos\n",
    "            dirnames[:] = [d for d in dirnames if not should_skip(os.path.join(dirpath, d))]\n",
    "            if should_skip(dirpath):\n",
    "                continue\n",
    "\n",
    "            for fn in filenames:\n",
    "                fp = os.path.join(dirpath, fn)\n",
    "                if should_skip(fp):\n",
    "                    continue\n",
    "\n",
    "                # caminho relativo dentro do zip (mantém estrutura)\n",
    "                arcname = os.path.relpath(fp, start=os.path.dirname(root))\n",
    "                try:\n",
    "                    zf.write(fp, arcname=arcname)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] não consegui adicionar: {fp} | {e}\")\n",
    "\n",
    "size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "print(f\"[OK] ZIP criado: {zip_path}\")\n",
    "print(f\"[OK] Arquivos incluídos: {count}\")\n",
    "print(f\"[OK] Tamanho: {size_mb:.2f} MB\")\n",
    "\n",
    "# 5) Baixar (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "except Exception as e:\n",
    "    print(\"[INFO] Não consegui acionar download automático (fora do Colab?).\")\n",
    "    print(\"Faça download manual do arquivo:\", zip_path)\n",
    "    print(\"Detalhe:\", str(e)[:200])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  },
  "tiago_phd": {
   "added_cells": {
    "km_topcnv_presence": true,
    "pearson_corr_cnv_burden_vs_clinical": true,
    "timestamp": "2026-01-24T15:33:22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
